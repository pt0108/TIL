> 부트캠프 10주차 2022년 12월 22일

# 역전파 이용 학습

→ [역전파 이해하기](https://wikidocs.net/37406), [역전파](https://thebook.io/006789/ch05/03/04/)

계산을 왼쪽에서 오른쪽으로 진행하는 단계 : 순전파 (forward propagation)

순전파의 반대 방향 : **역전파 (backward propagation)** ← 미분을 계산할 때 중요!

역전파는 기울기를 계산하고, 정확한 입력을 정확한 결과에 매핑한다.

합성 함수는 여러 함수로 구성된 함수인데, 합성 함수의 미분에 대한 성질이 연쇄법칙의 원리이다. **합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱**으로 나타낼 수 있다.

계산 그래프의 이점은 순전파와 역전파를 활용해서 **각 변수의 미분을 효율적으로 구할 수 있는 것.**

![image](https://user-images.githubusercontent.com/106129152/209090656-027d9598-f7a6-4fec-81d4-6110c1f5199c.png)

→ [오차 역전파법](https://deep-learning-study.tistory.com/16), [오차 역전파법(2)](https://deep-learning-study.tistory.com/17)

순전파 때 수행하는 행렬의 곱을 **어파인 변환(Affine transformation)**이라고 한다.

np.dot()을 생각하면 된다! (대응하는 차원의 원소 수를 일치시킴)

단, 덧셈의 역전파는 그대로 전달.

→ [딥러닝 계층 구현](https://amber-chaeeunk.tistory.com/21)

```python
import numpy as np
from dataset.mnist import load_mnist
```

```python
# 손실함수 (Cross Entropy)
def cross_entropy_error(y, t):
    delta = 1e-7 # 0.0000001
    
    if y.ndim == 1:
        t = t.reshape(1, t.size) 
        y = y.reshape(1, y.size)
        
    batch_size = y.shape[0]
    return -np.sum(t*np.log(y+delta))/batch_size
```

```python
# 신경망에서 사용할 W(Matrix 형태)의 편미분 행렬을 구하는 함수
# 신경망의 기울기 : 그레디언트 (편미분 벡터)
def numerical_gradient(f, x): # x의 shape이 (784, 20) => grads 도 (784, 20)
    h = 1e-4 # 0.0001
    grads = np.zeros_like(x)
    
    it = np.nditer(x, flags=["multi_index"], op_flags=["readwrite"])
        
    while not it.finished:
        idx = it.multi_index
        tmp_val = x[idx]

        x[idx] = tmp_val + h
        fxh1 = f(x) # f(x+h)
        x[idx] = tmp_val - h
        fxh2 = f(x) # f(x-h)
    
        grads[idx] = (fxh1 - fxh2) / (2*h)
        x[idx] = tmp_val
        it.iternext()
        
    return grads
```

```python
# Softmax
def softmax(x):
    if x.ndim == 2:
        x = x.T # 10*100
        x = x - np.max(x, axis=0) # 10*100 - 100 = 10*100
        y = np.exp(x) / np.sum(np.exp(x), axis=0)
        return y.T 

    x = x - np.max(x) # 오버플로 대책
    return np.exp(x) / np.sum(np.exp(x))
```

```python
# Sigmoid
def sigmoid(x):
    return 1/(1+np.exp(-x))
```

## 2층 신경망 구현하기

```python
class TwoLayerNet:
    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):
        # W1 shape (784, 20), B1 shape (20, ), W2 shape (20, 10). B2 shape (10,)
        self.params = {}
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size)
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
        self.params['b2'] = np.zeros(output_size)
        
    def predict(self, x):
        W1, W2 = self.params['W1'], self.params['W2']
        b1, b2 = self.params['b1'], self.params['b2']
        
        a1 = np.dot(x, W1) + b1 # (784,) X (784, 20) + (20,)
        z1 = sigmoid(a1) # (20,)
        
        a2 = np.dot(z1, W2) + b2 # (20,) X (20, 10) + (10,)
        y = softmax(a2) # (10,)
        
        return y
    
    def loss(self, x, t):
        y = self.predict(x)
        loss = cross_entropy_error(y, t)
        return loss      
    
    def accuracy(self, x, t):
        y = self.predict(x) # y는 0~1사이의 확률값
        y = np.argmax(y, axis=1) # y는 클래스의 인덱스
        t = np.argmax(t, axis=1)
        accuracy = np.sum(y==t)/x.shape[0]
        
        return accuracy
    
    def numerical_gradient(self, x, t):
        f = lambda w : self.loss(x, t)
        
        # W1 shape (784, 20), B1 shape (20, ), W2 shape (20, 10). B2 shape (10,)
        grads = {}
        grads["W1"] = numerical_gradient(f, self.params['W1']) # W1 (784, 20) --> dW1 (784, 20)
        grads["b1"] = numerical_gradient(f, self.params['b1']) # b1 (20,) --> db1 (20,)
        grads["W2"] = numerical_gradient(f, self.params['W2']) # W2 (20, 10) --> dW2 (20, 10)
        grads["b2"] = numerical_gradient(f, self.params['b2']) # b2 (10,) --> db2 (10,)
        
        return grads
    
    def gradient(self, x, t):    
        grads = {}
        
        W1, W2 = self.params['W1'], self.params['W2']
        b1, b2 = self.params['b1'], self.params['b2']
        
        a1 = np.dot(x, W1) + b1
        z1 = sigmoid(a1)
        a2 = np.dot(z1, W2) + b2
        y = softmax(a2)
        
        batch_size = x.shape[0]
        da2 = (y-t)/batch_size
        
        db2 = np.sum(da2, axis=0) ##
        dw2 = np.dot(z1.T, da2) ##
        
        dz1 = np.dot(da2, W2.T)
        da1 = (1-z1)*z1*dz1
        
        db1 = np.sum(da1, axis=0) ##
        dw1 = np.dot(x.T, da1) ##
        
        grads['W1'], grads['W2'] = dw1, dw2
        grads['b1'], grads['b2'] = db1, db2
        
        return grads
```

```python
(X_train, y_train), (X_test, y_test) = load_mnist(normalize=True, flatten=True, one_hot_label=True)

network = TwoLayerNet(input_size=784, hidden_size=20, output_size=10)

train_loss_list = []
train_acc_list = []
test_acc_list = []

# 하이퍼파라미터
iters_num = 10000
batch_size = 100 # 미니배치 사이즈
learning_rate = 0.1

# 1 에폭(epoch) : 훈련데이터 전체를 소진하는데 걸리는 시간
# 60000개의 훈련 데이터를 100개(배치사이즈)씩 학습했을 때 모두 소진하는데 걸리는 횟수 : 600번
train_size = X_train.shape[0] # 60,000
iter_per_epoch = train_size/batch_size # 60000(전체)/100(배치) = 600

for i in range(iters_num):
    batch_mask = np.random.choice(60000, 100) # 랜덤하게 뽑은 배치의 인덱스
    x_batch = X_train[batch_mask]
    t_batch = y_train[batch_mask]
    
    # 1. Gradient
    grads = network.gradient(x_batch, t_batch)
    
    # 2. Gradient Descent (모델 파라미터 업데이트)
    for keys in ('W1', 'W2', 'b1', 'b2'):
        # W(new) ← W(old) - (lr * Gradient) : 경사하강법
        network.params[keys] = network.params[keys] - (learning_rate * grads[keys])
        
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)
    
    # epoch 마다 로그 출력하기 - 600, 1200, 1800, 240, ...
    if i % iter_per_epoch == 0: # 600, 1200, ...
        train_accuracy = network.accuracy(X_train, y_train)
        test_accuracy = network.accuracy(X_test, y_test)
        train_acc_list.append(train_accuracy)
        test_acc_list.append(test_accuracy)
        print(str(i) +' : Train Accuracy : ' + str(train_accuracy) + ' Test Accuracy : ' + str(test_accuracy))
```

```
0 : Train Accuracy : 0.10651666666666666 Test Accuracy : 0.1046
600 : Train Accuracy : 0.7444166666666666 Test Accuracy : 0.754
1200 : Train Accuracy : 0.8649 Test Accuracy : 0.8691
1800 : Train Accuracy : 0.8904 Test Accuracy : 0.8947
2400 : Train Accuracy : 0.9029833333333334 Test Accuracy : 0.9049
3000 : Train Accuracy : 0.9099166666666667 Test Accuracy : 0.9106
3600 : Train Accuracy : 0.9157833333333333 Test Accuracy : 0.9139
4200 : Train Accuracy : 0.92035 Test Accuracy : 0.9191
4800 : Train Accuracy : 0.9240333333333334 Test Accuracy : 0.9223
5400 : Train Accuracy : 0.92795 Test Accuracy : 0.9254
6000 : Train Accuracy : 0.9299666666666667 Test Accuracy : 0.9273
6600 : Train Accuracy : 0.9334333333333333 Test Accuracy : 0.9311
7200 : Train Accuracy : 0.9361166666666667 Test Accuracy : 0.934
7800 : Train Accuracy : 0.9372166666666667 Test Accuracy : 0.9363
8400 : Train Accuracy : 0.94005 Test Accuracy : 0.9377
9000 : Train Accuracy : 0.9419166666666666 Test Accuracy : 0.939
9600 : Train Accuracy : 0.94355 Test Accuracy : 0.9406
```

### **정확도(Accuracy)**

```python
import matplotlib.pyplot as plt

x = np.arange(len(train_acc_list))
plt.plot(x, train_acc_list, label='train_accuracy')
plt.plot(x, test_acc_list, label='test_accuracy')
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.legend()
```

![image](https://user-images.githubusercontent.com/106129152/209090766-83813969-ce80-409f-8055-8a2d28ba1c79.png)

### 손실값(Loss)

```python
x = np.arange(len(train_loss_list))
plt.plot(x, train_loss_list)
plt.xlabel('Iterations')
plt.ylabel('Loss')
```

![image](https://user-images.githubusercontent.com/106129152/209090818-55f48234-4167-4a9f-b77c-b7e43a86fbce.png)

```python
x = np.arange(len(train_loss_list)-9000)
plt.plot(x, train_loss_list[:1000])
plt.xlabel('Iterations')
plt.ylabel('Loss')
```

![image](https://user-images.githubusercontent.com/106129152/209090868-bf57179a-8bb5-41cc-aef5-8d37cfdbd53a.png)
