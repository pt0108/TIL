> 부트캠프 11주차 2022년 12월 26일

# 파이토치 기초

- 파이토치 공식 문서 : [http://pytorch.org/](http://pytorch.org/)
- Numpy 배열과 유사한 Tensor
- GPU에서 빠른 속도로 처리되는 Tensor
- 자동 미분 계산 기능(오차역전파용)
- 그 외 신경망을 구축하기 위한 모듈들
- 직관적인 인터페이스

```python
import torch
import numpy as np
```

## 1. 텐서

- 텐서는 파이토치의 데이터 형태
- 텐서는 단일 데이터 형식으로 된 자료들의 다차원 행렬
- 텐서는 간단한 명령어(변수 뒤에 .cuda()를 추가)를 사용해서 GPU로 연산을 수행하게 할 수 있음

![image](https://user-images.githubusercontent.com/106129152/209522227-0ff6af8a-9fc3-418d-858d-5b7d2fa83b86.png)

### 텐서 생성

```python
x = torch.empty(5, 4)
x
# tensor([[2.4471e+32, 2.7374e+20, 2.6219e+20, 2.1127e-19],
#         [1.8990e+28, 2.9602e+29, 7.1447e+31, 2.2266e-15],
#         [1.7753e+28, 1.0891e+27, 3.3776e-15, 7.3988e+31],
#         [4.4849e+21, 2.7370e+20, 6.4640e-04, 1.8461e+20],
#         [9.0941e-04, 1.7860e+25, 2.8940e+12, 7.5338e+28]])

torch.ones(3, 3)
# tensor([[1., 1., 1.],
#         [1., 1., 1.],
#         [1., 1., 1.]])

torch.zeros(2)
# tensor([0., 0.])

torch.randn(5, 6)
# tensor([[ 1.2216,  0.8583, -0.2881,  0.7690, -0.6663, -0.5343],
#        [ 0.3026,  2.2199,  1.1018,  0.4223,  0.6199, -1.7852],
#        [ 0.9616, -0.0040,  0.5491,  0.5483, -0.4358,  0.6407],
#        [ 0.3533,  1.0935, -1.4916, -1.3044,  0.3854,  0.8009],
#        [-2.8238,  0.7762, -1.4293, -0.6499, -0.2292, -0.4322]])

l = [3, 4]
torch.tensor(l)
# tensor([3, 4])

n = np.array([4, 5, 6])
torch.tensor(n)
# tensor([4, 5, 6], dtype=torch.int32)
```

### 텐서 사이즈

```python
x = torch.empty(5, 4)
x.shape
# torch.Size([5, 4])

x.size()
# torch.Size([5, 4])

x.size()[0], x.size()[1]
# (5, 4)

x.size(0), x.size(1)
# (5, 4)
```

### 텐서 타입

```python
type(x)
# torch.Tensor

x.dtype # 개별 원소의 타입
# torch.float32

a = torch.tensor([1, 2, 3])
a
# tensor([1, 2, 3])

a.dtype
# torch.int64

b = torch.tensor([1.1, 2.1, 3.1])
b
# tensor([1.1000, 2.1000, 3.1000])

b.dtype
# torch.float32

a_f = torch.tensor([1, 2, 3], dtype=torch.float32)
a_f.dtype
# torch.float32

c = torch.Tensor([1, 2, 3]) # Tensor는 기본 데이터 타입이 float32, 데이터 타입을 함수 내부에서 지정 불가
c
# tensor([1., 2., 3.])

c.dtype
# torch.float32
```

```python
l_t = torch.LongTensor([1, 2])
f_t = torch.FloatTensor([1, 2])
d_t = torch.DoubleTensor([1, 2])
l_t.dtype, f_t.dtype, d_t.dtype
# (torch.int64, torch.float32, torch.float64)

g = d_t.type(torch.FloatTensor) # type() : 형변환 함수
g.dtype
# torch.float32
```

### 텐서 연산

```python
x = torch.rand(2, 2)
y = torch.rand(2, 2)

x
# tensor([[0.7657, 0.4533],
#        [0.1286, 0.2546]])

y
# tensor([[0.6734, 0.5996],
#         [0.1520, 0.0668]])

x + y
# tensor([[1.4390, 1.0529],
#         [0.2806, 0.3214]])

torch.add(x, y)
# tensor([[1.4390, 1.0529],
#         [0.2806, 0.3214]])

y.add(x)
# tensor([[1.4390, 1.0529],
#        [0.2806, 0.3214]])

y.add_(x) # _ (언더바) : inplace 연산
y
# tensor([[1.4390, 1.0529],
#        [0.2806, 0.3214]])
```

### 텐서 색인

```python
y[0]
# tensor([1.4390, 1.0529])

y[:, 1:]
# tensor([[1.0529],
#        [0.3214]])

y[:, 1]
# tensor([1.0529, 0.3214])
```

### 텐서의 크기 변환

```python
x = torch.rand(8, 8)
x.size()
# torch.Size([8, 8])

a = x.view(64) # numpy reshape 기능
a.size()
# torch.Size([64])

b = x.view(16, 4)
b.size()
# torch.Size([16, 4])

c = x.view(-1, 4) 
c.size()
# torch.Size([16, 4])

d = x.view(-1, 4, 4)
d.size()
# torch.Size([4, 4, 4])

d.resize_(16, 4)
d.size()
# torch.Size([16, 4])

e = d.view(4, 4, 4)
e.size()
# torch.Size([4, 4, 4])

f = torch.flatten(e)
f.size()
# torch.Size([64])

f = torch.flatten(e, 0) # 0번 dim(차원)부터 flatten
f.size()
# torch.Size([64])

g = torch.flatten(e, 1) # 1번 dim(차원)부터 flatten
g.size()
# torch.Size([4, 16])
```

### 텐서 → 넘파이, 넘파이 → 텐서

```python
type(g)
# torch.Tensor

g_n = g.numpy() # tensor를 numpy로
type(g_n)
# numpy.ndarray

g_t = torch.from_numpy(g_n) # numpy를 tensor로
type(g_t)
# torch.Tensor
```

### 단일 텐서에서 값으로 변환

```python
x = torch.ones(1)
x
# tensor([1.])

x.item()
# 1.0
```

## 2. 역전파

→ [pytorch 자동미분](https://kingnamji.tistory.com/44)

```python
x = torch.ones(2, 2, requires_grad=True) # requires_grad : 텐서에 대한 기울기 저장
x
# tensor([[1., 1.],
#         [1., 1.]], requires_grad=True)

y = x + 1
y
# tensor([[2., 2.],
#         [2., 2.]], grad_fn=<AddBackward0>)

z = 2 * y**2
z
# tensor([[8., 8.],
#         [8., 8.]], grad_fn=<MulBackward0>)

l = z.mean()
l
# tensor(8., grad_fn=<MeanBackward0>)

l.backward() # backward : 미분

# y는 x에 대한 식, z는 y에 대한 식, l은 z에 대한 식
# 최종식은 합성함수로 표현될 수 있음, l은 x로 표현이 가능하고 미분도 가능

x.grad # 미분 결과
# tensor([[2., 2.],
#         [2., 2.]])
```

# MNIST in Pytorch

## MNIST in Pytorch - (1) 학습

```python
import torch # 파이토치 기본 라이브러리 
import torchvision # 이미지 관련 파이토치 라이브러리
from torchvision import datasets # 토치비전에서 제공하는 데이터셋
from torchvision import transforms # 이미지 전처리 기능들을 제공하는 라이브러리
from torch.utils.data import DataLoader # 데이터를 모델에 사용할 수 있도록 적재해 주는 라이브러리
import numpy as np 
import matplotlib.pyplot as plt
```

## ****1. 데이터 불러오기****

```python
# 현재 위치 기준 디렉토리 생성, 파일 다운로드, 트레인 데이터/테스트 데이터 생성, 텐서로 변환
trainset = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transforms.ToTensor()) 
testset = datasets.MNIST('MNIST_data/', download=True, train=False, transform=transforms.ToTensor())
```

```python
print(type(trainset), len(trainset))
print(type(testset), len(testset))
# <class 'torchvision.datasets.mnist.MNIST'> 60000
# <class 'torchvision.datasets.mnist.MNIST'> 10000
```

```python
# trainset[0][0] # tensor로 변환된 이미지 데이터
# trainset[0][1] # 이미지 데이터의 라벨(정답)

print(type(trainset[0][0]), type(trainset[0][1]))
# <class 'torch.Tensor'> <class 'int'>

trainset[0][0].size() # trainset[0][0].shape()
# torch.Size([1, 28, 28])
```

일반적으로 OpenCV, matplotlib에서 읽어들인 이미지 array는 height * width * channels 형상

그러나 pytorch로 읽어들인 **이미지 tensor는 channels * height * width** 형상

→ ***1 x 28 x 28 (1개의 채널, 높이는 28, 너비는 28인 이미지)***

***<참고>***

아래와 같이 transform 파라미터에 **Compose** 함수를 사용해서

데이터셋을 로드할 때 원하는 전처리를 차례대로 넣을 수 있음! 

```python
# transforms.Compose([전처리1, 전처리2])
transform = transforms.Compose([transfoms.Resize(16), transforms.ToTensor()])
trainset = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)
```

## 2. 데이터 시각화

### tensor를 matplotlib에서 표시했을 경우

```python
sample_img = trainset[0][0]
sample_img.size() # torch.Size([1, 28, 28])

sample_img[0].size() # 한차원 줄임
# torch.Size([28, 28])

plt.imshow(sample_img[0], cmap='gray')
```

![image](https://user-images.githubusercontent.com/106129152/209522276-5aae5200-2b30-4834-bd14-a281d480c5d4.png)

### numpy로 변환 후 matplotlib에서 표시했을 경우

```python
numpy_sample = sample_img.numpy()
numpy_sample.shape # (1, 28, 28)

type(numpy_sample) # numpy.ndarray

plt.imshow(numpy_sample[0], cmap='gray')
```

![image](https://user-images.githubusercontent.com/106129152/209522317-6e024268-be3e-4e94-afe4-42a223503742.png)

### tensor 자체에서 차원을 색인해서사용하는 방법

```python
sample_img.size() # torch.Size([1, 28, 28])

sample_t_permute = sample_img.permute(1, 2, 0) # 1차원, 2차원, 0차원 순으로 차원을 맞교환
sample_t_permute.size() # torch.Size([28, 28, 1])

sample_t_permute_squeeze = sample_t_permute.squeeze(axis=2) # 지정된 차원을 제거
sample_t_permute_squeeze.size() # torch.Size([28, 28])

# 위를 합쳐서 한번에
sample_tensor = sample_img.permute(1, 2, 0).squeeze(axis=2)
plt.imshow(sample_tensor, cmap='gray')
```

![image](https://user-images.githubusercontent.com/106129152/209522332-f3ce3762-d19e-4fc6-b015-8f321b03350e.png)

컬러 이미지를 다룰 경우에는 squeeze를 할 필요가 없다.

→ `color_sampe = sample_img.permute(1, 2, 0)` 그냥 permute만! 

```python
figure, axes = plt.subplots(nrows=1, ncols=8, figsize=(22, 6))
for i in range(8):
    axes[i].imshow(trainset[i][0][0], cmap='gray')
    axes[i].set_title(trainset[i][1])
```

![image](https://user-images.githubusercontent.com/106129152/209522362-01aeeba0-523c-46c4-adca-ffa9bfa07b6e.png)

## 3. 데이터 적재

```python
# torch.utils.data.DataLoader
# 데이터를 셔플하거나, 병렬로 데이터를 부르거나, 혹은 미니 배치 작업 단위로 데이터를 준비

batch_size = 100
trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True) # 훈련용
testloader = DataLoader(testset, batch_size=batch_size, shuffle=False) # 테스트용

print(type(trainloader), len(trainloader))
print(type(testloader), len(testloader))
# <class 'torch.utils.data.dataloader.DataLoader'> 600
# <class 'torch.utils.data.dataloader.DataLoader'> 100
```

```python
train_iter = iter(trainloader)
images, labels = next(train_iter)
print(images.size(), labels.size())
# torch.Size([100, 1, 28, 28]) torch.Size([100])
```

100개씩 준비가 되어있음을 확인할 수 있다.

## 4. 모델 생성

```python
from torch import nn # 파이토치에서 제공하는 다양한 계층(Linear layer, Convolutional Layer, ...)
from torch import optim # 옵티마이저 (경사하강법, ...)
import torch.nn.functional as F # 파이토치에서 제공하는 함수(활성화 함수, ...)
```

![image](https://user-images.githubusercontent.com/106129152/209522400-1dc93d22-1513-45c6-bd53-aed2317cd785.png)

```python
class MNIST_DNN(nn.Module):
    def __init__(self):
        super().__init__()
        # 계층 정의 784(input), 20(hiddne), 10(output)
        # fully connecter layer(Affine Layer) : np.dot(x, w) + b
        self.fc1 = nn.Linear(in_features=784, out_features=20) 
        self.fc2 = nn.Linear(in_features=20, out_features=10)
        
    def forward(self, x):
        x = self.fc1(x)
        x = F.sigmoid(x)
        x = self.fc2(x)
        # x = F.softmax(x)
        return x
```

***<참고>*** 

that this case is equivalent to the combination of LofSoftmax and NLLLoss.

기존의 Softmax + CrossEntropy 조합이 LogSoftMax + NLLLoss 조합으로 바뀌었음

**torch.nn.CrossEntropyLoss는 nn.LogSoftmax와 nn.NLLLoss의 연산의 조합**

따라서 **CrossEntropyLoss()를 손실함수로 사용하게 되면, 예측을 할 때 softmax()를 사용하지 않는 것이 좋음**

→ ****[nn.CrossEntropyLoss](http://www.gisdeveloper.co.kr/?p=8668)****

```python
model = MNIST_DNN()
model
# MNIST_DNN(
#   (fc1): Linear(in_features=784, out_features=20, bias=True)
#   (fc2): Linear(in_features=20, out_features=10, bias=True)
# )
```

```python
for name, parameter in model.named_parameters():
    print(name, parameter.size())
# fc1.weight torch.Size([20, 784])
# fc1.bias torch.Size([20])
# fc2.weight torch.Size([10, 20])
# fc2.bias torch.Size([10])
```

-> [torchsummary](https://teddylee777.github.io/pytorch/pytorch-torchsummary) : 모델의 구조도 요약 라이브러리

```python
from torchsummary import summary

summary(model.cuda(), (1, 784)) # (channle, input_size)
```

![image](https://user-images.githubusercontent.com/106129152/209522420-d7d1d1e7-f15d-4299-bc43-f19e734145ed.png)

pytorch랑 torchsummary를 설치해서 실행하는 것은 잘 되는데,

계속 **Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!** 에러가 발생한다. 일단 구글 코랩에서 같이 수업을 따라가고, 파일만 ipynb로 다운받아야할 듯.

## 5. 모델 컴파일 ****(손실함수, 옵티마이저 선택)****

```python
learning_rate = 0.1 
# 손실함수
criterion = nn.CrossEntropyLoss()

# 옵티마이저(경사하강법, 최적화 함수)
optimizer = optim.SGD(model.parameters(), lr=learning_rate)
```

## 6. 모델 훈련

```python
epochs = 17
steps = 0

steps_per_epoch = len(trainset)/batch_size # 600 iterations

for e in range(epochs):
    running_loss = 0
    for images, labels in iter(trainloader): # 이터레이터로부터 미니배치 100개씩을 가져와 images, labels에 준비
        steps += 1
        # 1. 입력 데이터 준비
        images.resize_(images.size()[0], 784) # 100, 1, 28, 28

        # 2. 전방향(Forward) 예측 
        outputs = model.forward(images) # 예측
        loss = criterion(outputs, labels) # 예측과 결과를 통해 Cross Entropy Loss 반환

        # 3. 역방향(Backward) 오차(Gradient) 전파
        optimizer.zero_grad() # 파이토치에서 gradient가 누적되지 않게 하기 위해
        loss.backward()

        # 4. 경사하강법으로 모델 파라미터 업데이트
        optimizer.step() # W <- W -lr*Gradient

        running_loss += loss.item()
        if (steps % steps_per_epoch) == 0: # step : 600, 1200, 1800 (epoch 마다)
            print('Epoch : {}/{}.....'.format(e+1, epochs),
                'Loss : {}'.format(running_loss/steps_per_epoch))
            running_loss = 0
```

```
Epoch : 1/17..... Loss : 1.3202967202663423
Epoch : 2/17..... Loss : 0.5441073541839917
Epoch : 3/17..... Loss : 0.4021781631807486
Epoch : 4/17..... Loss : 0.34481423450013
Epoch : 5/17..... Loss : 0.31262667107085385
Epoch : 6/17..... Loss : 0.2906053453187148
Epoch : 7/17..... Loss : 0.2740460284178456
Epoch : 8/17..... Loss : 0.26072987254709007
Epoch : 9/17..... Loss : 0.2498336524764697
Epoch : 10/17..... Loss : 0.2405887465427319
Epoch : 11/17..... Loss : 0.23240887054552634
Epoch : 12/17..... Loss : 0.22521105974912645
Epoch : 13/17..... Loss : 0.2186594757437706
Epoch : 14/17..... Loss : 0.21287034366279842
Epoch : 15/17..... Loss : 0.20735944436863066
Epoch : 16/17..... Loss : 0.20257277709742388
Epoch : 17/17..... Loss : 0.19795541713635126
```

## MNIST in Pytorch - (2) 검증, 예측

```python
from torch.utils.data import random_split

# trainset을 train용과 valid용으로 나누고 싶을 때 torch.utils.data.random_split() 사용
trainset, validset = random_split(trainset, [50000, 10000])

print(type(validset), len(validset))
# <class 'torch.utils.data.dataset.Subset'> 10000

validloader = DataLoader(validset, batch_size=batch_size, shuffle=False) # 검증용

print(type(validloader), len(validloader))
# <class 'torch.utils.data.dataloader.DataLoader'> 100
```

→ 학습 과정과 같은 구조, 대신 위처럼 검증용으로 사용할 데이터를 준비함.

## 6. 모델 훈련(검증)

```python
def validation(model, validloader, criterion):
  # 전방향 예측후 나온 점수를 softmax 통과시켜 확률 값을 얻은 뒤
  # 나온 확률 값중 최대값을 최종 예측으로 준비
  # 이 최종 예측과 정답을 비교
  # 전체 중 맞은 것의 개수 비율을 정확도(accuracy)로 계산
  valid_accuracy = 0
  valid_loss = 0

  # 전방향 예측을 구할 때는 gradient가 필요가 없음
  with torch.no_grad():
    for images, labels in validloader: # 10000개의 데이터에 대해 100개씩(미니배치 사이즈) 100번을 iterations
      # 1. 입력데이터 준비
      images.resize_(images.size()[0], 784) # 100, 1, 28, 28
      # 2. 전방향(Forward) 예측 
      logits = model.forward(images) # 점수 반환
      probs = F.softmax(logits, dim=1) # 100x10 형태의 확률
      _, preds = torch.max(probs, 1) # 100개에 대한 최종 예측
      # preds= probs.max(dim=1)[1] 
      correct = (preds == labels).sum()

      accuracy = correct / images.shape[0]
      loss = criterion(logits, labels) # 100개에 대한 loss
      
      valid_accuracy += accuracy
      valid_loss += loss.item() # tensor 값을 꺼내옴
    

  return valid_loss, valid_accuracy # 100세트 전체 대한 총 loss, 총 accuracy
```

```python
epochs = 17
steps = 0

# 1 에폭(epoch)당 반복수
#steps_per_epoch = len(trainset)/batch_size # 500 iterations
steps_per_epoch = len(trainloader) # 500 iterations

for e in range(epochs):
  model.train()
  train_loss = 0
  for images, labels in iter(trainloader): # 이터레이터로부터 미니배치 100개씩을 가져와 images, labels에 준비
    steps += 1
    # 1. 입력 데이터 준비
    images.resize_(images.size()[0], 784) # 100, 1, 28, 28

    # 2. 전방향(Forward) 예측 
    outputs = model.forward(images) # 예측
    loss = criterion(outputs, labels) # 예측과 결과를 통해 Cross Entropy Loss 반환

    # 3. 역방향(Backward) 오차(Gradient) 전파
    optimizer.zero_grad() # 파이토치에서 gradient가 누적되지 않게 하기 위해
    loss.backward()

    # 4. 경사하강법으로 모델 파라미터 업데이트
    optimizer.step() # W <- W -lr*Gradient

    train_loss += loss.item()
    if (steps % steps_per_epoch) == 0: # step : 600, 1200, 1800 (epoch 마다)
      model.eval() # 배치 정규화, 드롭아웃이 적용될 때는 model.forward 연산이 training때와 다르므로 반드시 설정
      valid_loss, valid_accuracy = validation(model, validloader, criterion)
      print('Epoch : {}/{}.....'.format(e+1, epochs),
            'Train Loss : {:.3f}'.format(train_loss/len(trainloader)),
            'Valid Loss : {:.3f}'.format(valid_loss/len(validloader)),
            'Valid Accuracy : {:.3f}'.format(valid_accuracy/len(validloader)))
      train_loss = 0
      model.train()
```

```
Epoch : 1/35..... Train Loss : 0.278 Valid Loss : 0.271 Valid Accuracy : 0.922
Epoch : 2/35..... Train Loss : 0.269 Valid Loss : 0.263 Valid Accuracy : 0.924
Epoch : 3/35..... Train Loss : 0.261 Valid Loss : 0.257 Valid Accuracy : 0.926
Epoch : 4/35..... Train Loss : 0.253 Valid Loss : 0.250 Valid Accuracy : 0.927
Epoch : 5/35..... Train Loss : 0.246 Valid Loss : 0.245 Valid Accuracy : 0.929
Epoch : 6/35..... Train Loss : 0.240 Valid Loss : 0.239 Valid Accuracy : 0.931
Epoch : 7/35..... Train Loss : 0.234 Valid Loss : 0.235 Valid Accuracy : 0.932
Epoch : 8/35..... Train Loss : 0.229 Valid Loss : 0.230 Valid Accuracy : 0.933
Epoch : 9/35..... Train Loss : 0.224 Valid Loss : 0.226 Valid Accuracy : 0.933
Epoch : 10/35..... Train Loss : 0.219 Valid Loss : 0.222 Valid Accuracy : 0.935
Epoch : 11/35..... Train Loss : 0.214 Valid Loss : 0.218 Valid Accuracy : 0.937
Epoch : 12/35..... Train Loss : 0.210 Valid Loss : 0.215 Valid Accuracy : 0.938
Epoch : 13/35..... Train Loss : 0.206 Valid Loss : 0.212 Valid Accuracy : 0.938
Epoch : 14/35..... Train Loss : 0.202 Valid Loss : 0.210 Valid Accuracy : 0.940
Epoch : 15/35..... Train Loss : 0.198 Valid Loss : 0.206 Valid Accuracy : 0.940
Epoch : 16/35..... Train Loss : 0.195 Valid Loss : 0.203 Valid Accuracy : 0.941
Epoch : 17/35..... Train Loss : 0.191 Valid Loss : 0.200 Valid Accuracy : 0.941
Epoch : 18/35..... Train Loss : 0.188 Valid Loss : 0.198 Valid Accuracy : 0.943
Epoch : 19/35..... Train Loss : 0.185 Valid Loss : 0.196 Valid Accuracy : 0.943
Epoch : 20/35..... Train Loss : 0.183 Valid Loss : 0.193 Valid Accuracy : 0.943
Epoch : 21/35..... Train Loss : 0.180 Valid Loss : 0.191 Valid Accuracy : 0.944
Epoch : 22/35..... Train Loss : 0.177 Valid Loss : 0.189 Valid Accuracy : 0.944
Epoch : 23/35..... Train Loss : 0.175 Valid Loss : 0.188 Valid Accuracy : 0.945
Epoch : 24/35..... Train Loss : 0.172 Valid Loss : 0.185 Valid Accuracy : 0.946
Epoch : 25/35..... Train Loss : 0.170 Valid Loss : 0.184 Valid Accuracy : 0.947
Epoch : 26/35..... Train Loss : 0.168 Valid Loss : 0.184 Valid Accuracy : 0.947
Epoch : 27/35..... Train Loss : 0.166 Valid Loss : 0.180 Valid Accuracy : 0.948
Epoch : 28/35..... Train Loss : 0.164 Valid Loss : 0.180 Valid Accuracy : 0.947
Epoch : 29/35..... Train Loss : 0.162 Valid Loss : 0.177 Valid Accuracy : 0.948
Epoch : 30/35..... Train Loss : 0.160 Valid Loss : 0.176 Valid Accuracy : 0.949
Epoch : 31/35..... Train Loss : 0.158 Valid Loss : 0.175 Valid Accuracy : 0.949
Epoch : 32/35..... Train Loss : 0.156 Valid Loss : 0.174 Valid Accuracy : 0.950
Epoch : 33/35..... Train Loss : 0.155 Valid Loss : 0.174 Valid Accuracy : 0.949
Epoch : 34/35..... Train Loss : 0.153 Valid Loss : 0.171 Valid Accuracy : 0.950
Epoch : 35/35..... Train Loss : 0.151 Valid Loss : 0.170 Valid Accuracy : 0.950
```

## 7. 모델 예측

```python
test_iter = iter(testloader)
images, labels = next(test_iter)
print(images.size(), labels.size()) # torch.Size([100, 1, 28, 28]) torch.Size([100])

rnd_idx = 2
images[rnd_idx].shape, labels[rnd_idx] # 1, 28, 28
# (torch.Size([1, 28, 28]), tensor(1))

flattend_img = images[rnd_idx].view(1, 784)
with torch.no_grad():
	logit = model.forward(flattend_img)

pred = logit.max(dim=1)[1]
pred == labels[rnd_idx]
# tensor([True])

plt.imshow(images[rnd_idx][0], cmap='gray')
```

![image](https://user-images.githubusercontent.com/106129152/209522458-de0538d5-fdf4-4264-adbd-c06399a05e5e.png)

## 8. 모델 평가

```python
def evaluation(model, testloader, criterion):
  # 전방향 예측후 나온 점수(logits)의 최대값을 최종 예측으로 준비
  # 이 최종 예측과 정답을 비교
  # 전체 중 맞은 것의 개수 비율을 정확도(accuracy)로 계산
  test_accuracy = 0
  test_loss = 0

  # 전방향 예측을 구할 때는 gradient가 필요가 없음
  with torch.no_grad():
    for images, labels in testloader: # 10000개의 데이터에 대해 100개씩(미니배치 사이즈) 100번을 iterations
      # 1. 입력데이터 준비
      images.resize_(images.size()[0], 784) # 100, 1, 28, 28
      # 2. 전방향(Forward) 예측 
      logits = model.forward(images) # 점수 반환
      _, preds = torch.max(logits, 1) # 100개에 대한 최종 예측
      # preds= probs.max(dim=1)[1] 
      correct = (preds == labels).sum()

      accuracy = correct / images.shape[0]
      loss = criterion(logits, labels) # 100개에 대한 loss
      
      test_accuracy += accuracy.item()
      test_loss += loss.item() # tensor 값을 꺼내옴
    

  print('Test Loss : ', test_loss/len(testloader))
  print('Test Accuracy : ', test_accuracy/len(testloader))

evaluation(model, testloader, criterion)
# Test Loss :  0.169079061280936
# Test Accuracy :  0.9502000039815903
```

## MNIST in Pytorch - (3) 모델 수정

```python
# nn.Module을 상속받아 Class를 만드는 방법 이외에도 
# 간단한 망을 구성할 때는 아래와 같이 nn.Sequential을 사용해도 된다
model = nn.Sequential(nn.Linear(in_features=784, out_features=20),
                      nn.Sigmoid(),
                      nn.Linear(in_features=20, out_features=10))
```

이번에는 층을 하나 더 만들어서 모델을 수정해보자.

```python
class MNIST_DNN(nn.Module):
  def __init__(self):
    super().__init__()
    # 계층 정의하기 784(input), 20(hiddne), 10(output)
    # fully connected layer(dense lyaer, affine layer) : np.dot(x, w) + b
    self.fc1 = nn.Linear(in_features=784, out_features=128) 
    self.fc2 = nn.Linear(in_features=128, out_features=64)
    self.fc3 = nn.Linear(in_features=64, out_features=10) 
              

  def forward(self, x):
    x = self.fc1(x)
    x = F.relu(x)
    x = self.fc2(x)
    x = F.relu(x)
    x = self.fc3(x)
    # x = F.softmax(x)
    return x
```

```python
model = MNIST_DNN()
model
# MNIST_DNN(
#   (fc1): Linear(in_features=784, out_features=128, bias=True)
#   (fc2): Linear(in_features=128, out_features=64, bias=True)
#   (fc3): Linear(in_features=64, out_features=10, bias=True)
# )

for parameter in model.parameters():
  print(parameter.size())
# torch.Size([128, 784])
# torch.Size([128])
# torch.Size([64, 128])
# torch.Size([64])
# torch.Size([10, 64])
# torch.Size([10])

for name, parameter in model.named_parameters():
  print(name, parameter.size())
# fc1.weight torch.Size([128, 784])
# fc1.bias torch.Size([128])
# fc2.weight torch.Size([64, 128])
# fc2.bias torch.Size([64])
# fc3.weight torch.Size([10, 64])
# fc3.bias torch.Size([10])
```

```python
from torchsummary import summary
summary(model, (1, 784)) # (channle, input_size) # 참고 : Conv layer 사용시 (3, 28, 28)
```

![image](https://user-images.githubusercontent.com/106129152/209522523-d6e930a0-6af2-48ba-a858-0f0d691bf512.png)

![image](https://user-images.githubusercontent.com/106129152/209522544-7d18232e-d2ad-4d92-aaad-407940ff740b.png)

처음 (2)에서 나왔던 Accuracy는 약 95퍼센트 정도였는데,

모델 수정 후 약 97퍼센트 정도로 성능된 결과를 확인할 수 있었다. 

# 옵티마이저

## **1. Momentum**

![image](https://user-images.githubusercontent.com/106129152/209522565-40815086-cab9-4e11-b059-748ab4c131b8.png)

## **2. AdaGrad(Adaptive Gradient)**

![image](https://user-images.githubusercontent.com/106129152/209522592-f13435c8-e08a-4a9e-9448-c655a851d584.png)

![image](https://user-images.githubusercontent.com/106129152/209522604-9c2028d6-311c-457c-86bf-86e2dbc621a4.png)

## **3. RMSProp**

![image](https://user-images.githubusercontent.com/106129152/209522626-d16e40d4-796e-4748-97b7-acc75fca053e.png)

![image](https://user-images.githubusercontent.com/106129152/209522639-057b361a-afe3-4561-9ffc-afe1807b1658.png)

## **4. Adam(Adaptive Moment Estimation)**

![image](https://user-images.githubusercontent.com/106129152/209522651-eb906f38-566d-4135-a29c-70e0cae88184.png)
