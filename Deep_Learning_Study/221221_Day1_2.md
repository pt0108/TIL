> 부트캠프 10주차 2022년 12월 21일

# 신경망

**퍼셉트론** : 신경망(딥러닝)의 기원이 되는 알고리즘

다수의 신호를 입력받아 하나의 신호로 출력.

퍼셉트론으로 AND, NAND, OR 논리 회로를 표현할 수 있다.

XOR 게이트는 2층 퍼셉트론을 이용해서 표현할 수 있다.

(단층 퍼셉트론 : 직선형 영역 / 2층 퍼셉트론 : 곡선형 영역)

→ [논리 회로 게이트 종류](https://wpaud16.tistory.com/160) ,  [퍼셉트론](https://ksm2853305.tistory.com/42)

퍼셉트론은 가중치 설정 작업을 수동으로 해야 하지만 **신경망은 가충지 값을 데이터로부터 자동 학습**한다! 신경망은 이미지를 있는 그대로 학습하고, 이미지에 포함된 중요한 특징까지도 기계가 스스로 학습한다.

→ [신경망이란](https://aws.amazon.com/ko/what-is/neural-network/), [NN(Neural Network)의 개념](https://techblog-history-younghunjo1.tistory.com/38), [Tensorflow Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.14667&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)

```python
import numpy as np
import matplotlib.pyplot as plt
from IPython.display import Image
```

## 활성화 함수

입력 신호의 총합을 출력 신호로 변환하는 함수를 일반적으로 **활성화 함수**라고 한다.

→ [활성화 함수](http://www.incodom.kr/%ED%99%9C%EC%84%B1%ED%99%94%ED%95%A8%EC%88%98)

### 계단 함수

0과 1중 하나의 값만 돌려준다. (딥러닝의 활성화 함수로는 부적절!)

```python
def step_function(x):
    return np.array(x > 0, dtype=np.int32)

X = np.arange(-5.0, 5.0, 0.1)
Y = step_function(X)

plt.plot(X, Y)
```

![image](https://user-images.githubusercontent.com/106129152/208862471-fff3ca25-4c7d-4f9d-81c9-5d93fd4c1816.png)

### 시그모이드 함수

0과 1 사이의 연속적인 실수를 돌려준다.

```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

X = np.arange(-5.0, 5.0, 0.1)
Y = sigmoid(X)

plt.plot(X, Y)
```

![image](https://user-images.githubusercontent.com/106129152/208862515-60ab8ecd-b874-4a1a-9a5c-f638af6634f0.png)

계단 함수와 시그모이드 함수의 중요한 공통점은 비선형 함수라는 점인데, 

**신경망에서는 활성화 함수로 비선형 함수를 사용**해야 한다. 

**선형 함수를 이용하면 신경망의 층을 깊게 하는 의미가 없어지기 때문**!

### ReLU 함수

입력이 0을 넘으면 그대로 출력하고, 0 이하이면 0을 출력한다.

```python
def relu(x):
    return np.maximum(0, x)

X = np.arange(-5.0, 5.0, 0.1)
Y = relu(X)

plt.plot(X, Y)
```

![image](https://user-images.githubusercontent.com/106129152/208862555-4ede868b-7aa7-4a92-b19f-179975d611eb.png)

## 다차원 배열 계산

→ [다차원 배열 계산](https://velog.io/@joo4438/%EB%8B%A4%EC%B0%A8%EC%9B%90-%EB%B0%B0%EC%97%B4%EC%9D%98-%EA%B3%84%EC%82%B0)

```python
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])

np.dot(A, B)
# array([[19, 22],
#        [43, 50]])
```

![image](https://user-images.githubusercontent.com/106129152/208862599-566fd9b4-9d38-42ce-8fbe-c5c8d0db3b2e.png)

```python
A = np.array([[1, 2], [3, 4], [5, 6]])
B = np.array([[7, 8, 9, 10], [11, 12, 13, 14]])

np.dot(A, B)
# array([[ 29,  32,  35,  38],
#        [ 65,  72,  79,  86],
#        [101, 112, 123, 134]])
```

![image](https://user-images.githubusercontent.com/106129152/208862629-3af10609-a4aa-4a57-806b-caf33b33453f.png)

```python
A = np.array([[1, 2], [3, 4], [5, 6]])
B = np.array([7, 8])
C = np.dot(A, B)
print(A.shape, B.shape, C.shape)
# (3, 2) (2,) (3,)
```

![image](https://user-images.githubusercontent.com/106129152/208862657-cb02e250-1c8a-4d1f-b56e-792bb2eefe77.png)

## 신경망에서의 행렬곱

![image](https://user-images.githubusercontent.com/106129152/208862696-12efe975-fe27-42f1-8bb1-842cab1128cd.png)

```python
X = np.array([1, 2])
W = np.array([[1, 3, 5], [2, 4, 6]])
Y = np.dot(X, W)
print(X.shape, W.shape, Y.shape)
# (2,) (2, 3) (3,)
```

![image](https://user-images.githubusercontent.com/106129152/208862733-0c46a962-ef14-4570-809d-cfbcf17789e6.png)

![image](https://user-images.githubusercontent.com/106129152/208862787-0eb37436-6901-48dd-8086-f80222b0e52a.png)

```python
X = np.array([1.0, 0.5]) # (2,)
W1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]]) # (2, 3)
B1 = np.array([0.1, 0.2, 0.3]) # (3,)

A1 = np.dot(X, W1) + B1
print(X.shape, W1.shape, B1.shape, A1.shape)
# (2,) (2, 3) (3,) (3,)
```

![image](https://user-images.githubusercontent.com/106129152/208862834-3e6879c0-7695-440b-9baa-7a37c424e81a.png)

```python
# 위에서 나온 A1에 시그모이드 함수를 통과
def sigmoid(x):
    return 1 / (1+np.exp(-x))

Z1 = sigmoid(A1)
Z1 # (3,)
# array([0.57444252, 0.66818777, 0.75026011])
```

![image](https://user-images.githubusercontent.com/106129152/208862882-dd3e24ae-bd29-490e-83d7-7a9ea6947833.png)

```python
# A2 = Z1W2 + B2
# () = (3,)(3,2)+(2,)

W2 = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]]) # (3, 2)
B2 = np.array([0.1, 0.2]) # (2,)

A2 = np.dot(Z1, W2) + B2 # (2,)
Z2 = sigmoid(A2) # (2,)
print(W2.shape, B2.shape, A2.shape, Z2.shape)
# (3, 2) (2,) (2,) (2,)
```

![image](https://user-images.githubusercontent.com/106129152/208862916-d59ff614-7568-4e11-a264-e9f6fb01a499.png)

```python
def identity_function(x): # 큰 의미 X 회귀문제
    return x

# A3 = Z2W3 + B3
# () = (2,)(2, 2) + (2,)

W3 = np.array([[0.1, 0.3], [0.2, 0.4]]) # (2, 2)
B3 = np.array([0.1, 0.2]) # (2,)
A3 = np.dot(Z2, W3) + B3 # (2,)
Y = identity_function(A3)
print(W3.shape, B3.shape, A3.shape, Y.shape)
# (2, 2) (2,) (2,) (2,)
```

## 출력층(Softmax 함수)

오버플로우를 막을 목적으로 입력 신호 중 최댓값을 이용하는 것이 일반적이다.

```python
def softmax(a):
    c = np.max(a)
    exp_a = np.exp(a - c)
    sum_exp_a = np.sum(exp_a)
    y = exp_a/sum_exp_a
    return y
```

## 3층 신경망 구현

```python
def init_network():
    network = {}
    network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]]) # (2, 3)
    network['B1'] = np.array([0.1, 0.2, 0.3]) # (3,)
    network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]]) # (3, 2)
    network['B2'] = np.array([0.1, 0.2]) # (2,)
    network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]]) # (2, 2)
    network['B3'] = np.array([0.1, 0.2]) # (2,)
    
    return network

def forward(network, X):
    W1, W2, W3 = network['W1'], network['W2'], network['W3']
    B1, B2, B3 = network['B1'], network['B2'], network['B3']
    
    A1 = np.dot(X, W1) + B1
    Z1 = sigmoid(A1)
    A2 = np.dot(Z1, W2) + B2 # (2,)
    Z2 = sigmoid(A2) # (2,)
    A3 = np.dot(Z2, W3) + B3 # (2,)
    Y = identity_function(A3)
    
    return Y
```

```python
network = init_network()
X = np.array([1.0, 0.5]) # (2,)
Y = forward(network, X)
Y
# array([0.31682708, 0.69627909])
```

## 손글씨 숫자(MNIST)

```python
from dataset.mnist import load_mnist

# 데이터 준비
def get_data():
    (X_train, y_train), (X_test, y_test) = load_mnist(normalize=True, flatten=True, one_hot_label=False)
    return X_test, y_test

X_test, y_test = get_data()

print(X_test.shape, y_test.shape)
# (10000, 784) (10000,)
```

```python
# 데이터 확인
plt.imshow(X_test[0].reshape(28, 28), cmap='binary')
```

![image](https://user-images.githubusercontent.com/106129152/208862974-3b691ffc-6171-453a-ba2f-b6aa1bbe114e.png)

```python
import pickle

def init_network():
    with open('sample_weight.pkl', 'rb') as f:
        network = pickle.load(f)
    return network

network = init_network()
print(type(network), network.keys())
# <class 'dict'> dict_keys(['b2', 'W1', 'b1', 'W2', 'W3', 'b3'])

network['W1'].shape, network['W2'].shape, network['W3'].shape
# ((784, 50), (50, 100), (100, 10))

network['b1'].shape, network['b2'].shape, network['b3'].shape
# ((50,), (100,), (10,))
```

- 입력 이미지 한 장이 신경망을 통과하는 과정(전방향 연산, 예측)

![image](https://user-images.githubusercontent.com/106129152/208863025-3fca2401-d3f3-4431-b86a-2286114cf113.png)

```python
def predict(network, X):
    W1, W2, W3 = network['W1'], network['W2'], network['W3']
    B1, B2, B3 = network['b1'], network['b2'], network['b3']
    
    A1 = np.dot(X, W1) + B1 # (50.)
    Z1 = sigmoid(A1) # (50,)
    A2 = np.dot(Z1, W2) + B2 # (100,)
    Z2 = sigmoid(A2) # (100,)
    A3 = np.dot(Z2, W3) + B3 # (10,)
    Y = softmax(A3) # (10,)
    
    return Y

X_test[50].shape # (784,)

y = predict(network, X_test[50])
np.round(y, 3)
# array([0.006, 0.   , 0.005, 0.   , 0.   , 0.005, 0.983, 0.   , 0.   ,
#       0.   ], dtype=float32)
```

```python
correct_cnt = 0
for i, test_img in enumerate(X_test): # 10000 iterations
    prob = predict(network, test_img)
    pred = np.argmax(prob)
    if pred == y_test[i]:
        correct_cnt += 1

accuracy = correct_cnt / len(y_test)
print("Accuracy : ", accuracy)
# Accuracy :  0.9352
```

- image 배치 입력에 대한 신경망 예측 과정

![image](https://user-images.githubusercontent.com/106129152/208863078-f57f3167-4a52-49e0-a8c7-5525b5489f1a.png)

```python
batch_size = 100
correct_cnt = 0
for i in range(0, len(X_test), batch_size): # 10000 iterations
    test_img_batch = X_test[i:i+batch_size] # 100개씩 슬라이싱
    prob_batch = predict(network, test_img_batch)
    pred_batch = np.argmax(prob_batch, axis=1)
    correct_cnt += np.sum(pred_batch == y_test[i:i+batch_size])

accuracy = correct_cnt / len(y_test)
print("Accuracy : ", accuracy)
# Accuracy :  0.9352
```

# 신경망 학습

학습 : 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것.

신경망이 학습을 가능하도록 해주는 지표가 손실 함수.

이 **손실 함수의 결괏값을 가장 작게 만드는 가중치 매개변수를 찾는 것**이 학습의 목표!

정확도를 지표로 삼지 않는 이유는 미분 값이 대부분의 장소에서 0이 되어 매개변수를 갱신할 수 없기 때문이다.

→ [손실 함수](https://amber-chaeeunk.tistory.com/10)

학습 알고리즘 구현하기

1단계 : 미니 배치 → 2단계 : 기울기 산출 → 3단계 : 매개변수 갱신 → 4단계 : 1~3단계 반복

## 오차제곱합(SSE)

![image](https://user-images.githubusercontent.com/106129152/208863153-3769d2ac-6a82-4a55-af94-598c5a4002f4.png)

## 교차 엔트로피(CEE)

![image](https://user-images.githubusercontent.com/106129152/208863198-281b0a67-3fdc-4dda-8c0c-0a3e5075bd89.png)

```python
def cross_entropy_error(y, t):
    delta = 1e-7 # 0.0000001
    return -np.sum(t*np.log(y+delta))
```

```python
t = [0, 0, 1, 0, 0, 0, 0 , 0 , 0, 0] # 정답 : 2 (one-hot encoding)
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0] # 예측 : 2에 60% 확률로 예측
cross_entropy_error(np.array(y), np.array(t))
# 0.510825457099338

t = [0, 0, 1, 0, 0, 0, 0 , 0 , 0, 0] # 정답 : 2 (one-hot encoding)
y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0] # 예측 : 7에 60% 확률로 예측
cross_entropy_error(np.array(y), np.array(t))
# 2.302584092994546
```

→ **예측을 잘못할수록 cross entropy의 값은 커진다**

모든 데이터를 대상으로 손실 함수의 합을 구하려면 시간이 걸리기 때문에 **전체 데이터에서 일부를 랜덤으로 선별하여 근사치 값**을 구한다. 

선별된 일부 데이터 = **미니배치**

```python
# 배치용 크로스 엔트로피 함수
def cross_entropy_error(y, t):
    delta = 1e-7 # 0.0000001
    if y.ndim == 1: # 배치 단위가 아니라 1차원으로 입력되었을 경우
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)
    batch_size = y.shape[0]
    return -np.sum(t*np.log(y+delta))/batch_size
```

## 미분

![image](https://user-images.githubusercontent.com/106129152/208863264-00ca2152-80b8-4c65-9d08-b6a34ba68afa.png)

```python
def function_1(x):
    return 0.01*x**2 + 0.1*x

x = np.arange(0.0, 20.0, 0.1)
y = function_1(x)
plt.xlabel('x')
plt.ylabel('f(x)')
plt.plot(x, y)
```

![image](https://user-images.githubusercontent.com/106129152/208863299-3a6b45bc-d304-4019-8be2-55415eb2c58e.png)

```python
# 중앙차분에 의한 수치미분
def numerical_diff(f, x):
    h = 1e-4 # 0.0001
    return (f(x+h) - f(x-h)) / 2*h

numerical_diff(function_1, 5)
# 1.9999999999908982e-09
```

## 편미분

![image](https://user-images.githubusercontent.com/106129152/208863347-19d45b87-9546-4434-8dc4-81251eb6c883.png)

**변수가 여럿인 함수에 대한 미분**을 편미분이라고 한다.

편미분을 통해 여러 파라미터를 업데이트.

위의 식에서 *x*는 *w*라고 생각하면 된다! (*w* : 가중치)

```python
def function_2(x):
    return x[0]**2 + x[1]**2

# 그레디언트 (편미분 벡터)
def numerical_gradient(f, x):
    h = 1e-4 # 0.0001
    grads = np.zeros_like(x)
    for idx in range(x.size): # x[0], x[1], ....
        tmp_val = x[idx]
        x[idx] = tmp_val + h
        fxh1 = f(x) # f(x+h)
        x[idx] = tmp_val - h
        fxh2 = f(x) # f(x-h)
        grads[idx] = (fxh1 - fxh2) / (2*h)
        x[idx] = tmp_val
    return grads

numerical_gradient(function_2, np.array([3.0, 4.0]))
# array([6., 8.])
```

## 경사하강법

![image](https://user-images.githubusercontent.com/106129152/208863397-6f238b1c-3eb1-46ff-99a2-d4668ccd756c.png)

```python
def gradient_descent(f, init_x, lr=0.01, step_num = 100): # lr : learning rate
    x = init_x
    for i in range(step_num):
        grad = numerical_gradient(f, x) # grad
        x = x- (lr * grad)
    return x
```

```python
# init_x : 초기 랜덤한 weight 벡터 (w1, w2, ...)
# function_2 : loss function (손실 함수)
# gradient_descent : weight의 기울기(미분)를 단서로 loss의 최소값을 찾아가는 과정(경사 하강법)

init_x = np.array([3.0, 4.0]) # 랜덤하게 시작
gradient_descent(function_2, init_x, lr=0.1, step_num=100)
# array([6.11110793e-10, 8.14814391e-10])
```

## 신경망에서의 기울기

기울기가 가리키는 쪽은 **각 장소에서 함수의 출력값을 가장 크게 줄이는 방향**이다.

기울기 : 가중치 매개변수에 대한 손실 함수의 기울기.

- (참고) **np.nditer** 사용법
    
    ```python
    import numpy as np
    x=np.array([[1,2,3],[4,5,6]])
    it=np.nditer(x,flags=['multi_index'],op_flags=['readwrite'])
    while not it.finished:
        print(it.multi_index) #(0,0),(0,1),(0,2),(1,0),(1,1),(1,2)
        print(x[it.multi_index]) # 1,2,3,4,5,6
        it.iternext()
    ```
    
    ```
    (0, 0)
    1
    (0, 1)
    2
    (0, 2)
    3
    (1, 0)
    4
    (1, 1)
    5
    (1, 2)
    6
    ```
    

```python
# 신경망에서 사용할 W(Matrix 형태)의 편미분 행렬을 구하는 함수
# 그레디언트 (편미분 벡터)

def numerical_gradient(f, x):
    h = 1e-4 # 0.0001
    grads = np.zeros_like(x)
    
    it = np.nditer(x, flags=["multi_index"], op_flags=["readwrite"])
        
    while not it.finished:
        idx = it.multi_index
        tmp_val = x[idx]

        x[idx] = tmp_val + h
        fxh1 = f(x) # f(x+h)
        x[idx] = tmp_val - h
        fxh2 = f(x) # f(x-h)
    
        grads[idx] = (fxh1 - fxh2) / (2*h)
        x[idx] = tmp_val
        it.iternext()
        
    return grads
```

```python
def softmax(a):
    c = np.max(a)
    exp_a = np.exp(a - c)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    
    return y
```

```python
class simpleNet:
    def __init__(self):
        self.W = np.random.randn(2, 3) # 정규분포로 초기화
        
    def predict(self, x):
        return np.dot(x, self.W)
    
    def loss(self, x, t):
        z = self.predict(x)
        y = softmax(z)
        loss = cross_entropy_error(y, t)
        return loss
```

```python
x = np.array([0.6, 0.9]) # (2,)
t = np.array([0, 0, 1]) # (3,)

net = simpleNet()
f = lambda w: net.loss(x, t) # f : Loss 함수

dW = numerical_gradient(f, net.W)

dW # net.W를 미분한 기울기 벡터
# array([[ 0.04149708,  0.52301639, -0.56451347],
#        [ 0.06224561,  0.78452459, -0.8467702 ]])
```

# 수치미분 이용 학습

```python
import numpy as np
from dataset.mnist import load_mnist
```

## ****Helper functions****

```python
# 손실함수 (Cross Entropy)
def cross_entropy_error(y, t):
    delta = 1e-7 # 0.0000001
    
    if y.ndim == 1:
        t = t.reshape(1, t.size) 
        y = y.reshape(1, y.size)
        
    batch_size = y.shape[0]
    return -np.sum(t*np.log(y+delta))/batch_size
```

```python
# 신경망에서 사용할 W(Matrix 형태)의 편미분 행렬을 구하는 함수
# 신경망의 기울기 : 그레디언트 (편미분 벡터)
def numerical_gradient(f, x): # x의 shape이 (784, 20) => grads 도 (784, 20)
    h = 1e-4 # 0.0001
    grads = np.zeros_like(x)
    
    it = np.nditer(x, flags=["multi_index"], op_flags=["readwrite"])
        
    while not it.finished:
        idx = it.multi_index
        tmp_val = x[idx]

        x[idx] = tmp_val + h
        fxh1 = f(x) # f(x+h)
        x[idx] = tmp_val - h
        fxh2 = f(x) # f(x-h)
    
        grads[idx] = (fxh1 - fxh2) / (2*h)
        x[idx] = tmp_val
        it.iternext()
        
    return grads
```

```python
# Softmax
def softmax(x):
    if x.ndim == 2:
        x = x.T # 10*100
        x = x - np.max(x, axis=0) # 10*100 - 100 = 10*100
        y = np.exp(x) / np.sum(np.exp(x), axis=0)
        return y.T 

    x = x - np.max(x) # 오버플로 대책
    return np.exp(x) / np.sum(np.exp(x))
```

```python
# Sigmoid
def sigmoid(x):
    return 1/(1+np.exp(-x))
```

## 2층 신경망 구현하기

```python
class TwoLayerNet:
    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):
        # 모델 파라미터 초기화
        # W1.shape (784, 20), b1.shape (20,), W2.shape (20, 10), b2.shape (10,)
        self.params = {}
        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)
        self.params['b1'] = np.zeros(hidden_size)
        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)
        self.params['b2'] = np.zeros(output_size)
        
    def predict(self, x):
        W1, W2 = self.params['W1'], self.params['W2']
        b1, b2 = self.params['b1'], self.params['b2']
        
        a1 = np.dot(x, W1) + b1 # (20,)
        z1 = sigmoid(a1) # (20,)
        a2 = np.dot(z1, W2) + b2 # (10,)
        y = softmax(a2) # (10,)
        return y
        
    def loss(self, x, t):
        y = self.predict(x)
        loss = cross_entropy_error(y, t)
        return loss
        
    def numerical_gradient(self, x, t):
        f = lambda w : self.loss(x, t)
        
        grads = {}
        grads['W1'] = numerical_gradient(f, self.params['W1']) # W1 (784, 20) -> dW (784, 20)
        grads['b1'] = numerical_gradient(f, self.params['b1']) # b1 (20,) -> dv (20,)
        grads['W2'] = numerical_gradient(f, self.params['W2']) # W2 (20, 10) -> dW2 (20, 10)
        grads['b2'] = numerical_gradient(f, self.params['b2']) # b2 (10,) -> db2 (10,)
        
        return grads
        
    def accuracy(self):
        pass
```

```python
(X_train, y_train), (X_test, y_test) = load_mnist(normalize=True, flatten=True, one_hot_label=True)

network = TwoLayerNet(input_size=784, hidden_size=20, output_size=10)

# 하이퍼파라미터
iters_num = 1000
batch_size = 100 # 미니배치 사이즈
learning_rate = 0.1

for i in range(iters_num):
    batch_mask = np.random.choice(60000, 100) # 랜덤하게 뽑은 배치의 인덱스
    x_batch = X_train[batch_mask]
    t_batch = y_train[batch_mask]

    # 1. Gradient 
    grads = network.numerical_gradient(x_batch, t_batch)

    # 2. Gradient Descent (경사하강법 - 모델 파라미터 업데이트)
    for keys in ('W1', 'W2', 'b1', 'b2'):
        # W(new) ← W(old) - (lr * Gradient) : 경사 하강법
        network.params[keys] = network.params[keys] - (learning_rate * grads[keys])

    loss = network.loss(x_batch, t_batch)
    print(i, loss)
```
