> 부트캠프 10주차 2022년 12월 19일

# ****Day and Night Image Classifier****

- Dataset : day/night rgb image, 200 day images, 200 night images
- Goal : 두 종류(day/night)의 이미지에서 day와 night만의 구별된 특징을 찾아 분류
- [Amos Dataset](http://mvrl.cs.uky.edu/datasets/amos/)
- 입력(Input) -> 탐색(Visualize) -> 전처리(Preprocessing) -> 특성추출(Feature Extraction) -> 분류(Classify)-> 테스트(Accuracy)

## ****Import Resources****

```python
import cv2
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

import glob
```

## ****Training and Testing Data****

→ **[glob 함수 사용법](https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=siniphia&logNo=221397012627)**

```python
image_dir_training = "./day_night_images/training"
image_dir_test = "./day_night_images/test"

train_images = glob.glob("./day_night_images/training/*/*.jpg")
test_images = glob.glob("./day_night_images/test/*/*.jpg")
print("for train:", len(train_images), "   for test:", len(test_images))
train_images[:3]

train_day_images = glob.glob("./day_night_images/training/day/*.jpg")
train_night_images = glob.glob("./day_night_images/training/night/*.jpg")
print("for day(train):", len(train_day_images), "   for night(train):", len(train_night_images))

test_day_images = glob.glob("./day_night_images/test/day/*.jpg")
test_night_images = glob.glob("./day_night_images/test/night/*.jpg")
print("for day(test):", len(test_day_images), "   for night(test):", len(test_night_images))
```

## ****탐색(Visualize)****

```python
# 첫 4장 이미지 보기
# for day
figure, axes = plt.subplots(nrows=1, ncols=4, figsize=(22,6))
for i, train_day_path in enumerate(train_day_paths[:4]):
    image = mpimg.imread(train_day_path)
    axes[i].imshow(image)
    axes[i].set_title('day')

# for night
figure, axes = plt.subplots(nrows=1, ncols=4, figsize=(22,6))
for i, train_night_path in enumerate(train_night_paths[:4]):
    image = mpimg.imread(train_night_path)
    axes[i].imshow(image)
    axes[i].set_title('night')
```
![image](https://user-images.githubusercontent.com/106129152/208376325-40e6e59f-9e0b-4cec-bd3d-e3f3fe4566de.png)

![image](https://user-images.githubusercontent.com/106129152/208376364-76fd7c2a-b01e-4360-8d38-2c04e996ce09.png)

```python
# random하게 4장 이미지 보기
# for day
figure, axes = plt.subplots(nrows=1, ncols=4, figsize=(22,6))
for i in range(4):
    rand_i = np.random.randint(0, len(train_day_paths))
    image = mpimg.imread(train_day_paths[rand_i])
    axes[i].imshow(image)
    axes[i].set_title('day')

# for night
figure, axes = plt.subplots(nrows=1, ncols=4, figsize=(22,6))
for i in range(4):
    rand_i = np.random.randint(0, len(train_night_paths))
    image = mpimg.imread(train_night_paths[rand_i])
    axes[i].imshow(image)
    axes[i].set_title('night')
```

![image](https://user-images.githubusercontent.com/106129152/208376411-b3580f61-007a-4a07-ae96-d4fe843b69ed.png)

![image](https://user-images.githubusercontent.com/106129152/208376440-45b63045-a330-4e40-95ca-3bd88f22f81f.png)

## ****전처리(Preprocessing)****

### ****데이터 적재****

```python
import os

# 이미지의 path에 적혀있는 day와 night로 레이블을 저장
def load_dataset(image_dir):
    image_list = []
    label_types = ['day', 'night']
    for label_type in label_types:
        file_paths = glob.glob(os.path.join(image_dir, label_type, '*.jpg'))
        for file_path in file_paths:
            image = mpimg.imread(file_path)
            image_list.append((image, label_type))
    return image_list

train_image_list = load_dataset(image_dir_training)
test_image_list = load_dataset(image_dir_test)
```

```python
print("train len:", len(train_image_list))
print("0번째 train image shape:", train_image_list[0][0].shape)
print("0번째 train image label:", train_image_list[0][1])

# train len: 240
# 0번째 train image shape: (458, 800, 3)
# 0번째 train image label: day
```

`plt.imshow(train_image_list[100][0]) # numpy ndarray`

**![Untitled](12%2019%20OpenCV%2023c47984424a45708db66f32d387972f/Untitled%204.png)**

### ****표준화 (Resize, Label Encoding)****

```python
# 입력 이미지를 resize
# width:1200, height:600
def resize_input(image):
    resized_image = cv2.resize(image, (1200, 600))
    return resized_image
```

```python
# label encoding
# day -> 1
# night -> 0
def lebel_encode(label): # day or night
    val = 0
    if label == 'day':
        val = 1
    else:
        val = 0
    return val
```

```python
# 위의 두 함수를 사용
def preprocess(image_lists):
    preprocessed_list = []
    for image_list in image_lists:
        image = image_list[0] # image
        label = image_list[1] # label

        preprocessed_image = resize_input(image)
        preprocessed_label = label_encode(label)
        preprocessed_list.append((preprocessed_image, preprocessed_label))
    return preprocessed_list
        
preprocessed_train_list = preprocess(train_image_list)
preprocessed_test_list = preprocess(test_image_list)

len(preprocessed_train_list), len(preprocessed_test_list)
# (240, 160)

preprocessed_train_list[10][0].shape, preprocessed_train_list[10][1]
# ((600, 1200, 3), 1)
```

## ****특성추출(Feature Extraction)****

- **밝기 정보 추출**
    
    ```python
    # HSV : H(색상), S(채도), V(밝기)
    index = 137
    sample_image = preprocessed_train_list[index][0]
    sample_label = preprocessed_train_list[index][1]
    
    hsv = cv2.cvtColor(sample_image, cv2.COLOR_RGB2HSV)
    h, s, v = cv2.split(hsv)
    
    figure, axes = plt.subplots(nrows=1, ncols=4, figsize=(22, 6))
    
    # original image
    axes[0].imshow(sample_image)
    axes[0].set_title(sample_label)
    
    # h
    axes[1].imshow(h, cmap='gray')
    axes[1].set_title('h channel')
    # s
    axes[2].imshow(s, cmap='gray')
    axes[2].set_title('s channel')
    #v
    axes[3].imshow(v, cmap='gray')
    axes[3].set_title('v channel')
    ```
    
    ![image](https://user-images.githubusercontent.com/106129152/208376589-454ad03b-5fe2-48b0-90d7-d55f76f9162d.png)
    
    ```python
    # sample image에 대해 평균 밝기값 출력
    v.sum()/v.size
    # 27.685306944444445
    ```
    
    ```python
    # 평균 밝기값을 구하는 함수
    def average_brightness(image):
        hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
        h, s, v = cv2.split(hsv)
        avg_brightness = v.sum()/v.size
        return avg_brightness
    ```
    

## ****분류(Classify)****

- **평균 밝기값을 기준으로 day 또는 night 인지를 예측**
    
    ```python
    def predict_label(image): # 시험해볼 이미지
        avg = average_brightness(image)
        
        threshold = 99
        
        if avg > threshold:
            predicted_label = 1 # day
        else:
            predicted_label = 0 # night
        
        return predicted_label
    ```
    

## ****테스트 (Measure Accuracy)****

```python
wellclassified_list = []
missclassified_list = []
for test_item in preprocessed_test_list:
    image = test_item[0] # image
    label = test_item[1] # label
    
    # 예측 (분류, classify)
    predict = predict_label(image)
    
    if (label == predict):
        wellclassified_list.append((image, predict, label))
    else:
        missclassified_list.append((image, predict, label))
    
# Accuracy(correct/total)
accuracy = len(wellclassified_list)/len(preprocessed_test_list)
accuracy 
# 0.9375
```

### ****잘못 분류된 이미지 시각화 하기****

```python
len(missclassified_list) # 10
```

```python
figure, axes = plt.subplots(nrows=1, ncols=4, figsize=(22, 6))
for i in range(4):
    rand_i = np.random.randint(0, len(missclassified_list))
    image = missclassified_list[rand_i][0]
    predict = missclassified_list[rand_i][1]
    label = missclassified_list[rand_i][2]
    axes[i].imshow(image)
    axes[i].set_title('[predict] : ' + str(predict) + ' [label] : ' + str(label))
    print(i, 'average brightness : ', average_brightness(image))

# 0 average brightness :  106.49033333333334
# 1 average brightness :  176.83882916666667
# 2 average brightness :  108.24727222222222
# 3 average brightness :  106.49033333333334
```

![image](https://user-images.githubusercontent.com/106129152/208376649-44b6de7b-6d62-426a-b2d7-2862c2f98815.png)

# OpenCV - Day6

```python
import cv2
import numpy as np
```

# ****모폴로지 연산****

→ [[OpenCV]모폴로지 연산](https://bkshin.tistory.com/entry/OpenCV-19-%EB%AA%A8%ED%8F%B4%EB%A1%9C%EC%A7%80Morphology-%EC%97%B0%EC%82%B0-%EC%B9%A8%EC%8B%9D-%ED%8C%BD%EC%B0%BD-%EC%97%B4%EB%A6%BC-%EB%8B%AB%ED%9E%98-%EA%B7%B8%EB%A0%88%EB%94%94%EC%96%B8%ED%8A%B8-%ED%83%91%ED%96%87-%EB%B8%94%EB%9E%99%ED%96%87)

모폴로지 연산은 주로 이진 영상에서 객체의 모양 단순화나, 잡음 제거 등의 용도로 사용된다.

## ****침식과 팽창****

침식 연산은 이미지를 깎아내는 것으로, 객체 영역은 축소되고 배경은 확대된다.

반대로 팽창 연산은 객체 영역이 확대되고, 배경 영역이 줄어든다.

```python
src = cv2.imread('./data/milkdrop.bmp', cv2.IMREAD_GRAYSCALE)
thresh, src_bin = cv2.threshold(src, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)

# cv2.erode() : 침식 연산
erode = cv2.erode(src_bin, None)

# cv2.dilate() : 팽창 연산
dilate = cv2.dilate(src_bin, None)

cv2.imshow('bin', src_bin)
cv2.imshow('erode', erode)
cv2.imshow('dilate', dilate)
cv2.waitKey()
cv2.destroyAllWindows()
```

![image](https://user-images.githubusercontent.com/106129152/208376686-3f3fc058-aba7-4988-a88d-cef0c4bc6440.png)

## 열기와 닫기

**열기 연산** : 침식 연산 → 팽창 연산 (작은 객체의 크기가 효과적으로 제거)

**닫기 연산** : 팽창 연산 → 침식 연산 (객체 내부의 작은 구멍들을 제거)

```python
src = cv2.imread('./data/milkdrop.bmp', cv2.IMREAD_GRAYSCALE)
thresh, src_bin = cv2.threshold(src, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)

# 열기 연산
opening = cv2.morphologyEx(src_bin, cv2.MORPH_OPEN, None, iterations = 3)

# 닫기 연산
clossing = cv2.morphologyEx(src_bin, cv2.MORPH_CLOSE, None, iterations = 3)

cv2.imshow('bin', src_bin)
cv2.imshow('opening', opening)
cv2.imshow('clossing', clossing)
cv2.waitKey()
cv2.destroyAllWindows()
```

![image](https://user-images.githubusercontent.com/106129152/208376716-90e82efd-e19e-45cb-b120-540c5c4cddfa.png)

# 레이블링과 외곽선 검출

각각의 객체를 구분하고 분석하는 작업이 필요한데, 이때 레이블링 기법을 사용한다.

입력 영상의 픽셀 값이 0이면 배경, 0이 아니면 객체 픽셀로 인식한다.

## 레이블링 기본

```python
src = np.array([[0, 0, 1, 1, 0, 0, 0, 0],
                [1, 1, 1, 1, 0, 0, 1, 0],
                [1, 1, 1, 1, 0, 0, 0, 0],
                [0, 0, 0, 0, 0, 1, 1, 0],
                [0, 0, 0, 1, 1, 1, 1, 0],
                [0, 0, 0, 1, 0, 0, 1, 0],
                [0, 0, 1, 1, 1, 1, 1, 0],
                [0, 0, 0, 0, 0, 0, 0, 0]]).astype(np.uint8)

src = src * 255
cnt, labels = cv2.connectedComponents(src)

print(cnt)
print(labels)
```

```
4
[[0 0 1 1 0 0 0 0]
 [1 1 1 1 0 0 2 0]
 [1 1 1 1 0 0 0 0]
 [0 0 0 0 0 3 3 0]
 [0 0 0 3 3 3 3 0]
 [0 0 0 3 0 0 3 0]
 [0 0 3 3 3 3 3 0]
 [0 0 0 0 0 0 0 0]]
```

클래스가 4개인 것은 0, 1, 2, 3으로 분류되었기 때문(배경인 0도 포함)

```python
src = cv2.imread('./data/circles.jpg', cv2.IMREAD_GRAYSCALE)
thresh, src_bin = cv2.threshold(src, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)

cnt, labels = cv2.connectedComponents(src_bin)

dst = cv2.cvtColor(src, cv2.COLOR_GRAY2BGR) # 객체별로 색을 다르게 표시할 3차원 도화지 준비

dst[labels == 0] = (0, 255, 255) # 배경 : 노란색
dst[labels == 1] = (0, 0, 255) # 1번 객체 : 빨간색
dst[labels == 2] = (255, 0, 255) # 2번 객체 : 보라색
dst[labels == 3] = (0, 255, 0) # 3번 객체 : 초록색

cv2.imshow('src', src)
cv2.imshow('src_bin', src_bin)
cv2.imshow('dst', dst)
cv2.waitKey()
cv2.destroyAllWindows()
```

![image](https://user-images.githubusercontent.com/106129152/208376763-9f7a5765-f2e8-4559-815d-e20ecce80f4e.png)

## 레이블링 응용

```python
src = cv2.imread('./data/circles.jpg', cv2.IMREAD_GRAYSCALE)
thresh, src_bin = cv2.threshold(src, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)

cnt, labels, stats, centroids = cv2.connectedComponentsWithStats(src_bin)

dst = cv2.cvtColor(src, cv2.COLOR_GRAY2BGR)

for i in range(1, cnt): # 객체 1. 2. 3
    b = np.random.randint(0, 256)
    g = np.random.randint(0, 256)
    r = np.random.randint(0, 256)
    dst[labels == i] = (b, g, r) 
    
    # bounding box -> rectangle
    x, y, width, height, area = stats[i]
    cv2.rectangle(dst, (x, y), (x+width, y+height), (0, 0, 255))
    
    # centroids -> circle
    cx, cy = centroids[i]
    cv2.circle(dst, (int(cx), int(cy)), 5, (255, 0, 0), -1)

cv2.imshow('src', src)
cv2.imshow('src_bin', src_bin)
cv2.imshow('dst', dst)
cv2.waitKey()
cv2.destroyAllWindows()
```

![image](https://user-images.githubusercontent.com/106129152/208376792-52ed6e0c-95dd-4aa1-a024-b08b0e973a1d.png)

```python
src = cv2.imread('./data/keyboard.bmp') # BGR 3 채널 데이터
gray = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)

thresh, src_bin = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)

cnt, labels, stats, centroids = cv2.connectedComponentsWithStats(src_bin)

dst = src.copy() # BGR 3 채널 도화지

for i in range(1, cnt): # 0번 배경은 제회, 1~37번 객체
    x, y, width, height, area = stats[i]
    if area > 20:
        cv2.rectangle(dst, (x, y), (x+width, y+height), (0, 255, 255))
    
cv2.imshow('src', src)
cv2.imshow('src_bin', src_bin)
cv2.imshow('dst', dst)
cv2.waitKey()
cv2.destroyAllWindows()
```

![image](https://user-images.githubusercontent.com/106129152/208376818-b274a761-ec6b-4cbf-96c1-a6bda1b871cc.png)

## 외곽선 검출과 그리기

```python
src = cv2.imread('./data/contours.bmp')
gray = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)
thresh, gray_bin = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY)

dst = src.copy()
contours, hierarchy = cv2.findContours(gray_bin, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

for i in range(len(contours)): # 9 iterations
    b = np.random.randint(0, 256)
    g = np.random.randint(0, 256)
    r = np.random.randint(0, 256)
    cv2.drawContours(dst, contours, i, (b, g, r), 2)

cv2.imshow('src', src)
cv2.imshow('dst', dst)
cv2.waitKey()
cv2.destroyAllWindows()
```

![image](https://user-images.githubusercontent.com/106129152/208376855-478969b0-a6fd-419f-9725-899b6339d889.png)

```python
src = cv2.imread('./data/thumbs_up_down.jpg')
gray = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)

thresh, gray_bin = cv2.threshold(gray, 220, 255, cv2.THRESH_BINARY_INV)

contours, hierarchy = cv2.findContours(gray_bin, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)

dst = src.copy()
cv2.drawContours(dst, contours, -1, (0, 0, 255), 3, cv2.LINE_AA)

cv2.imshow('src', src)
cv2.imshow('gray', gray)
cv2.imshow('gray_bin', gray_bin)
cv2.imshow('dst', dst)
cv2.waitKey()
cv2.destroyAllWindows()
```

![image](https://user-images.githubusercontent.com/106129152/208376878-9d32458f-2b8e-45b4-b41d-269946a984eb.png)

```python
# 왼손만 bounding box 그리기
# left_hand = contours[1] # 왼손을 둘러싸는 점의 좌표들
# x, y, width, height = cv2.boundingRect(left_hand)

# 오른손만 bounding box 그리기
right_hand = contours[0] # 왼손을 둘러싸는 점의 좌표들
x, y, width, height = cv2.boundingRect(right_hand)

dst2 = src.copy()

cv2.rectangle(dst2, (x, y), (x+width, y+height), (0, 255, 255), 3)

cv2.imshow('dst2', dst2)
cv2.waitKey()
cv2.destroyAllWindows()
```

![image](https://user-images.githubusercontent.com/106129152/208376914-0b06b5f4-0811-464a-a865-c9f19956d519.png)

# 객체 검출

## ****캐스케이드 분류기와 얼굴 검출****

**opencv** [https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html](https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html)

**haar cascade classifier** [https://towardsdatascience.com/viola-jones-algorithm-and-haar-cascade-classifier-ee3bfb19f7d8](https://towardsdatascience.com/viola-jones-algorithm-and-haar-cascade-classifier-ee3bfb19f7d8)

[https://webnautes.tistory.com/1352](https://webnautes.tistory.com/1352) (한글 블로그)

**adaboost** [https://www.youtube.com/watch?v=LsK-xG1cLYA](https://www.youtube.com/watch?v=LsK-xG1cLYA) (유튜브)

OpenCV에서 제공하는 얼굴 검출 기능은 캐스케이드 분류기 알고리즘을 기반으로 만들어진 것!

```python
image = cv2.imread('./data/lena.jpg')
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

face_cascade = cv2.CascadeClassifier('./data/haarcascade_frontalface_default.xml')

faces = face_cascade.detectMultiScale(gray)

for face in faces:
    x, y, w, h = face
    cv2.rectangle(image, (x, y), (x+w, y+h), (0, 0, 255), 2)

cv2.imshow('image', image)    
cv2.waitKey()
cv2.destroyAllWindows()
```

![image](https://user-images.githubusercontent.com/106129152/208376932-d7ffc816-d421-48e0-b008-e809a38f0820.png)

```python
image = cv2.imread('./data/kids2.png')
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

face_cascade = cv2.CascadeClassifier('./data/haarcascade_frontalface_default.xml')
eye_cascade = cv2.CascadeClassifier('./data/haarcascade_eye.xml')

faces = face_cascade.detectMultiScale(gray)

for face in faces:
    x, y, w, h = face
    cv2.rectangle(image, (x, y), (x+w, y+h), (0, 0, 255), 2)
    face_rect = image[y:y+h, x:x+w]
    eyes = eye_cascade.detectMultiScale(face_rect)
    
    for eye in eyes:
        x1, y1, w1, h1 = eye
        cv2.rectangle(face_rect, (x1, y1), (x1+w1, y1+h1), (255, 0, 0), 2)
    
cv2.imshow('image', image)
cv2.waitKey()
cv2.destroyAllWindows()
```

![image](https://user-images.githubusercontent.com/106129152/208376967-1dc44891-7142-4d19-a2ed-32b77516c985.png)

**detectMultiScale**

- scaleFactor : 검색 윈도우 확대 비율 (default =1.1)
- minNeighbors : 검출 영역으로 선택하기 위한 최소 검출 횟수 (default = 3)
- minSize : 검출할 객체의 최소크기
- maxSize : 검출할 객체의 최대크기

```python
image = cv2.imread('./data/multi_faces.jpg')
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

face_casecade = cv2.CascadeClassifier('./data/haarcascade_frontalface_default.xml')
#faces = face_casecade.detectMultiScale(gray, 1.3, 8)
#faces = face_casecade.detectMultiScale(gray, minSize=(100, 100), maxSize=(130, 130))
faces = face_casecade.detectMultiScale(gray, 2, 5)

for face in faces:
    x, y, w, h = face
    cv2.rectangle(image, (x, y), (x+w, y+h), (0, 0, 255), 2)

cv2.imshow('image', image)    
cv2.waitKey()
cv2.destroyAllWindows()
```

![image](https://user-images.githubusercontent.com/106129152/208377001-60ce47a0-9a64-4d07-b249-3ee1803569b5.png)

## ****HOG 알고리즘과 보행자 검출****

**HOG Descriptor**
 [https://docs.opencv.org/4.x/d5/d33/structcv_1_1HOGDescriptor.html#a723b95b709cfd3f95cf9e616de988fc8](https://docs.opencv.org/4.x/d5/d33/structcv_1_1HOGDescriptor.html#a723b95b709cfd3f95cf9e616de988fc8)
