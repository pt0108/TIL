> 부트캠프 9주차 2022년 12월 16일

## Lane Finding (차선 인식)

## ****Lane Finding (white lane)****

```python
import cv2
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

image = mpimg.imread('./data/test.jpg')
plt.imshow(image)
```

![image](https://user-images.githubusercontent.com/106129152/208050773-8c62b5d5-d4aa-4f97-8d2e-7ec5b80f725e.png)

### ****Color Selection****

[Color Picker Tool](https://annystudio.com/software/colorpicker/#download)로 해당 이미지 색상의 RGB 코드값 얻어오기 

```python
color_select = np.copy(image)

red_threshold = 230
green_threshold = 230
blue_threshold = 230

color_threshold = ((image[:, :, 0] < red_threshold) | 
                  (image[:, :, 1] < green_threshold) |
                  (image[:, :, 2] < blue_threshold))

color_select[color_threshold] = [0, 0, 0]
plt.imshow(color_select)
```

![image](https://user-images.githubusercontent.com/106129152/208050836-2c17e181-ae91-4fe9-a416-716407baf27e.png)

### ****Region Selection****

```python
region_select = np.copy(image)

left_bottom = [0, 540]
right_bottom = [900, 540]
apex = [475, 320] # 꼭짓점

pts = np.array([left_bottom, right_bottom, apex])
cv2.fillPoly(region_select, [pts], color=[0, 0, 255]) 
plt.imshow(region_select)
```

![image](https://user-images.githubusercontent.com/106129152/208050871-46b65343-6686-4bb2-9d66-7f6dfd62735d.png)

### ****Color and Region Selection****

```python
# 1. Color Selection
color_select = np.copy(image)

red_threshold = 230
green_threshold = 230
blue_threshold = 230

color_threshold = ((image[:, :, 0] < red_threshold) | 
                  (image[:, :, 1] < green_threshold) |
                  (image[:, :, 2] < blue_threshold))

color_select[color_threshold] = [0, 0, 0]

# 2. Region Selection

region_select = np.copy(image)

left_bottom = [0, 540]
right_bottom = [900, 540]
apex = [475, 320] # 꼭짓점

pts = np.array([left_bottom, right_bottom, apex])
cv2.fillPoly(region_select, [pts], color=[0, 0, 255]) 

region_threshold = ((region_select[:, :, 0] == 0) & # R채널
                    (region_select[:, :, 1] == 0) & # G채널
                    (region_select[:, :, 2] == 255)) # B채널

# 3. Color Selection + Region Selection
# color_threshold : 차선(흰색)이 아닌 부분 True 설정
# region_threshold : 관심영역(region of interes, rol)에만 True 설정
lane_select = np.copy(image)
lane_select[~color_threshold & region_threshold] = [255, 0, 0]

plt.imshow(lane_select)
```

![image](https://user-images.githubusercontent.com/106129152/208050907-612b8cf6-929f-475b-a445-b1dbe72defed.png)

## ****Lane Finding (white & yellow lane)****

```python
image = mpimg.imread('./data/exit-ramp.jpg')
plt.imshow(image)
```

![image](https://user-images.githubusercontent.com/106129152/208050927-5c5022f3-c095-46cb-ade8-d0bd79d3f521.png)

### ****step 1 : Gray scale 변환****

```python
gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
plt.imshow(gray, cmap='gray')
```

![image](https://user-images.githubusercontent.com/106129152/208050949-a2797303-9806-4337-977f-acc79082f8f4.png)

### ****step 2 : Gaussian blurring (option)****

```python
kernel_size = 5
blur_gray = cv2.GaussianBlur(gray, (kernel_size, kernel_size), 0)
plt.imshow(blur_gray, cmap='gray')
```

![image](https://user-images.githubusercontent.com/106129152/208050967-f9191f00-4491-4dda-89a3-f746a2710df7.png)

### ****step 3 : Edge detext****

```python
low_threshold = 50
high_threshold = 150 # low:high의 비율은 1:2 or 1:3이 적절
edges = cv2.Canny(blur_gray, low_threshold, high_threshold)
plt.imshow(edges, cmap='gray')
```

![image](https://user-images.githubusercontent.com/106129152/208050993-b96e08eb-1cc8-454b-9717-2ca40365a524.png)

### ****step 4 : ROI selection****

```python
# 관심영역의 mask 준비
height = edges.shape[0]
width = edges.shape[1]

pts = np.array([[0, height-1], [450, 290], [490, 290], [width-1, height-1]])
mask = np.zeros(edges.shape, edges.dtype)
cv2.fillPoly(mask, [pts], 255) 
plt.imshow(mask, cmap='gray')
```

![image](https://user-images.githubusercontent.com/106129152/208051066-97129149-583a-4000-9805-5695bc0aa1cc.png)

```python
# mask와 edge를 bitwise_and
masked_edges = cv2.bitwise_and(edges, mask)
plt.imshow(masked_edges, cmap='gray')
```

![image](https://user-images.githubusercontent.com/106129152/208051101-32293749-c7e7-4412-91f5-e304f9435695.png)

### ****step 5 : Line detect (with hough transform)****

```python
rho = 1 # 1 or 2 : 숫자가 작을수록 더 정밀하게 검출하나 연산시간이 늘어남
theta = np.pi/180 # 라디안 단위
threshold = 30 # 축적배열의 숫자가 높다는 것은 직선을 이루는 점들이 많다는 뜻,
                # 얼마나 큰 값을 직선으로 판단할지는 threshold로 결정
minLineLength = 40 # 검출할 선분의 최소 길이
maxLineGap = 20 # 직선으로 간주할 최대 에지 점 간격

lines = cv2.HoughLinesP(masked_edges, rho, theta, threshold,
                        minLineLength = minLineLength,
                        maxLineGap = maxLineGap)

# (참고) 1차원 데이터를 3차원으로 확장하는 방법
# dst = cv2.cvtColor(masked_edges, cv2.COLOR_GRAY2BGR) # 직선을 그릴 도화지(3채널)
# cv2.merge([masked_edges, masked_edges, masked_edges])
dst = np.dstack((masked_edges, masked_edges, masked_edges)) # 라인을 그릴 도화지

if lines is not None:
    for i in range(len(lines)):
        line = lines[i][0]
        pt1 = line[0], line[1]
        pt2 = line[2], line[3]
        cv2.line(dst, pt1, pt2, (255, 0, 0), 2, cv2.LINE_AA)
plt.imshow(dst)
```

![image](https://user-images.githubusercontent.com/106129152/208051752-c4ba578f-5a7e-41ef-9818-ae6300622ad0.png)

### ****Pipeline (Step 1 ~ Step 5)****

```python
### step 1 : gray scale 변환
gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)

### step 2 : gaussian blurring(option)
kernel_size = 5
blur_gray = cv2.GaussianBlur(gray, (kernel_size, kernel_size), 0)

### step 3 : edge detect 
low_threshold = 50
high_threshold = 150 # low:high의 비율 1:2 or 1:3
edges = cv2.Canny(blur_gray, low_threshold, high_threshold)

### step 4 : roi selection
# 관심영역의 mask 준비
height = edges.shape[0]
width = edges.shape[1]
pts = np.array([[0, height-1], [450, 290], [490, 290], [width-1, height-1]])
mask = np.zeros(edges.shape, edges.dtype)
cv2.fillPoly(mask, [pts], 255)

# 위에서 준비한 mask와 edges를 bitwise_and
masked_edges = cv2.bitwise_and(edges, mask)

### step 5: line detect (with hough transform)
rho = 1 # 1 or 2 : 숫자가 작을수록 더 정밀하게 검출하지만 연산시간이 더 걸림
theta = np.pi/180 # 라디안 단위
threshold = 30 # 축적배열의 숫자가 높다는 것은 직선을 이루는 점들이 많다는 뜻.
                 # 얼마나 큰 값을 직선으로 판단할지는 threshold에 달려있음
minLineLength = 40 # 검출할 선분의 최소길이
maxLineGap = 20 # 직선으로 간주할 최대 에지 점 간격

lines = cv2.HoughLinesP(masked_edges, rho, theta, threshold,
                        minLineLength = minLineLength,
                        maxLineGap = maxLineGap)

# (참고) 1차원 데이터를 3차원으로 확장하는 방법
# dst = cv2.cvtColor(masked_edges, cv2.COLOR_GRAY2BGR) # 직선을 그릴 도화지 (3 채널 도화지)
# dst = cv2.merge([masked_edges, masked_edges, masked_edges])
dst = np.dstack((masked_edges, masked_edges, masked_edges)) # 라인을 그릴 도화지는 3 채널로 준비

if lines is not None:
    for i in range(len(lines)):
        line = lines[i][0]
        pt1 = line[0], line[1]
        pt2 = line[2], line[3]
        cv2.line(dst, pt1, pt2, (255, 0, 0), 2, cv2.LINE_AA)

#result = cv2.add(image, dst)
result = cv2.addWeighted(image, 1.0, dst, 0.8, 0)

plt.imshow(result)
```

![image](https://user-images.githubusercontent.com/106129152/208051810-adb68143-1479-4755-b5e5-665117be4389.png)

## (Exercise) Lane Finding Pipeline

```python
import cv2
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import numpy as np
```

```python
import math

def grayscale(img):
    return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)

    
def canny(img, low_threshold, high_threshold):
    return cv2.Canny(img, low_threshold, high_threshold)

def gaussian_blur(img, kernel_size):
    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)

def region_of_interest(img, vertices):
    mask = np.zeros_like(img)
    
    if len(img.shape) > 2: 
        channel_count = img.shape[2] # 3 or 4 depending on your image
        mask_color = (255,) * channel_count
    else:
        mask_color = 255
        
    cv2.fillPoly(mask, vertices, mask_color)
    
    # mask와 edges bitwise_and
    masked_edges = cv2.bitwise_and(img, mask)
    return masked_edges

def hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap):
    lines = cv2.HoughLinesP(img, rho, theta, threshold, 
                            minLineLength = min_line_len, 
                            maxLineGap = max_line_gap)
    
    line_img = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)
    
    for line in lines:
        for x1,y1,x2,y2 in line:
            cv2.line(line_img, (x1, y1), (x2, y2), [255, 0, 0], 5)
            
    if lines is not None: 
        for i in range(lines.shape[0]) : # lines.shape[0] : 직선의 개수
            pt1 = lines[i][0][0], lines[i][0][1] # 라인의 한쪽끝 x, y 좌표
            pt2 = lines[i][0][2], lines[i][0][3] # 라인의 다른 한쪽끝 x, y 좌표
            cv2.line(line_img, pt1, pt2, (255, 0, 0), 2, cv2.LINE_AA)
            
    return line_img            

def weighted_img(img, initial_img, α=0.8, β=1., γ=0.):
    return cv2.addWeighted(initial_img, α, img, β, γ)
```

```python
def lane_finding(image):
    # Gray scale Image
    gray = grayscale(image)
    
    # Gaussian smoothing
    kernel_size = 5
    blur_gray = gaussian_blur(gray, kernel_size)
    
    # Canny transform
    low_threshold = 50
    high_threshold = 150    
    edges = canny(blur_gray, low_threshold, high_threshold)
    
    # Select ROI
    vertices = np.array([[(50, image.shape[0]), 
                          (image.shape[1]/2 - 45, image.shape[0]/2 + 60), 
                          (image.shape[1]/2 + 45, image.shape[0]/2 + 60), 
                          (image.shape[1]-50, image.shape[0])]], dtype=np.int32)
    
    masked_edges = region_of_interest(edges, vertices)
    
    # Hough Transform
    rho = 1
    theta = np.pi/180
    threshold = 30
    min_line_len = 40
    max_line_gap = 20
    
    line_image = hough_lines(masked_edges, rho, theta, threshold, min_line_len, max_line_gap)
    
    # Draw the line on the original image
    result_image = weighted_img(line_image, image)
        
    return result_image
```

jpg로 테스트 : 

```python
import glob

test_images = glob.glob("./test_images/*.jpg")

for i, test_image in enumerate(test_images):
    image = mpimg.imread(test_image)
    result = lane_finding(image)
    plt.subplot(2, 3, i+1)
    plt.imshow(result)
```

![image](https://user-images.githubusercontent.com/106129152/208051853-664f7a40-602e-4b91-9e2a-3117fef953a6.png)

mp4로 테스트 :

```python
import sys

test_videos = cv2.VideoCapture("./test_videos/solidWhiteRight.mp4")

w = test_videos.get(cv2.CAP_PROP_FRAME_WIDTH)
h = test_videos.get(cv2.CAP_PROP_FRAME_HEIGHT)
fps = test_videos.get(cv2.CAP_PROP_FPS)

fourcc = cv2.VideoWriter_fourcc("D", "I", "V", "X")

# out = cv2.VideoWriter('./test_videos_output/soliWhiteRight_result.mp4', fourcc, fps, (w, h))

delay = round(1000/fps)

while True:
    ret, frame = test_videos.read()
    
    if not ret:
        break
    cv2.imshow("frmae", frame)
    result = lane_finding(frame)
    cv2.imshow("result", result)
    
    key = cv2.waitKey(delay)
    if key == 27: # ESC
        break

if test_videos.isOpened():
    print("test_videos release!")
    test_videos.release()
    
# if out.isOpened():  
#     print("out released")
#     out.release()
            
cv2.destroyAllWindows()
```

## 컬러 영상 다루기

```python
import cv2
import numpy as np
```

### 컬러 영상의 픽셀값 참조

```python
src = cv2.imread('./data/butterfly.jpg', cv2.IMREAD_COLOR)
src.shape # (356, 493, 3)

# 좌상단 (0, 0) 좌표의 값
b = src[0, 0, 0] # B channel
g = src[0, 0, 1] # G channel
r = src[0, 0, 2] # R channel
b, g, r
# (47, 88, 50)

cv2.imshow('src', src)
cv2.waitKey()
cv2.destroyAllWindows()
```

- 참고
    
    matplotlib의 디폴트 백엔드는 inline.
    
    이걸 qt 로 설정하면 픽셀의 좌표와 RGB가 표시된다.
    
    ```python
    import matplotlib.pyplot as plt
    
    # matplotlib의 backend를 qt로 사용하면 픽셀의 좌표와 RGB가 표시됨
    %matplotlib qt
    plt.imshow(src)
    ```
    
    ![image](https://user-images.githubusercontent.com/106129152/208051917-b422a342-8eda-4d40-9652-2e3eef075fd2.png)
    

### 컬러 영상의 픽셀값 반전

```python
src = cv2.imread('./data/butterfly.jpg', cv2.IMREAD_COLOR)

dst = src.copy()

# option 1
dst = 255 - src

# option 2
# dst[:, :, 0] = 255 - stc[:, :, 0]
# dst[:, :, 1] = 255 - stc[:, :, 1]
# dst[:, :, 2] = 255 - stc[:, :, 2]

cv2.imshow('src', src)
cv2.imshow('dst', dst)
cv2.waitKey()
cv2.destroyAllWindows()
```

![image](https://user-images.githubusercontent.com/106129152/208051959-cdf616dd-5781-443e-9eb8-d8a1e6615a80.png)

### 색공간 변환

컬러 영상을 그레이스케일 영상으로 변환하는 이유는 연산 속도와 메모리 사용량을 줄이기 위함! → 컬러 영상은 3배 더 많은 메모리를 필요로 하기 때문. 

```python
src = cv2.imread('./data/butterfly.jpg', cv2.IMREAD_COLOR)
gray = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY) # 색공간 변환

cv2.imshow('src', src)
cv2.imshow('gray', gray)
cv2.waitKey()
cv2.destroyAllWindows()
```

![image](https://user-images.githubusercontent.com/106129152/208051991-a34f2203-f05f-4a22-83d1-ac395eeeedb4.png)

### 색상 채널 나누기

**BGR -> HSV**

HSV는 색상(Hue), 채도(Saturation), 명도(Value)로 색을 표현하는 방식이다.

```python
src = cv2.imread('./data/butterfly.jpg', cv2.IMREAD_COLOR)
hsv = cv2.cvtColor(src, cv2.COLOR_BGR2HSV)

h, s, v = cv2.split(hsv)

cv2.imshow('src', src)
cv2.imshow('h', h)
cv2.imshow('s', s)
cv2.imshow('v', v)
cv2.waitKey()
cv2.destroyAllWindows()
```

![image](https://user-images.githubusercontent.com/106129152/208052027-038cdf78-4a11-4965-8e05-97d7e5ee929d.png)

**BGR -> YCRCB**

YCrCb에서 Y는 밝기 또는 휘도(luminance), Cr과 Cb는 색생 또는 색차(chrominance) 정보.

영상을 그레이스케일 정보와 색상 정보로 분리하여 처리할 때 유용하다.

```python
src = cv2.imread('./data/butterfly.jpg', cv2.IMREAD_COLOR)
yCrCb = cv2.cvtColor(src, cv2.COLOR_BGR2YCrCb)

y, Cr, Cb = cv2.split(yCrCb)

cv2.imshow('src', src)
cv2.imshow('y', y)
cv2.imshow('Cr', Cr)
cv2.imshow('Cb', Cb)
cv2.waitKey()
cv2.destroyAllWindows()
```

![image](https://user-images.githubusercontent.com/106129152/208052097-0116af49-bfee-4823-bb11-0ebbc78f923d.png)

```python
src = cv2.imread('./data/candies.png', cv2.IMREAD_COLOR)

b, g, r= cv2.split(src)

cv2.imshow('src', src)
cv2.imshow('b', b)
cv2.imshow('g', g)
cv2.imshow('r', r)
cv2.waitKey()
cv2.destroyAllWindows()
```

![image](https://user-images.githubusercontent.com/106129152/208052131-40b55279-f710-4342-a9a7-821f7b30515a.png)

## 컬러 영상 처리 기법

### 컬러 히스토그램 평활화

```python
# 명암비를 조정한다는것은 "밝기"값하고만 상관이 있음
src = cv2.imread("./data/pepper.bmp")
src_yCrCb = cv2.cvtColor(src, cv2.COLOR_BGR2YCrCb) # YCrCb : Y(밝기 정보), Cr(붉은색 색상정보), Cb(푸른색 색상정보)

y, Cr, Cb = cv2.split(src_yCrCb)

y_equalized = cv2.equalizeHist(y) # 밝기 정보만을 담고 있는 y 채널에 대해서만 평활화를 수행

dst_yCrCb = cv2.merge([y_equalized, Cr, Cb])

dst = cv2.cvtColor(dst_yCrCb, cv2.COLOR_YCrCb2BGR)

cv2.imshow("src", src)
cv2.imshow("dst", dst)
cv2.waitKey()
cv2.destroyAllWindows()
```

![image](https://user-images.githubusercontent.com/106129152/208052170-90675b93-beca-4788-b984-02f65305f900.png)

### 색상 범위 지정에 의한 영역 분할

**B, G, R 값의 구간 조정하여 영역 분할**

```python
def on_level_change(pos):
    lower_b = cv2.getTrackbarPos('lower b', 'dst')
    upper_b = cv2.getTrackbarPos('upper b', 'dst')
    lower_g = cv2.getTrackbarPos('lower g', 'dst')
    upper_g = cv2.getTrackbarPos('upper g', 'dst')
    lower_r = cv2.getTrackbarPos('lower r', 'dst')
    upper_r = cv2.getTrackbarPos('upper r', 'dst')
    
    # blue m&m (rgb) : 0, 114, 236
    # lower = (220, 100, 0)
    # upper = (255, 120, 10)
    
    lower = (lower_b, lower_g, lower_r)
    upper = (upper_b, upper_g, upper_r)
    
    dst = cv2.inRange(src, lower, upper) # threshold의 color 버전이라고 생각하면 됨
    cv2.imshow("dst", dst)

src = cv2.imread('./data/candies.png')

cv2.imshow("src", src)
cv2.imshow("dst", dst)

cv2.createTrackbar('lower b', 'dst', 0, 255, on_level_change)
cv2.createTrackbar('upper b', 'dst', 0, 255, on_level_change)

cv2.createTrackbar('lower g', 'dst', 0, 255, on_level_change)
cv2.createTrackbar('upper g', 'dst', 0, 255, on_level_change)

cv2.createTrackbar('lower r', 'dst', 0, 255, on_level_change)
cv2.createTrackbar('upper r', 'dst', 0, 255, on_level_change)

cv2.waitKey()
cv2.destroyAllWindows()
```

![image](https://user-images.githubusercontent.com/106129152/208052201-8b58773b-7254-4ae7-9ca9-44141e8e14e4.png)

**H, S, V 값의 구간 조정하여 영역 분할**

```python
# 초록색 m&m 초콜릿 영역만 분할
# hsv

def on_level_change(pos):
    lower_h = cv2.getTrackbarPos('lower h', 'dst')
    upper_h = cv2.getTrackbarPos('upper h', 'dst')
    lower_s = cv2.getTrackbarPos('lower s', 'dst')
    upper_s = cv2.getTrackbarPos('upper s', 'dst')
    lower_v = cv2.getTrackbarPos('lower v', 'dst')
    upper_v = cv2.getTrackbarPos('upper v', 'dst')
    
    
    lower = (lower_h, lower_s, lower_v)
    upper = (upper_h, upper_s, upper_v)
    
    dst = cv2.inRange(hsv, lower, upper)
    cv2.imshow("dst", dst)

src = cv2.imread('./data/candies.png')
hsv = cv2.cvtColor(src, cv2.COLOR_BGR2HSV)

cv2.imshow("src", src)
cv2.imshow("dst", dst)

cv2.createTrackbar('lower h', 'dst', 0, 255, on_level_change)
cv2.createTrackbar('upper h', 'dst', 0, 255, on_level_change)

cv2.createTrackbar('lower s', 'dst', 0, 255, on_level_change)
cv2.createTrackbar('upper s', 'dst', 0, 255, on_level_change)

cv2.createTrackbar('lower v', 'dst', 0, 255, on_level_change)
cv2.createTrackbar('upper v', 'dst', 0, 255, on_level_change)

cv2.waitKey()
cv2.destroyAllWindows()
```

![image](https://user-images.githubusercontent.com/106129152/208052225-5aacc3b6-0bd2-4c40-9b6a-419a4c27b80a.png)

### 히스토그램 역투영

```python
ref = cv2.imread('./data/ref.png')
mask = cv2.imread('./data/mask.bmp', cv2.IMREAD_GRAYSCALE)
src = cv2.imread('./data/kids.png')

ref_yCrCb = cv2.cvtColor(ref, cv2.COLOR_BGR2YCrCb)
src_yCrCb = cv2.cvtColor(src, cv2.COLOR_BGR2YCrCb)

channels = [1, 2] # Cr, Cb channel
histSize = [128, 128]
ranges = [0, 256] + [0, 256]

hist = cv2.calcHist(images = [ref_yCrCb], channels=channels, mask=mask, histSize=histSize, ranges=ranges)
# hist.shape : 128, 128

back_proj = cv2.calcBackProject([src_yCrCb], channels, hist, ranges, scale=1)

cv2.imshow('ref', ref)
cv2.imshow('mask', mask)
cv2.imshow('src', src)
cv2.imshow('back_proj', back_proj)
cv2.waitKey()
cv2.destroyAllWindows()
```

![image](https://user-images.githubusercontent.com/106129152/208052252-4a709a48-d76a-4de0-bb58-c1bfbba7d77c.png)

## 영상의 이진화

### 전역 이진화

**[Otsu 알고리즘](https://deep-learning-study.tistory.com/224)**에 의한 임계값 사용

자동으로 영상을 이진화한다고 보면 된다!

```python
src = cv2.imread('./data/neutrophils.png', cv2.IMREAD_GRAYSCALE)

thresh, dst = cv2.threshold(src, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)

cv2.imshow("src", src)
cv2.imshow("dst", dst)
cv2.waitKey()
cv2.destroyAllWindows()
```

![image](https://user-images.githubusercontent.com/106129152/208052336-12a8a64f-14b5-433e-a264-efc7e866526f.png)

```python
src = cv2.imread('./data/Heart10.jpg', cv2.IMREAD_GRAYSCALE)

thresh, dst = cv2.threshold(src, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)

cv2.imshow("src", src)
cv2.imshow("dst", dst)
cv2.waitKey()
cv2.destroyAllWindows()
```

![image](https://user-images.githubusercontent.com/106129152/208052354-e62f4e0b-2680-40c2-95cf-7d7ac70fe003.png)

### 적응형 이진화

전역 이진화를 했을 때(Otsu) :

```python
src = cv2.imread('./data/sudoku.jpg', cv2.IMREAD_GRAYSCALE)
thresh, dst = cv2.threshold(src, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)

cv2.imshow("src", src)
cv2.imshow("dst", dst)
cv2.waitKey()
cv2.destroyAllWindows()
```

![image](https://user-images.githubusercontent.com/106129152/208052380-25f9db63-7873-4f72-9331-b594a386ad6f.png)

적응형 이진화를 했을 때 :

```python
def on_trackbar(pos):
    
    b_size = pos
    if b_size % 2 == 0 : #짝수
        b_size += 1
    if b_size < 3 :
        b_size = 3    
    
    C = 5

    dst = cv2.adaptiveThreshold(src, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, b_size, C)
    cv2.imshow('dst', dst)

src = cv2.imread('./data/sudoku.jpg', cv2.IMREAD_GRAYSCALE)

cv2.imshow("src", src)
cv2.imshow("dst", dst)

cv2.createTrackbar('block size', 'dst', 0, 200, on_trackbar)
cv2.setTrackbarPos('block size', 'dst', 11)

cv2.waitKey()
cv2.destroyAllWindows()
```

![image](https://user-images.githubusercontent.com/106129152/208052417-ddceeec5-d4c5-487f-8960-edece67791d2.png)
