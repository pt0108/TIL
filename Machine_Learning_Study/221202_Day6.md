> 부트캠프 7주차 2022년 12월 2일

## 캘리포니아 주택 가격 예측

### 캘리포니아 주택 가격 예측 모델 만들기

캘리포니아 인구조사 데이터를 사용해 캘리포니아 주택 가격 예측 모델 만들기

레이블된 샘플이 있으므로 지도학습

연속된 값을 예측하고, 여러 특성을 사용하므로 다중 회귀

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import sklearn
```

### 1. 데이터 가져오기

```python
housing = pd.read_csv('./datasets/housing.csv')
```

### 2. 데이터 훑어보기

```python
housing.head()
```

![image](https://user-images.githubusercontent.com/106129152/205238589-9252eb38-7ec7-4280-8ccb-405be4c147ce.png)

```python
housing.info()
```

```
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 20640 entries, 0 to 20639
Data columns (total 10 columns):
 #   Column              Non-Null Count  Dtype
---  ------              --------------  -----
 0   longitude           20640 non-null  float64
 1   latitude            20640 non-null  float64
 2   housing_median_age  20640 non-null  float64
 3   total_rooms         20640 non-null  float64
 4   total_bedrooms      20433 non-null  float64
 5   population          20640 non-null  float64
 6   households          20640 non-null  float64
 7   median_income       20640 non-null  float64
 8   median_house_value  20640 non-null  float64
 9   ocean_proximity     20640 non-null  object
dtypes: float64(9), object(1)
memory usage: 1.6+ MB
```

**범주형 특성 탐색**

```python
housing['ocean_proximity'].value_counts()
```

```
<1H OCEAN     9136
INLAND        6551
NEAR OCEAN    2658
NEAR BAY      2290
ISLAND           5
Name: ocean_proximity, dtype: int64
```

**수치형 특성 탐색**

```python
housing.describe()
```

![image](https://user-images.githubusercontent.com/106129152/205238622-e81b62ac-dd28-402e-99d1-36a4e410bc65.png)

**수치형 특성별 히스토그램**

```python
h = housing.hist(bins=50, figsize=(20, 15))
```

![image](https://user-images.githubusercontent.com/106129152/205238642-5bbe77dd-4162-42d8-a634-9e5b3679bd29.png)


### 3. 데이터 세트 분리

- 훈련 데이터 / 테스트 데이터
    
    **계층적 샘플링(Straityfied sampling)**
    
    ```python
    bins = [0, 1.5, 3.0, 4.5, 6.0, np.inf]
    labels = [1, 2, 3, 4, 5]
    housing['income_cat'] = pd.cut(housing['median_income'], bins=bins, labels=labels)
    
    housing['income_cat'].value_counts() # 도수
    # 3    7236
    # 2    6581
    # 4    3639
    # 5    2362
    # 1     822
    # Name: income_cat, dtype: int64
    
    housing['income_cat'].value_counts() / len(housing) # 위의 값이 차지하는 비율
    # 3    0.350581
    # 2    0.318847
    # 4    0.176308
    # 5    0.114438
    #1    0.039826
    # Name: income_cat, dtype: float64
    ```
    
    ```python
    from sklearn.model_selection import train_test_split
    
    strat_train_set, strat_test_set = train_test_split(housing, stratify=housing['income_cat'], test_size=0.2, random_state=42)
    ```
    
    **데이터 되돌리기**
    
    ```python
    strat_train_set.drop('income_cat', axis=1, inplace=True)
    strat_test_set.drop('income_cat', axis=1, inplace=True)
    ```
    

### 4. 데이터 탐색

**탐색적 데이터분석**

- 어떤 특성을 선택할지, 제거할지, 조합을 통해서 더 좋은 특성을 만들 수 있는지를 분석
- 시각화(데이터의 경향성, 왜곡, 특잇값, 통계적정보)
- 상관관계, 누락데이터(결측치)를 종합적으로 분석
- 탐색적 데이터 분석의 결과로 모델링 방향을 결정할 수 있음

```python
# 훈련셋만 대상으로 탐색과 시각화 적용 (strat_test_set는 최종 예측에 사용)
housing = strat_train_set.copy()
```

**4.1 지리적 데이터 시각화**

```python
# longitude(경도) : 동서
# latitude(위도) : 남북
housing.plot(kind='scatter', x = 'longitude', y = 'latitude', alpha=0.3, grid=True)
```

![image](https://user-images.githubusercontent.com/106129152/205238716-2a1894d1-196e-441e-81cf-cc5d1e70eb96.png)

```python
housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.8, grid=True,
             c='median_house_value', cmap='jet', colorbar=True, figsize=(10, 7),
             s=housing['population']/50, sharex=False)
```

![image](https://user-images.githubusercontent.com/106129152/205238745-e23b166a-4927-49ff-9dc3-62240a3d21ce.png)

→ 지리적 데이터 분석 결과 ****: 해안가이면서 밀집 지역일수록 주택가격이 높음

**4.2 상관관계 조사**

- 상관계수
    
    ```python
    # 모든 수치형 특성간의 상관계수 확인
    corr_metrix = housing.corr()
    corr_metrix
    ```
    
    ![image](https://user-images.githubusercontent.com/106129152/205238777-0cd1da4a-2a62-469f-bf2d-fc6f7c2684d4.png)
    
    ```python
    # 중간 주택 가격(타겟)과 다른 특성간의 상관관계
    corr_metrix['median_house_value'].sort_values(ascending=False)
    ```
    
    ```
    median_house_value    1.000000
    median_income         0.687151
    total_rooms           0.135140
    housing_median_age    0.114146
    households            0.064590
    total_bedrooms        0.047781
    population           -0.026882
    longitude            -0.047466
    latitude             -0.142673
    Name: median_house_value, dtype: float64
    ```
    
- 산점도
    
    ```python
    attributes = ['median_house_value', 'median_income', 'total_rooms', 'housing_median_age']
    
    obj = pd.plotting.scatter_matrix(housing[attributes], figsize=(12, 8))
    ```
    
    ![image](https://user-images.githubusercontent.com/106129152/205238811-ac67c12a-6fa3-4256-82b8-f06f67cbca69.png)
    
    ```python
    # 중간 주택 가격(타겟)과 중간소득의 산점도
    housing.plot(kind='scatter', x='median_income', y='median_house_value', alpha=0.1, grid=True)
    ```
    
    ![image](https://user-images.githubusercontent.com/106129152/205238831-d0f8ac98-fb1c-47e3-8537-a4c12fcfba92.png)
    

**4.3 특성 조합을 실험**

```python
housing['rooms_per_household'] = housing['total_rooms']/housing['households']
housing['bedrooms_per_rooms'] = housing['total_bedrooms']/housing['total_rooms']
housing['population_per_hosehold'] = housing['population']/housing['households']
```

```python
corr_matrix = housing.corr()
corr_matrix['median_house_value'].sort_values(ascending=False)
```

```
median_house_value         1.000000
median_income              0.687151
rooms_per_household        0.146255
total_rooms                0.135140
housing_median_age         0.114146
households                 0.064590
total_bedrooms             0.047781
population_per_hosehold   -0.021991
population                -0.026882
longitude                 -0.047466
latitude                  -0.142673
bedrooms_per_rooms        -0.259952
Name: median_house_value, dtype: float64
```

### 5. 데이터 전처리

```python
# strat_train_set (시각화, 전처리)
# strat_test_set (최종 예측)

# 특성과 레이블을 분리
housing = strat_train_set.drop('median_house_value', axis=1) # 특성 (X 데이터)
housing_label = strat_train_set['median_house_value'].copy # 레이블 (y 데이터)
```

**5.1 데이터 전처리(1) - 결손값 처리**

**결손값 처리 방법**

- 옵션1 : 해당 구역 제거
- 옵션2 : 전체 특성 삭제
- 옵션3 : 어떤 값으로 대체(0, 평균, 중간값 등)

**scikit-learn의 전처리기를 이용하여 옵션 3을 처리**

```python
housing_num = housing.drop('ocean_proximity', axis=1)
# housing_num = housing.select_dtypes(include=[np.number])

# SimpleImputer를 결측값 대체(옵션3) 가능
from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy='median') # 변환기 객체 생성
imputer.fit(housing_num) # 변환 준비 : 중앙값을 구함

X = imputer.transform(housing_num) # 실제 변환
```

**5.2 데이터 전처리(2) - 데이터 인코딩**

→ 데이터 인코딩 이유 : 머신러닝에서 수치값만 기대하기 때문

```python
housing_cat = housing[['ocean_proximity']] # 2차원의 dataframe으로 준비
```

(1) 레이블 인코딩

```python
# pandas
pd.factorize(housing['ocean_proximity'])
# (array([0, 1, 0, ..., 2, 2, 0], dtype=int64),
#  Index(['INLAND', 'NEAR OCEAN', '<1H OCEAN', 'NEAR BAY', 'ISLAND'], dtype='object'))
```

```python
# sklearn 변환기
from sklearn.preprocessing import OrdinalEncoder # LabelEncoder는 1차원 데이터를 기대

ordinal_encoder = OrdinalEncoder()
ordinal_encoder.fit_transform(housing_cat)
# array([[1.],
#        [4.],
#        [1.],
#       ...,
#       [0.],
#       [0.],
#       [1.]])
```

(2) **원핫 인코딩**

→ 숫자의 크기가 모델 훈련과정에서 잘못된 영향을 줄 수 있으므로 원핫 인코딩 사용

```python
# pandas
pd.get_dummies(housing_cat)
```

![image](https://user-images.githubusercontent.com/106129152/205238903-b02053fa-53c8-4393-94c9-09eb5d5254fe.png)

```python
# sklearn 변환기
from sklearn.preprocessing import OneHotEncoder

onehot_encoder = OneHotEncoder(sparse=False)
onehot_encoder.fit_transform(housing_cat)
# array([[0., 1., 0., 0., 0.],
#        [0., 0., 0., 0., 1.],
#        [0., 1., 0., 0., 0.],
#       ...,
#       [1., 0., 0., 0., 0.],
#       [1., 0., 0., 0., 0.],
#       [0., 1., 0., 0., 0.]])

onehot_encoder.categories_
# [array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],
#        dtype=object)]
```

**5.3 데이터 전처리(3) - 특성 스케일링**

```python
arr = np.arange(9).reshape(3, 3)

Z_arr = (arr - arr.mean())/arr.std() # 표준화 공식
Z_arr.mean(), Z_arr.std()
# (0.0, 1.0)

M_arr = (arr - arr.min())/(arr.max()-arr.min()) # 정규화 공식 (0~1)
M_arr.min(), M_arr.max()
# (0.0, 1.0)
```

```python
# 표준화 (Z score Standardize) : 평균 0, 표준편차 1
from sklearn.preprocessing import StandardScaler

std_scaler = StandardScaler()
housing_num_std = std_scaler.fit_transform(housing_num)
housing_num_std.mean(0), housing_num_std.std(0) # 0번축으로 각각의 컬럼별 평균/표준편차

# (array([-5.24924634e-15,  2.81159678e-16,  8.77850764e-17, -1.54914841e-17,
#                     nan,  6.45478503e-19, -1.05428155e-17,  1.14841384e-16]),
#  array([ 1.,  1.,  1.,  1., nan,  1.,  1.,  1.]))
```

```python
# 정규화 (Min Max Sacling) : 0~1 사이로 정규화
from sklearn.preprocessing import MinMaxScaler

min_max_scaler = MinMaxScaler(feature_range=(0, 1)) # feature_range=(0, 1) 가 기본값
housing_num_mm = min_max_scaler.fit_transform(housing_num)
housing_num_mm.min(0), housing_num_mm.max(0)

# (array([ 0.,  0.,  0.,  0., nan,  0.,  0.,  0.]),
# array([ 1.,  1.,  1.,  1., nan,  1.,  1.,  1.]))
```

```python
# 로그 스케일링 : 데이터의 분포가 왜곡되어 있을때 주로 사용
from sklearn.preprocessing import FunctionTransformer

log_transformer = FunctionTransformer(np.log) # np.log : 넘파이의 로그 함수
log_population = log_transformer.fit_transform(housing_num[['population']])
```

`log_population.hist(bins=50)` 로그 변환 후 정규분포 형태로 바뀜

![image](https://user-images.githubusercontent.com/106129152/205238951-b2f7de54-1189-4d6c-8224-ae5273d87904.png)

**5.4 데이터 전처리(4) - 변환 파이프라인**

```python
# 범주형 데이터를 위한 파이프라인 -> 원핫 인코딩

# 수치형 데이터를 위한 파이프라인
# 누락된 데이터 중앙값으로 대체 -> 표준화

from sklearn.pipeline import Pipeline

num_pipline = Pipeline([('imputer', SimpleImputer(strategy='median')),
                       ('std_scaler', StandardScaler())
                       ])
```

```python
num_attribs = list(housing_num.columns.values)
cat_attribs = ['ocean_proximity']

from sklearn.compose import ColumnTransformer

full_pipeline = ColumnTransformer([('num_pipeline', num_pipeline, num_attribs),
                                   ('onehot_encoder', OneHotEncoder(), cat_attribs)])

housing_prepared = full_pipeline.fit_transform(housing)
```

기존 사이즈와 전처리 후 사이즈를 비교해보면, 원핫 인코딩을 거치면서 컬럼이 늘어난 것을 확인할 수 있음!

```python
housing.shape # 기존 사이즈
# (16512, 9)

housing_prepared.shape # 원핫 인코딩을 거치면서 컬럼이 늘어났음
# (16512, 13)
```

### 6. 모델 선택과 훈련

```python
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score
```

```python
lin_reg = LinearRegression()
tree_reg = DecisionTreeRegressor(random_state=42)
rf_reg = RandomForestRegressor(random_state=42)

# cross_val_score(모델, 특성, 정답, 성능측정지표, 폴드수)
lin_scores = cross_val_score(lin_reg, housing_prepared, housing_label, scoring="neg_mean_squared_error", cv=10)
lin_rmse = np.sqrt(-lin_scores).mean()
lin_rmse # 69204.32275494766

tree_scores = cross_val_score(tree_reg, housing_prepared, housing_label, scoring="neg_mean_squared_error", cv=10)
tree_rmse = np.sqrt(-tree_scores).mean()
tree_rmse # 69081.361562518

rf_scores = cross_val_score(rf_reg, housing_prepared, housing_label, scoring="neg_mean_squared_error", cv=10, n_jobs=-1)
rf_rmse = np.sqrt(-rf_scores).mean()
rf_rmse # 49432.12678796127

```

### 7. 모델 세부 튜닝

**그리드 탐색**

```python
from sklearn.model_selection import GridSearchCV

rf_reg = RandomForestRegressor(random_state=42)

param_grid = {'n_estimators':[30, 50, 100], 'max_features':[2, 4, 6, 8]} 

# GridSearchCV(모델, 탐색할파라미터, 성능측정기준, 폴드수)
grid_search = GridSearchCV(rf_reg, param_grid, scoring='neg_mean_squared_error', cv=5, n_jobs=-1)
%time grid_search.fit(housing_prepared, housing_label)
```

```python
grid_search.best_params_
# {'max_features': 8, 'n_estimators': 100}

grid_search.best_estimator_
```

```python
for mean_score, params in zip(grid_search.cv_results_['mean_test_score'], grid_search.cv_results_['params']):
    print(np.sqrt(-mean_score), params)
```

```
52673.5498401615 {'max_features': 2, 'n_estimators': 30}
52071.46113915598 {'max_features': 2, 'n_estimators': 50}
51527.67198141185 {'max_features': 2, 'n_estimators': 100}
50370.55528306362 {'max_features': 4, 'n_estimators': 30}
49981.14659922965 {'max_features': 4, 'n_estimators': 50}
49582.79646511731 {'max_features': 4, 'n_estimators': 100}
50177.91173851986 {'max_features': 6, 'n_estimators': 30}
49655.44617680634 {'max_features': 6, 'n_estimators': 50}
49366.1574509085 {'max_features': 6, 'n_estimators': 100}
49941.11534754462 {'max_features': 8, 'n_estimators': 30}
49485.628343968834 {'max_features': 8, 'n_estimators': 50}
49160.66522746081 {'max_features': 8, 'n_estimators': 100}
```

**랜덤 탐색**

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

param_distribs = {'n_estimators' : randint(low=1, high=200), 'max_features' : randint(low=1, high=8)}

# RandomizedSearchCV(모델, 탐색할파라미터, 반복횟수, 성능측정기준, 폴드수)
rnd_search = RandomizedSearchCV(rf_reg, param_distribs, n_iter=10, scoring='neg_mean_squared_error', cv=5, n_jobs=-1) # 10*5
%time rnd_search.fit(housing_prepared, housing_label)
```

```python
rnd_search.best_params_
# {'max_features': 6, 'n_estimators': 129)

best_model = rnd_search.best_estimator_
```

```python
for mean_score, params in zip(rnd_search.cv_results_['mean_test_score'], rnd_search.cv_results_['params']):
    print(np.sqrt(-mean_score), params)
```

```
50201.32746120488 {'max_features': 3, 'n_estimators': 148}
49579.12987294769 {'max_features': 5, 'n_estimators': 59}
51266.16837130533 {'max_features': 2, 'n_estimators': 152}
49474.51397802696 {'max_features': 4, 'n_estimators': 166}
51835.75571780794 {'max_features': 2, 'n_estimators': 64}
49493.72258965925 {'max_features': 4, 'n_estimators': 160}
50200.86929109876 {'max_features': 3, 'n_estimators': 154}
62171.74914682597 {'max_features': 3, 'n_estimators': 3}
49503.791099645554 {'max_features': 4, 'n_estimators': 157}
49257.573900749674 {'max_features': 6, 'n_estimators': 129}
```

### 8. 모델 예측과 성능 평가

```python
X_test = strat_test_set.drop('median_house_value', axis=1)
y_test = strat_test_set['median_house_value']
X_test.shape, y_test.shape # ((4128, 9), (4128,))

# 훈련데이터를 변경할 때는 파이프라인의 fit_transform을 사용
# 테스트데이터를 변경할 때는 파이프라인의 transform 사용

X_test_prepared = full_pipeline.transform(X_test)
X_test_prepared.shape # (4128, 13)
```

```python
final_predictions = best_model.predict(X_test_prepared)

from sklearn.metrics import mean_squared_error

final_rmse = mean_squared_error(y_test, final_predictions, squared=False) # RMSE
final_rmse 
# 46645.41759186354
```

```python
# 파이프라인까지 모두 저장
full_pipeline_with_predictor = Pipeline([('preparation', full_pipeline),
                                    ('final', best_model)])
```

### 9. 모델 저장

```python
import joblib

# 모델 저장
joblib.dump(full_pipeline_with_predictor, 'my_model.pkl')

# 모델을 불러오고, 정상 작동하는지 확인
loaded_model = joblib.load('my_model.pkl')
final_predictions2 = loaded_model.predict(X_test)
# array([489292.30232558, 201498.47286822, 210089.93023256, ...,
#        332569.12403101, 277024.06976744, 225708.52713178])

mean_squared_error(y_test, final_predictions2, squared=False).
# 46508.02771914554
```
