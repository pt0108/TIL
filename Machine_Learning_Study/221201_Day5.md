> 부트캠프 7주차 2022년 12월 1일 

## 앙상블 학습

→ [앙상블의 기본 개념](https://tyami.github.io/machine%20learning/ensemble-1-basics/), [앙상블 학습](https://velog.io/@dbj2000/ML%EC%95%99%EC%83%81%EB%B8%94-%ED%95%99%EC%8A%B5-Random-Forest-GBM), [앙상블 모형 이론 및 실습](https://velog.io/@changhtun1/ensemble)

여러 개의 결정 트리(Decision Tree)를 결합하여 하나의 결정 트리보다 더 좋은 성능을 내는 머신러닝 기법

*(아래 실습 코드들은 약식으로, 교차검증은 생략했다)*

### **1. 투표기반 분류기**

분류기(Classifier)가 서로 독립적일 때, 최고의 성능을 발휘한다. 

매우 다른 종류의 오차를 만들어 앙상블 모델의 정확도를 향상시킨다.

일반적으로 하드보팅보다 소프트보팅의 예측 성능이 상대적으로 우수하다.

```python
from sklearn.datasets import make_moons
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split

from sklearn.ensemble import VotingClassifier

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
```

```python
X, y = make_moons(n_samples=500, noise=0.3, random_state=42)

X.shape, y.shape # X의 특성은 2개
# ((500, 2), (500,))

plt.plot(X[:, 0][y == 0], X[:, 1][y == 0], 'b.') # 정답이 0인 경우 blue
plt.plot(X[:, 0][y == 1], X[:, 1][y == 1], 'r.') # 정답이 1인 경우 red
plt.xlabel('X1')
plt.ylabel('X2', rotation=0)
```

![image](https://user-images.githubusercontent.com/106129152/204989603-b30c3cce-2d15-46de-8d61-92d03cac0ba3.png)

위와 같이 make_moons 는 초승달 모양 클러스터 두 개 형상의 데이터를 생성

```python
X_train ,X_test, y_train, y_test= train_test_split(X, y, random_state=42)
```

**1.1 하드보팅**

예측 결과값을 바탕으로 다수결 투표 → 직접 투표

```python
log_clf = LogisticRegression()
rnd_clf = RandomForestClassifier()
svm_clf = SVC()

voting_clf = VotingClassifier(
                estimators = [('lr', log_clf), ('rf', rnd_clf), ('svm', svm_clf)],
                voting = 'hard')
```

```python
for clf in (log_clf, rnd_clf, svm_clf, voting_clf):
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))
# LogisticRegression 0.864
# RandomForestClassifier 0.88
# SVC 0.896
# VotingClassifier 0.904
```

VotingClassifier의 성능이 상대적으로 좋은 것을 알 수 있다.

```python
y_pred = voting_clf.predict(X_test)

from sklearn.metrics import accuracy_score
accuracy_score(y_test, y_pred)
# 0.904
```

**1.2 소프트보팅**

예측 확률값의 평균 또는 가중치 합 사용 → 간접 투표

```python
log_clf = LogisticRegression(random_state=42)
rnd_clf = RandomForestClassifier(random_state=42)
svm_clf = SVC(probability=True, random_state=42)

voting_clf = VotingClassifier(
                estimators = [('lr', log_clf), ('rf', rnd_clf), ('svm', svm_clf)],
                voting = 'soft')
```

SVC는 probability를 True로 설정해야 소프트보팅이 가능하다.

```python
for clf in (log_clf, rnd_clf, svm_clf, voting_clf):
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))
# LogisticRegression 0.864
# RandomForestClassifier 0.896
# SVC 0.896
# VotingClassifier 0.92
```

하드보팅은 0.904, 소프트보팅은 0.92가 나온 것을 확인할 수 있다!

### **2. 배깅 앙상블**

→ [배깅과 부스팅](https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-11-%EC%95%99%EC%83%81%EB%B8%94-%ED%95%99%EC%8A%B5-Ensemble-Learning-%EB%B0%B0%EA%B9%85Bagging%EA%B3%BC-%EB%B6%80%EC%8A%A4%ED%8C%85Boosting)

Bagging은 Bootstrap Aggregating의 약자.

같은 유형의 알고리즘을 기반으로 하지만, 

데이터 샘플링을 서로 다르게 가져가 학습을 수행하고, 보팅한다.

또, 페이스팅과 비교하자면,

배깅과 페이스팅 모두 같은 알고리즘을 사용해 학습시키는 것은 동일하다.

배깅은 중복을 허용한 샘플링, 페이스팅은 중복을 허용하지 않고 샘플링한다.

보통 배깅을 선호하지만, 페이스팅과 교차 검증으로 확인하는 과정이 필요!

```python
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
```

```python
# 배깅 (max_samples수가 100개)
bag_clf = BaggingClassifier(
            DecisionTreeClassifier(random_state=42), n_estimators= 500,
            max_samples=100, bootstrap=True, random_state=42, n_jobs=-1)

bag_clf.fit(X_train, y_train)

y_pred = bag_clf.predict(X_test)
accuracy_score(y_test, y_pred)
# 0.904
```

```python
# 결정트리 (500개의 샘플을 모두 사용하였으나 배깅보다 성능이 떨어짐)
tree_clf = DecisionTreeClassifier(random_state=42)
tree_clf.fit(X_train, y_train)
y_pred = tree_clf.predict(X_test)
accuracy_score(y_test, y_pred)
# 0.856
```

**oob(out-of-bag) 평가**

배깅을 할 때, 추출되지 않은 샘플들을 oob 샘플이라고 부른다.

검증세트나 교차 검증을 거치지 않고 oob 샘플만으로 모델 최적화 평가를 할 수 있다! 

```python
bag_clf = BaggingClassifier(
            DecisionTreeClassifier(random_state=42), n_estimators= 500,
            max_samples=100, bootstrap=True, oob_score=True, random_state=42, n_jobs=-1)
bag_clf.fit(X_train, y_train)

bag_clf.oob_score_
# 0.9253333333333333
```

**랜덤포레스트 모델**

→ [[위키] 랜덤 포레스트](https://ko.wikipedia.org/wiki/%EB%9E%9C%EB%8D%A4_%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8)

랜덤 포레스트는 다수의 결정 트리들을 학습하는 앙상블 방법이다.

배깅 방식을 적용하고, 빠른 수행속도와 높은 예측 성능을 갖고 있다.

```python
# 배깅
bag_clf = BaggingClassifier(
            DecisionTreeClassifier(max_leaf_nodes=16, random_state=42), n_estimators=500, 
            bootstrap=True, oob_score=True, random_state=42, n_jobs=-1)

bag_clf.fit(X_train, y_train)
y_pred = bag_clf.predict(X_test)
accuracy_score(y_test, y_pred)
# 0.912
```

```python
# 랜덤포레스트
rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, random_state=42, n_jobs=-1)
rnd_clf.fit(X_train, y_train)
y_pred = rnd_clf.predict(X_test)
accuracy_score(y_test, y_pred)
# 0.912

rnd_clf.feature_importances_ # X2 특성이 57% 더 중요
# array([0.42253629, 0.57746371])
```

결정트리의 특징으로,

위와 같이 feature_importances_로 특성 중요도를 확인할 수 있다.

**타이타닉 데이터셋에 랜덤포레스트 응용**

→ [11/30](https://github.com/pt0108/Machine_Learning_Study/blob/main/221130_Day4.md)

```python
from sklearn.ensemble import RandomForestClassifier

forest_clf = RandomForestClassifier(n_estimators=1000, random_state=42)
# 교차검증
forest_scores = cross_val_score(forest_clf, X_train, y_train, cv=5)
forest_scores.mean()
# 0.8069989328981231
```

11/30에 추가로 응용했던 kNN과 로지스틱의 경우,

둘 다 약 79퍼센트로 비슷한 결과가 나온 반면

오늘 추가로 응용한 랜덤포레스트의 경우 약 80퍼센트의 결과가 나타났다.

## 서포트 벡터 머신

→ [[위키] 서포트 벡터 머신](https://ko.wikipedia.org/wiki/%EC%84%9C%ED%8F%AC%ED%8A%B8_%EB%B2%A1%ED%84%B0_%EB%A8%B8%EC%8B%A0), [SVM 쉽게 이해하기](https://hleecaster.com/ml-svm-concept/)

support vector machine, **SVM**

도로 경계에 위치한 샘플에 의해 **결정 경계**가 정해지는 샘플.

여기서 서포트 벡터는 결정 경계에 가장 가까운 각 클래스의 점들을 말한다.

선형 분류, 비선형 분류 모두 사용될 수 있다.

SVM은 파라미터 **C**를 이용해 허용되는 오류 양을 조절한다.

서포트 벡터와 결정 경계 사이의 거리를 **마진**이라고 하는데,

C값이 클수록 하드 마진, 작을수록 소프트 마진을 만든다.

다항식 특성을 추가할 때의 문제점은,

차수가 낮으면 복잡한 데이터셋을 표현하지 못하고

차수가 높으면 굉장히 많은 특성의 추가로 모델이 느려지는데,

SVM은 **다항식 커널**을 이용한다.

(커널 트릭은 실제로 다항 특성을 추가하지 않고 비슷한 효과를 만드는 것)

다항식 커널을 사용하면  데이터를 더 높은 차원으로 변형해서 초평면의 결정 경계를 얻을 수 있다!

또, 방사 기저 함수라는 RBF 커널 혹은 **가우시안 커널**도 있다.

하이퍼파라미터 감마와 C를 바꾸어 훈련하는 것인데,

값이 작을 수록 규제가 크다. 

감마값이 너무 높으면 과대적합을, 너무 낮으면 과소적합이 될 수 있다.

가우시안 커널은 훈련 세트가 크지 않을 때 시도하면 좋다.

여기까지가 SVM 분류에 대한 설명이었고,

**SVM은 회귀로도 사용**할 수 있다.

SVM 회귀는 제한된 마진 안에 가능한 많은 샘플이 들어가도록 한다.

마진 안에서는 훈련 샘플이 추가되어도 모델의 예측에 영향이 없기 때문이다.

→ [SVM 회귀](https://mldlcvmjw.tistory.com/204?category=935400)

## 차원 축소 알고리즘 → [참고 링크](https://box-world.tistory.com/61)로 대체

## K-평균 알고리즘

→ [[위키] K-평균 알고리즘](https://ko.wikipedia.org/wiki/K-%ED%8F%89%EA%B7%A0_%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98), [[머신러닝] K-평균 알고리즘](https://velog.io/@jhlee508/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-K-%ED%8F%89%EA%B7%A0K-Means-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98)

k-평균 알고리즘(K-means clustering algorithm)은 **주어진 데이터를 k개의 클러스터로 묶는 알고리즘**으로, 각 클러스터와 거리 차이의 분산을 최소화하는 방식으로 동작한다. 

가장 빠르고 간단한 군집 알고리즘 중 하나로, **비지도 학습(자율 학습)**의 일종이다.

**<과정>**

(1) 군집의 개수(K) 설정

(2) 초기 중심점(센트로이드, Centroid) 설정

(3) 데이터를 군집에 할당

(4) 중심점 재설정

(5) 데이터를 군집에 재할당

→ 중심점의 이동이 없어질 때까지 (4)와 (5)를 반복한다.

**<한계점>**

최적의 모델을 구하기 위해서 여러번 학습해야 한다.

군집의 수를 미리 지정해야 한다.

군집의 밀집도가 다르거나, 원형이 아닐 경우 잘 작동하지 않는다. 

### K-평균 활용

**이미지 분할**

```python
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

image = plt.imread('./images/ladybug.png')
plt.imshow(image)
```

![image](https://user-images.githubusercontent.com/106129152/204989674-1764f27e-85df-47be-983f-ece4bff85394.png)

```python
image.shape # (533, 800, 3)

X = image.reshape(-1, 3)
X.shape # (426400, 3)
```

```python
kmeans = KMeans(n_clusters=8, random_state=42)
kmeans.fit(X)

kmeans.labels_ # 군집된 레이블
# array([1, 1, 1, ..., 4, 1, 1])

import numpy as np
np.unique(kmeans.labels_)
# array([0, 1, 2, 3, 4, 5, 6, 7])

kmeans.labels_.shape
# (426400,)

kmeans.cluster_centers_ # 8개 그룹의 센터 : 각각의 그룹을 대표할 수 있는 RGB 조합
# array([[0.9836374 , 0.9359338 , 0.02574807],
#        [0.02289337, 0.11064845, 0.00578197],
#        [0.21914783, 0.38675755, 0.05800817],
#        [0.75775605, 0.21225454, 0.0445884 ],
#        [0.09990625, 0.2542204 , 0.01693457],
#        [0.61266166, 0.63010883, 0.38751987],
#        [0.37212682, 0.5235918 , 0.15730347],
#        [0.88459074, 0.7256049 , 0.03442055]], dtype=float32)

# 각 그룹의 대표되는 RGB값으로 426400개의 픽셀을 대체
segmented_img = kmeans.cluster_centers_[kmeans.labels_] # 426400개의 index 사용

segmented_img = segmented_img.reshape(image.shape)
plt.imshow(segmented_img)
```

![image](https://user-images.githubusercontent.com/106129152/204989720-44d1a983-5a6c-402f-ae31-d90e46e0966f.png)

```python
segmented_imgs = []
n_colors= [10, 8, 6, 4, 2]
for clusters in n_colors:
    kmeans = KMeans(n_clusters=clusters, random_state=42)
    kmeans.fit(X)
    segmented_img = kmeans.cluster_centers_[kmeans.labels_]
    segmented_img = segmented_img.reshape(image.shape)
    segmented_imgs.append(segmented_img)
```

```python
plt.figure(figsize=(10, 5))

plt.subplot(231)
plt.imshow(image)
plt.title('original image')
plt.axis('off')

for idx, n_clusters in enumerate(n_colors):
    plt.subplot(232+idx) # axis 지정
    plt.imshow(segmented_imgs[idx])
    plt.title('{} colors'.format(n_clusters))
    plt.axis('off')
```

![image](https://user-images.githubusercontent.com/106129152/204989754-f3f1a5a3-8962-4eaf-b60a-3b1d85c5a0fc.png)
