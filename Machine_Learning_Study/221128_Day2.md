> 부트캠프 7주차 2022년 11월 28일


## 선형회귀 *linear regression*

**회귀** : 여러 개의 독립변수와 한 개의 종속변수 간의 상관관계를 모델링하는 기법을 통칭 

**회귀 예측** : 주어진 특성과 레이블 데이터 기반에서 훈련을 통해 최적의 모델 파라미터를 찾아내는 것 → 비용함수(MSE)를 최소화하는 *θ*를 찾아야 함

**평균 제곱 오차(MSE)** : 실제값과 예측값 사이의 오류값을 제곱하여 더하는 방식 (이 값이 작을 수록 좋다!) 비용함수 또는 손실함수라고도 부른다

**결정계수 R²** : MSE만으로는 크고 작음을 직관적으로 알기 어렵기 때문에 결정계수로 수치를 보완함 (오차가 없을 때, R²=1)

**선형회귀** : 실제값과 예측값의 차이를 최소화하는 직선형 회귀선 최적화 방식

→ [선형회귀(위키)](https://ko.wikipedia.org/wiki/%EC%84%A0%ED%98%95_%ED%9A%8C%EA%B7%80), [회귀 분석(위키)](https://ko.wikipedia.org/wiki/%ED%9A%8C%EA%B7%80_%EB%B6%84%EC%84%9D), [[머신러닝 이론] 회귀](https://roytravel.tistory.com/57)

```python
import numpy as np
import matplotlib.pyplot as plt
```

### **1. 정규 방정식을 사용한 선형회귀**

- y=3X+4
    
    ```python
    X = 2 * np.random.rand(100, 1)
    y = 3 * X + 4 + np.random.randn(100, 1)
    
    plt.plot(X, y, '.')
    ```
    
    ![image](https://user-images.githubusercontent.com/106129152/204219074-4211e8ac-56be-4855-ae48-ff1446c24009.png)
    
    ```python
    X_b = np.c_[np.ones((100, 1)), X]
    ```
    
- 정규방정식 공식을 **np.linalg** 함수를 이용해서 해를 구함
    
    ![image](https://user-images.githubusercontent.com/106129152/204219107-da9315fd-0d3f-4cd9-a531-086629a02afc.png)
    
    ```python
    theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)
    theta_best
    # array([[4.21509616],
    #       [2.77011339]])
    
    # array([[4.21509616], # theta 0 : 절편
    #        [2.77011339]]) # theta 1 : 기울기
    # y = 3X + 4
    ```
    
- scikit-learn 제공 **LinearRegression** 사용
    
    → [sklearn 선형회귀 예제](https://hleecaster.com/ml-linear-regression-example/)
    
    ```python
    from sklearn.linear_model import LinearRegression
    
    lin_reg = LinearRegression()
    lin_reg.fit(X, y)
    
    lin_reg.intercept_, lin_reg.coef_
    # (array([4.21509616]), array([[2.77011339]]))
    ```
    
    **.coef_** : 기울기
    
    **.intercept_** : 절편
    

### ****2. 경사하강법을 사용한 선형회귀****

**경사하강법** : 모델 파라미터를 조금씩 수정하면서 비용 함수의 최소값을 찾는 방법

![image](https://user-images.githubusercontent.com/106129152/204219153-4fc29195-ec28-4b7c-a397-f0baecacbf65.png)

**그래디언트** :

![image](https://user-images.githubusercontent.com/106129152/204219182-b1fc6648-eb62-41e3-8955-c9edb25edfc0.png)

**학습률** : 그래디언트 적용량을 조정하는 하이퍼 파라미터

→ 적당한 학습률을 주는 것이 아주 중요!

경사하강법을 사용할 때는 ***반드시 모든 특성이 같은 스케일을 갖도록*** 해야한다!

- 배치 경사하강법
    
    ```python
    m = 100
    theta = np.random.randn(2, 1)
    eta = 0.1
    n_iterations = 1000
    
    for iteration in range(n_iterations):
        gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y) # gradients (theta1에 대한 미분, theta2에 대한 미분)
        theta = theta - eta * gradients # theta  (theta0, theta1)
    
    theta
    # array([[4.21509616],
    #        [2.77011339]])
    ```
    
- 확률적 경사하강법
    
    ```python
    m = 100
    theta = np.random.randn(2, 1)
    n_epochs = 50 
    t0, t1 = 5, 50
    def learning_schedule(t):
        return t0/(t + t1)
    
    for epoch in range(n_epochs): # epoch : 모든 데이터가 학습에 참여했을 때 1 epoch
        for i in range(m): # m: 100개 샘플수 (100개의 샘플중에 여러번 뽑히는 샘플도 있고, 전혀 선택되지 않는 샘플도 있음)
            random_index = np.random.randint(m)
            xi = X_b[random_index:random_index+1]
            yi = y[random_index:random_index+1]
            gradients = 2 * xi.T.dot(xi.dot(theta) - yi) # gradients (theta1에 대한 미분, theta2에 대한 미분)
            eta = learning_schedule(epoch*m+ i) # step수가 증감함에 따라 eta(학습률)이 작아지게 함
            theta = theta - eta*gradients # theta (theta 0, theta 1)
    
    theta # y = 3X + 4
    # array([[4.20742938],
    #        [2.74264448]])
    ```
    
- scikit-learn 제공하는 **SGDRegressor()** 사용
    
    ```python
    from sklearn.linear_model import SGDRegressor
    
    sgd_reg = SGDRegressor(max_iter=50, penalty=None, eta0=0.1, random_state=42)
    sgd_reg.fit(X, y.ravel()) # .ravel() : 다차원 배열이 1차원으로 변경
    # SGDRegressor(eta0=0.1, max_iter=50, penalty=None, random_state=42)
    
    sgd_reg.intercept_, sgd_reg.coef_ # y = 3X + 4
    # (array([4.24365286]), array([2.8250878]))
    ```
    

선형회귀 모델의 잠재적 **문제점**

- 데이터 자체가 선형적으로 표현이 안될 수 있음 → 비선형적인 데이터를 표현할 수 있게 특성 추가(다항회귀)
- 시간을 두고 쌓인 데이터, 지리적으로 가까운 데이터인 경우 잘 동작을 안함 → 다른 모델 사용(ARIMA, ARMA 등)
- 데이터(엄밀히는 예측과 실제의 차이)가 정규분포를 따르지 않으면 선형회귀 모델에서 성능이 떨어질 수 있음 → 로그변환
- 바깥값이 있어도 잘 동작을 안함 → 바깥값은 삭제
- 다중공선성 문제 → 상관관계 큰 특성을 삭제 또는 제 3의 특성을 추출

---

*(11/29) 이어서*

### 3. 다항 회귀

각 특성의 제곱을 새로운 특성으로 추가하고, 확장된 특성에 선형 모델을 훈련시킨다.

→ [선형 회귀와 다항 회귀](https://movefast.tistory.com/302)

```python
# 비선형성을 갖는 데이터를 준비하기 위해서
# 0.5X**2 + X + 2 형태의 데이터를 임의로 준비
# 모델이 훈련을 마친 후에 모델 파라미터 (0.5, 1, 2)에 근사하는지 확인

np.random.seed(42)

m = 100
X = 6 * np.random.rand(m, 1) - 3
y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)

plt.plot(X, y, '.')
```

![image](https://user-images.githubusercontent.com/106129152/204425930-eaaf4a98-961f-46e7-ae0e-e967b75a6d12.png)

```python
lin_reg = LinearRegression()
lin_reg.fit(X, y)
# LinearRegression()

lin_reg.intercept_, lin_reg.coef_
# (array([3.56401543]), array([[0.84362064]]))
```

이제 위의 원본 특성에 제곱항을 추가해보자.

```python
# 모델 = 예측기(or 분류기)()
# 모델.fit(X, y) # 학습
# 모델.predict(X) # 예측

# 변환기 = 변환기객체생성()
# 변환기.fit() # 변환할 준비
# 변환기.transform(X) # 실제 변환
```

```python
# 원본 특성에 제곱항을 추가
from sklearn.preprocessing import PolynomialFeatures

poly_features = PolynomialFeatures(degree=2, include_bias=False) # 제곱항 추가
X_poly = poly_features.fit_transform(X)

lin_reg = LinearRegression()
lin_reg.fit(X_poly, y)

lin_reg.intercept_, lin_reg.coef_ # 0.5X**2 + X + 2
# (array([1.78134581]), array([[0.93366893, 0.56456263]]))
```

```python
X_new = np.linspace(-3, 3, 100).reshape(100, 1) # linspace() : 1차원 배열로 만들어줌
X_new_poly = poly_features.transform(X_new) # 실제 변환

# X가 전처리(제곱항 특성 추가)된 부분이 새 데이터에도 반영이 되어야 함
y_pred = lin_reg.predict(X_new_poly)
print(X_poly.shape, X_new_poly.shape) # (100, 2) (100, 2)

plt.plot(X_new, y_pred, 'r-') # 새로운 데이터(X_new)에 대한 예측
plt.plot(X, y, 'b.') # 데이터 샘플
```

![image](https://user-images.githubusercontent.com/106129152/204425964-e618171e-3081-4633-90d9-8cf2b19d468f.png)

### 4. 규제 모델

→ [편향-분산 트레이드오프](https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-12-%ED%8E%B8%ED%96%A5Bias%EC%99%80-%EB%B6%84%EC%82%B0Variance-Trade-off), [모델 규제하기](https://velog.io/@eogns1208/%EB%AA%A8%EB%8D%B8-%EA%B7%9C%EC%A0%9C%ED%95%98%EA%B8%B0), [모델 훈련](https://yganalyst.github.io/ml/ML_chap3-4/)

편향이 크면 과소 적합을 야기하고, 분산이 크면 과대 적합을 야기한다.

과대 적합을 감소시키는 방법 : **규제**

****4.1 릿지 회귀 - L2 규제****

비용 함수에 L2-norm(놈, 노름 / 벡터의 길이 혹은 크기를 측정)의 제곱을 추가

α(= 하이퍼 파라미터)의 값을 통해 규제의 강도를 정함

α가 커지면 분산이 줄고 편향이 커진다.

![image](https://user-images.githubusercontent.com/106129152/204425993-90b97a45-dd19-425f-b765-0e86070e204c.png)

```python
np.random.seed(42)

m = 20
X = 3 * np.random.rand(m, 1)
y = 0.5 * X + 1 + np.random.randn(m, 1)/1.5
plt.plot(X, y, '.')
```

![image](https://user-images.githubusercontent.com/106129152/204426009-a69fefcb-3840-4322-9de4-e1b13cb4b2c1.png)

```python
# 선형 회귀 모델
lin_reg = LinearRegression()
lin_reg.fit(X, y)
lin_reg.intercept_, lin_reg.coef_ # y = 0.5X + 1
# (array([0.97573667]), array([[0.3852145]]))

# 릿지 회귀 모델 (L2 규제) - 해석적으로 해를 구함
from sklearn.linear_model import Ridge

ridge_reg = Ridge()
ridge_reg.fit(X, y)
ridge_reg.intercept_, ridge_reg.coef_ # 절편(theta 0)는 규제의 범위에 포함되지 않음
# (array([1.00650911]), array([[0.36280369]]))
```

```python
# 릿지 회귀 모델 (L2 규제) - 경사하강법으로 해를 구함
# 규제 없이
sgd_reg = SGDRegressor(penalty=None, random_state=42)
sgd_reg.fit(X, y.ravel())
print(sgd_reg.intercept_, sgd_reg.coef_) # [0.53945658] [0.62046175]

# 규제 추가
sgd_reg_l2 = SGDRegressor(penalty='l2', alpha=0.1, random_state=42)
sgd_reg_l2.fit(X, y.ravel())
print(sgd_reg_l2.intercept_, sgd_reg_l2.coef_) # [0.57901244] [0.58606577]
```

****4.2 라쏘 회귀 - L1 규제****

비용 함수에 L1-norm의 제곱을 추가

비용 함숫값이 큰 특성은 0으로 만들어 버린다. (특성 선택 효과)

릿지 회귀와 마찬가지로 α가 커지면 분산이 줄고 편향이 커진다.

![image](https://user-images.githubusercontent.com/106129152/204426026-9932dcea-ab00-458b-ba8a-15100089a054.png)

```python
# 라쏘 회귀 모델 (L1 규제) - 해석적으로 해를 구함
from sklearn.linear_model import Lasso

lasso_reg = Lasso(alpha=0.1, random_state=42)
lasso_reg.fit(X, y)
lasso_reg.intercept_, lasso_reg.coef_
# (array([1.14537356]), array([0.26167212]))
```

```python
# 라쏘 회귀 모델 (L1 규제) - 경사하강법으로 해를 구함
# 규제 없이
sgd_reg = SGDRegressor(penalty=None, random_state=42)
sgd_reg.fit(X, y.ravel())
print(sgd_reg.intercept_, sgd_reg.coef_)  # [0.53945658] [0.62046175]

# 규제 추가
sgd_reg_l1 = SGDRegressor(penalty='l2', alpha=0.1, random_state=42)
sgd_reg_l1.fit(X, y.ravel())
print(sgd_reg_l1.intercept_, sgd_reg_l1.coef_) # [0.57901244] [0.58606577]
```

**4.3 엘라스틱 넷 Elastic Net**

릿지 회귀와 라쏘 회귀를 절충한 규제 알고리즘

엘라스틱 넷은 하이퍼 파라미터가 두 개인데, 

r=0이면 릿지 회귀, r=1이면 라쏘 회귀와 같다.

![image](https://user-images.githubusercontent.com/106129152/204426052-d725cff3-302d-4f16-bdd4-9285882fd06e.png)

```python
# 엘라스틱넷 (L1규제, L2규제)
from sklearn.linear_model import ElasticNet

elastic_reg = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)
elastic_reg.fit(X, y)
elastic_reg.intercept_, elastic_reg.coef_
# (array([1.08639303]), array([0.30462619]))

# 규제 없이
sgd_reg = SGDRegressor(penalty=None, random_state=42)
sgd_reg.fit(X, y.ravel())
print(sgd_reg.intercept_, sgd_reg.coef_) # [0.53945658] [0.62046175]

# 규제 추가
sgd_reg_l1l2 = SGDRegressor(penalty='elasticnet', alpha=0.1, random_state=42)
sgd_reg_l1l2.fit(X, y.ravel())
print(sgd_reg_l1l2.intercept_, sgd_reg_l1l2.coef_) # [0.59684835] [0.57598861]
```
