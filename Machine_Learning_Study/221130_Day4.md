> 부트캠프 7주차 2022년 11월 30일

# 11/30 Machine Learning

## 로지스틱 회귀

→ [로지스틱 회귀](https://ko.wikipedia.org/wiki/%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1_%ED%9A%8C%EA%B7%80)

선형 회귀 방식을 분류에 적용한 알고리즘으로, 선형 방정식을 [시그모이드 함수](https://ko.wikipedia.org/wiki/%EC%8B%9C%EA%B7%B8%EB%AA%A8%EC%9D%B4%EB%93%9C_%ED%95%A8%EC%88%98)에 통과시켜 0~1 사이의 확률을 얻는다.

흔히 로지스틱 회귀는 종속변수가 이항형 문제(즉, 유효한 범주의 개수가 두개인 경우)를 지칭할 때 사용된다. 

선형 회귀에서 사용했던 비용 함수(MSE)는 로지스틱 회귀에 맞지 않는다.

[이 링크](https://wikidocs.net/22881)의 3번 항목, 비용 함수에서 자세한 설명이 나와있다.

비용 함수로 MSE(평균 제곱 오차)를 사용하면 울퉁불퉁한 형태가 되어버린다!

$$
J(w) = -\frac{1}{n} \sum_{i=1}^{n} [y^{(i)}logH(x^{(i)}) + (1-y^{(i)})log(1-H(x^{(i)}))]
$$

나의 지식으로는 위의 수식을 보고 단번에 이해하기 어렵지만,

결론만 말하자면, **로지스틱 회귀는 비용 함수로 크로스 엔트로피 함수를 사용**하고,

가중치를 찾기 위해 크로스 엔트로피 함수의 평균을 취한 함수를 사용한다고 한다.

→ [손실 함수](https://umbum.tistory.com/210), [엔트로피와 크로스 엔트로피의 쉬운 개념 설명](https://melonicedlatte.com/machinelearning/2019/12/20/204900.html)

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
```

```python
iris = load_iris()

X = iris.data[:, 3:] # 특성 1개만 사용 ('petal width (cm)')
y = (iris.target == 2).astype(np.int32) # Virginica면 1, 아니면 0
```

```python
from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression(random_state=42)
log_reg.fit(X, y) # 훈련 : 비용함수(로그 손실)를 최소화하는 모델파라미터(theta0, 1)를 찾음

log_reg.intercept_, log_reg.coef_
# (array([-7.1947083]), array([[4.3330846]]))

log_reg.predict([[2.5]]) 
# 야생에서 가져온 샘플의 petal width (2차원 데이터로 넣어주기)
# array([1])

log_reg.predict_proba([[2.5]]) 
# Virginica일 확률(0.025 : Virginica가 아닐 확률, 0.97 : Virginica일 확률)
# 이 두 확률이 합쳐졌을 때 1이 됨(= predict값)
# array([[0.02563061, 0.97436939]])

X_new = np.linspace(0, 3, 1000).reshape((-1, 1)) # (1000, 1)
y_prob = log_reg.predict_proba(X_new)

y_prob[0][0] # 1000개 중 0번째 샘플에 대해 Virginica가 아닐 확률
y_prob[0][1] # 1000개 중 0번째 샘플에 대해 Virginica일 확률
# array([9.99250016e-01, 7.49984089e-04])

decision_boundary = X_new[y_prob[:, 1]>0.5][0]
decision_boundary # 이 부분을 기점으로 0과 1을 나눔
# array([1.66066066])
```

```python
plt.figure(figsize=(8, 3))
plt.plot([decision_boundary, decision_boundary], [0, 1], 'k:')
plt.plot(X_new, y_prob[:, 0], label='Not Iris-Virginica') # Virginica가 아닐 확률
plt.plot(X_new, y_prob[:, 1], label='Virginica') # Virginica일 확률
plt.legend()
```

![image](https://user-images.githubusercontent.com/106129152/204738399-0c08e8a5-a2d6-44f3-8a2a-78c2f575a969.png)

```python
# 정말 decision_boundary 기준으로 값이 나뉘는지 확인

log_reg.predict([[1.7]])
# array([1])

log_reg.predict([[1.5]])
# array([0])
```

### 소프트맥스 회귀

→ [[머신러닝] 소프트맥스 회귀](https://velog.io/@rcchun/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4-%ED%9A%8C%EA%B7%80Softmax-Regression-%EB%8B%A4%EC%A4%91-%ED%81%B4%EB%9E%98%EC%8A%A4-%EB%B6%84%EB%A5%98)

소프트맥스 회귀는 로지스틱 회귀를 이용한 다중 분류다.

다항 로지스틱 회귀라고도 한다.

```python
X = iris.data[:, 2:] # 특성 2개만 사용 ('petal length (cm)', 'petal width (cm)')
y = iris.target # 3품종(클래스)을 구분하는 문제(다중 분류)

softmax_reg = LogisticRegression(multi_class='multinomial', random_state=42)
softmax_reg.fit(X, y) # 훈련 : 비용함수(로그손실)를 최소화하는 모델파라미터(theta 0, 1)를 찾음

softmax_reg.intercept_
# array([ 11.12767979,   3.22717485, -14.35485463])

softmax_reg.coef_
# array([[-2.74866104, -1.16890756],
#        [ 0.08356447, -0.90803047],
#        [ 2.66509657,  2.07693804]])

# 예측 : 야생에서 채집해온 데이터의 꽃잎 길이와 너비 -> (5cm, 2cm)
softmax_reg.predict([[5, 2]]) # 2번 class (Setosa로 예측)
# array([2])

softmax_reg.predict_proba([[5, 2]])
array([[2.43559894e-04, 2.14859516e-01, 7.84896924e-01]])
```

## Titanic Dataset 응용

→ [11월 22일](https://github.com/pt0108/Data_Analysis_Study/blob/main/221122_Titanic%20Dataset.md) 배웠던 내용에서 모델 훈련을 추가로 실습해보았다.

**2.6 범주형 데이터를 더미 변수로 변환**

```python
# 위에서 Pclass, Embarked, Sex에 대해 dummy 변수로 준비된 것을 titanic_train에 copy
titanic_train = pd.concat([titanic,Pclass_dummies, Embarked_dummies, Sex_dummies], axis=1)
```

**2.7 필요없는 특성 삭제, 새로운 특성 추가**

```python
# PassengerId, Name, Cabin, Ticket, AgeCat 삭제
titanic_train.drop(['PassengerId', 'Name', 'Cabin', 'Ticket', 'AgeCat'], axis=1, inplace=True)

# dummy 변수로 바꾼 범주형 데이터도 삭제
titanic_train.drop(['Pclass', 'Embarked', 'Sex'], axis=1, inplace=True)

titanic_train.columns
# Index(['Survived', 'Age', 'SibSp', 'Parch', 'Fare', 'Pclass_1', 'Pclass_2',
#        'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S', 'Sex_female',
#        'Sex_male'],
#       dtype='object')
```

**2.8 특성과 레이블 분리하기**

```python
# 레이블은 Survived (생존률)
X_train = titanic_train.drop('Survived', axis=1)
y_train = titanic['Survived'].copy()
```

**2.9 특성 스케일링**

- **정규화 (0 ~ 1 사이로 변환)**
    
    ```python
    def minmax_normalize(arr):
        return (arr-arr.min())/(arr.max()-arr.min())
    ```
    
- **표준화 (평균 0, 표준편차 1)**
    
    ```python
    def zscore_standize(arr): # 평균 0, 표준편차 1
        return (arr - arr.mean())/arr.std()
    ```
    

### 3. 모델 선택과 훈련

- **kNNClassifier, LogisticRegression**
    
    ```python
    from sklearn.neighbors import KNeighborsClassifier
    knn_clf = KNeighborsClassifier()
    
    from sklearn.linear_model import LogisticRegression
    log_clf = LogisticRegression(max_iter=1000, random_state=42)
    ```
    
- ****************************************교차 검증으로 평가****************************************
    
    ```python
    from sklearn.model_selection import cross_val_score
    
    # kNN
    knn_clf_scores = cross_val_score(knn_clf, X_train, y_train, cv=3) 
    knn_clf_scores.mean()
    # 0.7115600448933782
    
    # 로지스틱 회귀
    log_clf_scores = cross_val_score(log_clf, X_train, y_train, cv=3)
    log_clf_scores.mean()
    # 0.7912457912457912
    ```
    

### 4. 성능 올리기

- **특성 스케일링 후 모델 훈련**
    
    ```python
    X_train.plot()
    ```
    
    ![image](https://user-images.githubusercontent.com/106129152/204738543-cf575802-9c8c-4304-b8ce-aed258840319.png)
    
    ```python
    X_train_mm = X_train.apply(minmax_normalize) # 정규화 함수 적용
    X_train_mm.min(), X_train_mm.max()
    ```
    
    ```
    (Age           0.0
     SibSp         0.0
     Parch         0.0
     Fare          0.0
     Pclass_1      0.0
     Pclass_2      0.0
     Pclass_3      0.0
     Embarked_C    0.0
     Embarked_Q    0.0
     Embarked_S    0.0
     Sex_female    0.0
     Sex_male      0.0
     dtype: float64,
     Age           1.0
     SibSp         1.0
     Parch         1.0
     Fare          1.0
     Pclass_1      1.0
     Pclass_2      1.0
     Pclass_3      1.0
     Embarked_C    1.0
     Embarked_Q    1.0
     Embarked_S    1.0
     Sex_female    1.0
     Sex_male      1.0
     dtype: float64)
    ```
    
    ```python
    X_train_std = X_train.apply(zscore_standize) # 표준화 함수 적용
    X_train_std.mean(), X_train_std.std()
    ```
    
    ```
    (Age           3.345310e-16
     SibSp         1.528893e-16
     Parch        -3.638441e-17
     Fare          7.437622e-16
     Pclass_1     -3.384249e-16
     Pclass_2     -7.476249e-17
     Pclass_3      1.103993e-16
     Embarked_C    1.209283e-16
     Embarked_Q    2.811070e-16
     Embarked_S   -2.930690e-16
     Sex_female    3.528790e-16
     Sex_male     -4.273923e-16
     dtype: float64,
     Age           1.0
     SibSp         1.0
     Parch         1.0
     Fare          1.0
     Pclass_1      1.0
     Pclass_2      1.0
     Pclass_3      1.0
     Embarked_C    1.0
     Embarked_Q    1.0
     Embarked_S    1.0
     Sex_female    1.0
     Sex_male      1.0
     dtype: float64)
    ```
    
- **정규화된 데이터로 시도**
    
    ```python
    knn_clf_scores = cross_val_score(knn_clf, X_train_mm, y_train, cv=3) 
    knn_clf_scores.mean() # kNN의 경우, 정규화 후 성능이 향상된 것을 확인 가능
    # 0.7946127946127947
    
    log_clf = LogisticRegression(max_iter=1000, random_state=42)
    log_clf_scores = cross_val_score(log_clf, X_train_mm, y_train, cv=3)
    log_clf_scores.mean()
    # 0.7901234567901234
    ```
    
- **표준화된 데이터로 시도**
    
    ```python
    knn_clf_scores = cross_val_score(knn_clf, X_train_std, y_train, cv=3) 
    knn_clf_scores.mean()
    # 0.7934904601571269
    
    log_clf = LogisticRegression(max_iter=1000, random_state=42)
    log_clf_scores = cross_val_score(log_clf, X_train_std, y_train, cv=3)
    log_clf_scores.mean()
    # 0.7912457912457912
    ```
    
- **특성 추가하기**
    
    ```python
    # 함께 탑승한 가족의 인원수 컬럼을 합친 Family 특성 만들기
    titanic_train['Family'] = titanic_train['SibSp'] + titanic_train['Parch']
    
    # SibSp와 Parch를 더한 Family 특성을 추가하고, 기존의 SibSp와 Parch를 삭제
    X_train['Family'] = X_train['SibSp'] + X_train['Parch']
    X_train.drop(['SibSp', 'Parch'], axis=1, inplace=True)
    ```
    
    ```python
    # 표준화
    X_train_std = X_train.apply(zscore_standize)
    X_train_std.mean(), X_train_std.std()
    ```
    
    ```
    (Age           3.345310e-16
     Fare          7.437622e-16
     Pclass_1     -3.384249e-16
     Pclass_2     -7.476249e-17
     Pclass_3      1.103993e-16
     Embarked_C    1.209283e-16
     Embarked_Q    2.811070e-16
     Embarked_S   -2.930690e-16
     Sex_female    3.528790e-16
     Sex_male     -4.273923e-16
     Family        1.522663e-16
     dtype: float64,
     Age           1.0
     Fare          1.0
     Pclass_1      1.0
     Pclass_2      1.0
     Pclass_3      1.0
     Embarked_C    1.0
     Embarked_Q    1.0
     Embarked_S    1.0
     Sex_female    1.0
     Sex_male      1.0
     Family        1.0
     dtype: float64)
    ```
    
    ```python
    # 교차 검증
    knn_clf_scores = cross_val_score(knn_clf, X_train_std, y_train, cv=3) 
    knn_clf_scores.mean()
    # 0.7901234567901234
    
    log_clf_scores = cross_val_score(log_clf, X_train_std, y_train, cv=3)
    log_clf_scores.mean()
    # 0.7923681257014591
    ```
    

## 결정트리

→ [머신러닝 - 결정트리](https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-4-%EA%B2%B0%EC%A0%95-%ED%8A%B8%EB%A6%ACDecision-Tree), [[머신러닝] 의사결정트리](https://di-bigdata-study.tistory.com/2), [[위키] 결정 트리 학습법](https://ko.wikipedia.org/wiki/%EA%B2%B0%EC%A0%95_%ED%8A%B8%EB%A6%AC_%ED%95%99%EC%8A%B5%EB%B2%95)

결정트리, 의사결정트리 (**DecisionTree**) 라고도 하는 이것은,

분류(Classification)와 회귀(Regression) 모두 가능한 지도 학습 모델 중 하나이다.

지난달에 자료구조를 배우면서 봤던 트리 구조를 생각하면 꽤 익숙한 모양새다.

결정트리 모델은 훈련 전 파라미터 수가 결정되지 않기 때문에 비파라미터 모델이다.

[https://graphviz.org/download/](https://graphviz.org/download/) 

오픈소스 다이어그램 생성기인 graphviz를 다운받는다. → [graphviz 사용방법](https://narusas.github.io/2019/01/25/Graphviz.html)

그리고 아나콘다 프롬프트에서 conda install python-graphviz 를 입력 후 설치

### 1. 분류

```python
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import export_graphviz
import graphviz
```

```python
iris = load_iris()
X = iris.data[:, 2:] # 꽃잎의 길이, 너비 (peetal length, width)
y = iris.target

# 결정트리 객체 생성, fit
tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42) # max_depth : 최대 깊이 설정
tree_clf.fit(X, y)
```

```python
# graphviz 시각화 옵션 설정
export_graphviz(tree_clf,
               out_file = 'iris_tree.dot',
               feature_names = iris.feature_names[2:],
               class_names = iris.target_names,
               rounded = True,
               filled = True)

with open('iris_tree.dot') as f:
    dot_graph = f.read()

graphviz.Source(dot_graph)
```

![image](https://user-images.githubusercontent.com/106129152/204738698-4be4dae5-24d7-47c7-99c0-7d5a9ca8d317.png)

위의 gini는 지니 계수(혹은 지니 불순도)로, 0과 1 사이의 값을 가진다.

이 계수가 높을 수록 잘 분류되지 못한 것이다. (1에 가까울수록 불평등, 불확실)

- **CART(Classification And Regression Tree) 알고리즘**
    
    → [참고](https://data-scientist-brian-kim.tistory.com/73)
    
    사이킷런은 결정트리를 훈련시키기 위해 **CART 알고리즘**을 사용한다.
    
    CART 알고리즘은 [**탐욕적 알고리즘**](https://hanamon.kr/%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-%ED%83%90%EC%9A%95%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-greedy-algorithm/)이다.
    
    여기서 max_depth로 최대 깊이를 정하는 등, **규제 매개변수로 제한**해주지 않으면, 
    
    지니 계수가 0이 될 때까지 계속해서 분류를 하기 때문에 과대적합이 될 수 있다.
    


### ****2. 수치 예측****

**회귀 트리**는 각 노드에서 클래스를 예측하는 대신 **어떤 값을 예측**한다.

MSE(또는MAE)를 최소화하도록 분할한다.

```python
import numpy as np
import matplotlib.pyplot as plt
```

```python
np.random.seed(42)

m = 200
X = np.random.rand(m, 1)
# y = 4(X-0.5)**2
y = 4 * (X-0.5)**2 + np.random.randn(m, 1)/10
```

```python
plt.plot(X, y, 'b.')
```

![image](https://user-images.githubusercontent.com/106129152/204742981-af69d3b6-77e4-490c-aea8-ac7b72683e5d.png)

```python
# 결정 트리 회귀 분석
from sklearn.tree import DecisionTreeRegressor

tree_reg = DecisionTreeRegressor(max_depth=2, random_state=42)
tree_reg.fit(X, y)

# 시각화 해주는 옵션들 설정
export_graphviz(tree_reg,
                out_file = 'regression_tree.dot',
                feature_names = ['x1'],
                class_names = iris.target_names,
                rounded = True,
                filled = True
               )

with open('regression_tree.dot') as f:
    dot_graph = f.read()
graphviz.Source(dot_graph)
```

![image](https://user-images.githubusercontent.com/106129152/204743057-d9775b26-99a2-4133-99ad-32696746da66.png)

```python
tree_reg.predict([[0.6]])
# array([0.11063973])
```
