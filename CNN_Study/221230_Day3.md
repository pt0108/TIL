> 부트캠프 11주차 2022년 12월 30일

# **Cat and Dog - (2) VGGNet**

Cat and Dog(1)에서는 데이터를 가져오는 option 2가지 방법을 배웠는데, 

이번에는 새로운 option 3를 알게 되었다! (캐글의 데이터셋을 이용하는 방법)

```python
# option 3
!pip install kaggle

from google.colab import files
files.upload() # 캐글에서 다운받은 json파일 업로드

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/

# permmision warning 방지
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d tongpython/cat-and-dog

!unzip cat-and-dog.zip -d catanddog/
data_dir = './catanddog'
```

모델 생성 전까지의 과정은 이전과 동일하다.

(기존의 VGG Net에서 BatchNorm 계층 추가)

```python
class VGG16(nn.Module):
  def __init__(self):
    super().__init__()
    self.block1 = nn.Sequential(
        nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1, stride=1),
        nn.BatchNorm2d(64),
        nn.ReLU(),
        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1, stride=1),
        nn.BatchNorm2d(64),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=2, stride=2)  # 64 x 112 x 112
    )
    self.block2 = nn.Sequential(
        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1, stride=1),
        nn.BatchNorm2d(128),
        nn.ReLU(),
        nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1, stride=1),
        nn.BatchNorm2d(128),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=2, stride=2)  # 128 x 56 x 56      
    )
    self.block3 = nn.Sequential(
        nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1, stride=1),
        nn.BatchNorm2d(256),
        nn.ReLU(),
        nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1, stride=1),
        nn.BatchNorm2d(256),
        nn.ReLU(),
        nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1, stride=1),
        nn.BatchNorm2d(256),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=2, stride=2)  # 256 x 28 x 28   
    )
    self.block4 = nn.Sequential(
        nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1, stride=1),
        nn.BatchNorm2d(512),
        nn.ReLU(),
        nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1, stride=1),
        nn.BatchNorm2d(512),
        nn.ReLU(),
        nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1, stride=1),
        nn.BatchNorm2d(512),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=2, stride=2) # 512 x 14 x 14    
    )            
    self.block5 = nn.Sequential(
        nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1, stride=1),
        nn.BatchNorm2d(512),
        nn.ReLU(),
        nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1, stride=1),
        nn.BatchNorm2d(512),
        nn.ReLU(),
        nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1, stride=1),
        nn.BatchNorm2d(512),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=2, stride=2) # 512 x 7 x 7     
    )
    self.classifier = nn.Sequential(
        nn.Linear(in_features=512*7*7, out_features=512),
        nn.ReLU(),
        nn.Linear(in_features=512, out_features=64),
        nn.ReLU(),
        nn.Linear(in_features=64, out_features=2)
    )
  
  def forward(self, x):
    x = self.block1(x)
    x = self.block2(x)
    x = self.block3(x)
    x = self.block4(x)
    x = self.block5(x)
    # x = x.view(-1, 512*7*7)
    x = torch.flatten(x, 1) # 1번 dim 부터 flatten
    x = self.classifier(x)
    return x
```

```python
learning_rate = 0.001
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr = learning_rate)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=7, factor=0.1, verbose=True)
```

```python
def validation(model, validloader, criterion):
  # 전방향 예측후 나온 점수(logits)의 최대값을 최종 예측으로 준비
  # 이 최종 예측과 정답을 비교
  # 전체 중 맞은 것의 개수 비율을 정확도(accuracy)로 계산
  valid_accuracy = 0
  valid_loss = 0

  # 전방향 예측을 구할 때는 gradient가 필요가 없음
  with torch.no_grad():
    for images, labels in validloader: # 10000개의 데이터에 대해 16개씩(미니배치 사이즈) 10000/16번을 iterations
      # 0. Data를 GPU로 보내기
      images, labels = images.to(device), labels.to(device)

      # 1. 입력데이터 준비
      # not Flatten!!
      # images.resize_(images.size()[0], 784) # 16, 1, 28, 28
      
      # 2. 전방향(Forward) 예측 
      logits = model.forward(images) # 점수 반환
      _, preds = torch.max(logits, 1) # 16개에 대한 최종 예측
      # preds= probs.max(dim=1)[1] 
      correct = (preds == labels).sum()

      accuracy = correct / images.shape[0]
      loss = criterion(logits, labels) # 16개에 대한 loss
      
      valid_accuracy += accuracy
      valid_loss += loss.item() # tensor 값을 꺼내옴
    

  return valid_loss, valid_accuracy # validloader 전체 대한 총 loss, 총 accuracy
```

```python
def train(model, epochs, criterion, optimizer):
  steps = 0
  min_loss = 10000
  max_accuracy = 0

  trigger = 0
  patience = 13 # for Early stopping

  # 1 에폭(epoch)당 반복수
  #steps_per_epoch = len(trainset)/batch_size # 2500 iterations
  steps_per_epoch = len(trainloader) # 2500 iterations

  for epoch in range(epochs):
    model.train()
    train_loss = 0
    for images, labels in iter(trainloader): # 이터레이터로부터 미니배치 16개씩을 가져와 images, labels에 준비
      steps += 1
      # 0. Data를 GPU로 보내기
      images, labels = images.to(device), labels.to(device)

      # 1. 입력 데이터 준비
      # not Flatten!!
      # images.resize_(images.size()[0], 784) # 16, 1, 28, 28

      # 2. 전방향(Forward) 예측 
      outputs = model.forward(images) # 예측
      loss = criterion(outputs, labels) # 예측과 결과를 통해 Cross Entropy Loss 반환

      # 3. 역방향(Backward) 오차(Gradient) 전파
      optimizer.zero_grad() # 파이토치에서 gradient가 누적되지 않게 하기 위해
      loss.backward()

      # 4. 경사하강법으로 모델 파라미터 업데이트
      optimizer.step() # W <- W -lr*Gradient

      train_loss += loss.item()
      if (steps % steps_per_epoch) == 0: # step : 2500, .... (epoch 마다)
        model.eval() # 배치 정규화, 드롭아웃이 적용될 때는 model.forward 연산이 training때와 다르므로 반드시 설정
        valid_loss, valid_accuracy = validation(model, validloader, criterion)

        # tensorboad 시각화를 위한 로그 이벤트 등록
        writer.add_scalar("Loss/train", train_loss/len(trainloader), epoch)
        writer.add_scalar("Loss/valid", valid_loss/len(validloader), epoch)
        writer.add_scalars("Loss/train and valid",
                          {'train' : train_loss/len(trainloader),
                          'valid' : valid_loss/len(validloader)}, epoch)
        
        writer.add_scalar("Valid Accuracy", valid_accuracy/len(validloader), epoch)

        print('Epoch : {}/{}.....'.format(epoch+1, epochs),
              'Train Loss : {:.3f}'.format(train_loss/len(trainloader)),
              'Valid Loss : {:.3f}'.format(valid_loss/len(validloader)),
              'Valid Accuracy : {:.3f}'.format(valid_accuracy/len(validloader)))
        
        # Best model 저장
        # option 1
        # if valid_loss < min_loss:
        #   min_loss = valid_loss
        #   torch.save(model.state_dict(), 'best_checkpoint.pth')

        # option 2
        if valid_accuracy > max_accuracy: 
          max_accuracy = valid_accuracy
          torch.save(model.state_dict(), 'best_checkpoint.pth')

        # Early Stopping (조기 종료)
        if valid_loss > min_loss:
          trigger += 1 # valid loss가 min_loss 를 갱신하지 못할때마다 증가
          print('trigger : ', trigger )
          if trigger > patience:
            print('Early Stopping!!!')
            print('Traning step is finished!!')
            writer.flush()  
            return   
        else:
          trigger = 0
          min_loss = valid_loss

        train_loss = 0
        model.train()

        # Learning Rate Scheduler
        scheduler.step(valid_loss)

  writer.flush()

epochs=55
train(model, epochs, criterion, optimizer)
```

![image](https://user-images.githubusercontent.com/106129152/210048434-c21542a7-b203-4e2f-bd2b-f31e834d403e.png)

![image](https://user-images.githubusercontent.com/106129152/210048459-b42da72a-855d-4a9d-bd17-af06f22ba34e.png)

![image](https://user-images.githubusercontent.com/106129152/210048472-7cb66dfc-0811-48a3-901d-167fc4b9929c.png)

# **Cat and Dog - (3) VGGNet 추가**

이번에는 **Global Average Pooling** 적용까지!

```python
class VGG16(nn.Module):
  def __init__(self):
    super().__init__()
    self.block1 = nn.Sequential(
        nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1, stride=1),
        nn.BatchNorm2d(64),
        nn.ReLU(),
        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1, stride=1),
        nn.BatchNorm2d(64),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=2, stride=2)  # 64 x 112 x 112
    )
    self.block2 = nn.Sequential(
        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1, stride=1),
        nn.BatchNorm2d(128),
        nn.ReLU(),
        nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1, stride=1),
        nn.BatchNorm2d(128),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=2, stride=2)  # 128 x 56 x 56      
    )
    self.block3 = nn.Sequential(
        nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1, stride=1),
        nn.BatchNorm2d(256),
        nn.ReLU(),
        nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1, stride=1),
        nn.BatchNorm2d(256),
        nn.ReLU(),
        nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1, stride=1),
        nn.BatchNorm2d(256),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=2, stride=2)  # 256 x 28 x 28   
    )
    self.block4 = nn.Sequential(
        nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1, stride=1),
        nn.BatchNorm2d(512),
        nn.ReLU(),
        nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1, stride=1),
        nn.BatchNorm2d(512),
        nn.ReLU(),
        nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1, stride=1),
        nn.BatchNorm2d(512),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=2, stride=2) # 512 x 14 x 14    
    )            
    self.block5 = nn.Sequential(
        nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1, stride=1),
        nn.BatchNorm2d(512),
        nn.ReLU(),
        nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1, stride=1),
        nn.BatchNorm2d(512),
        nn.ReLU(),
        nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1, stride=1),
        nn.BatchNorm2d(512),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size=2, stride=2) # 512 x 7 x 7     
    )

    self.avg_pool = nn.AdaptiveAvgPool2d((1, 1)) # 512

    self.classifier = nn.Sequential(
        nn.Linear(in_features=512, out_features=64),
        nn.ReLU(),
        nn.Linear(in_features=64, out_features=2)
    )
  
  def forward(self, x):
    x = self.block1(x)
    x = self.block2(x)
    x = self.block3(x)
    x = self.block4(x)
    x = self.block5(x)
    x = self.avg_pool(x)
    # x = x.view(-1, 512)
    x = torch.flatten(x, 1) # 1번 dim 부터 flatten
    x = self.classifier(x)
    return x
```

```python
learning_rate = 0.001
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr = learning_rate)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=7, factor=0.1, verbose=True)
```

```python
def validation(model, validloader, criterion):
  # 전방향 예측후 나온 점수(logits)의 최대값을 최종 예측으로 준비
  # 이 최종 예측과 정답을 비교
  # 전체 중 맞은 것의 개수 비율을 정확도(accuracy)로 계산
  valid_accuracy = 0
  valid_loss = 0

  # 전방향 예측을 구할 때는 gradient가 필요가 없음
  with torch.no_grad():
    for images, labels in validloader: # 10000개의 데이터에 대해 16개씩(미니배치 사이즈) 10000/16번을 iterations
      # 0. Data를 GPU로 보내기
      images, labels = images.to(device), labels.to(device)

      # 1. 입력데이터 준비
      # not Flatten!!
      # images.resize_(images.size()[0], 784) # 16, 1, 28, 28
      
      # 2. 전방향(Forward) 예측 
      logits = model.forward(images) # 점수 반환
      _, preds = torch.max(logits, 1) # 16개에 대한 최종 예측
      # preds= probs.max(dim=1)[1] 
      correct = (preds == labels).sum()

      accuracy = correct / images.shape[0]
      loss = criterion(logits, labels) # 16개에 대한 loss
      
      valid_accuracy += accuracy
      valid_loss += loss.item() # tensor 값을 꺼내옴
    

  return valid_loss, valid_accuracy # validloader 전체 대한 총 loss, 총 accuracy
```

```python
def train(model, epochs, criterion, optimizer):
  steps = 0
  min_loss = 10000
  max_accuracy = 0

  trigger = 0
  patience = 13 # for Early stopping

  # 1 에폭(epoch)당 반복수
  #steps_per_epoch = len(trainset)/batch_size # 2500 iterations
  steps_per_epoch = len(trainloader) # 2500 iterations

  for epoch in range(epochs):
    model.train()
    train_loss = 0
    for images, labels in iter(trainloader): # 이터레이터로부터 미니배치 16개씩을 가져와 images, labels에 준비
      steps += 1
      # 0. Data를 GPU로 보내기
      images, labels = images.to(device), labels.to(device)

      # 1. 입력 데이터 준비
      # not Flatten!!
      # images.resize_(images.size()[0], 784) # 16, 1, 28, 28

      # 2. 전방향(Forward) 예측 
      outputs = model.forward(images) # 예측
      loss = criterion(outputs, labels) # 예측과 결과를 통해 Cross Entropy Loss 반환

      # 3. 역방향(Backward) 오차(Gradient) 전파
      optimizer.zero_grad() # 파이토치에서 gradient가 누적되지 않게 하기 위해
      loss.backward()

      # 4. 경사하강법으로 모델 파라미터 업데이트
      optimizer.step() # W <- W -lr*Gradient

      train_loss += loss.item()
      if (steps % steps_per_epoch) == 0: # step : 2500, .... (epoch 마다)
        model.eval() # 배치 정규화, 드롭아웃이 적용될 때는 model.forward 연산이 training때와 다르므로 반드시 설정
        valid_loss, valid_accuracy = validation(model, validloader, criterion)

        # tensorboad 시각화를 위한 로그 이벤트 등록
        writer.add_scalar("Loss/train", train_loss/len(trainloader), epoch)
        writer.add_scalar("Loss/valid", valid_loss/len(validloader), epoch)
        writer.add_scalars("Loss/train and valid",
                          {'train' : train_loss/len(trainloader),
                          'valid' : valid_loss/len(validloader)}, epoch)
        
        writer.add_scalar("Valid Accuracy", valid_accuracy/len(validloader), epoch)

        print('Epoch : {}/{}.....'.format(epoch+1, epochs),
              'Train Loss : {:.3f}'.format(train_loss/len(trainloader)),
              'Valid Loss : {:.3f}'.format(valid_loss/len(validloader)),
              'Valid Accuracy : {:.3f}'.format(valid_accuracy/len(validloader)))
        
        # Best model 저장
        # option 1
        # if valid_loss < min_loss:
        #   min_loss = valid_loss
        #   torch.save(model.state_dict(), 'best_checkpoint.pth')

        # option 2
        if valid_accuracy > max_accuracy: 
          max_accuracy = valid_accuracy
          torch.save(model.state_dict(), 'best_checkpoint.pth')

        # Early Stopping (조기 종료)
        if valid_loss > min_loss:
          trigger += 1 # valid loss가 min_loss 를 갱신하지 못할때마다 증가
          print('trigger : ', trigger )
          if trigger > patience:
            print('Early Stopping!!!')
            print('Traning step is finished!!')
            writer.flush()  
            return   
        else:
          trigger = 0
          min_loss = valid_loss

        train_loss = 0
        model.train()

        # Learning Rate Scheduler
        scheduler.step(valid_loss)

  writer.flush()
```

![image](https://user-images.githubusercontent.com/106129152/210048518-5996c1f9-d92e-4e2a-a43f-b26657f1e253.png)

![image](https://user-images.githubusercontent.com/106129152/210048529-0afdbfe4-2222-4ed0-9df3-e56ba1313c0d.png)

**<best model>**

![image](https://user-images.githubusercontent.com/106129152/210048538-07e68af7-7bcd-4ca1-a442-b41c31396fbb.png)

**<last model>**

![image](https://user-images.githubusercontent.com/106129152/210048556-1b1f2851-8460-4bc3-a70e-326dc793454f.png)

# **Cat and Dog - (4) 전이학습**

→ [전이학습이란?](https://dacon.io/forum/405988)

이번에는 기존에 훈련된 모델을 이용해서 학습을 해보자.

torchvision의 models 사용!

![image](https://user-images.githubusercontent.com/106129152/210048568-83757721-aea5-4f75-a4bf-32a52e743103.png)

또, 데이터를 적재할 때 **RandomHorizontalFlip**을 추가로 적용했다.

```python
transform = transforms.Compose([transforms.Resize([224, 224]), 
                                transforms.RandomHorizontalFlip(p=0.3),
                                transforms.ToTensor()])
```

```python
# 모델 불러오기
model = models.vgg16_bn(pretrained=True)

for parameter in model.parameters():
  parameter.requires_grad = False # 학습이 안되게 가중치 고정
for parameter in model.classifier.parameters():
  parameter.requires_grad = True # task에 맞게 다시 학습할 예정

model.classifier[3] = nn.Linear(in_features=4096, out_features=512)
model.classifier[6] = nn.Linear(in_features=512, out_features=2)

model.classifier
# Sequential(
#   (0): Linear(in_features=25088, out_features=4096, bias=True)
#   (1): ReLU(inplace=True)
#   (2): Dropout(p=0.5, inplace=False)
#   (3): Linear(in_features=4096, out_features=4096, bias=True)
#   (4): ReLU(inplace=True)
#   (5): Dropout(p=0.5, inplace=False)
#   (6): Linear(in_features=4096, out_features=1000, bias=True)
# )

model.to(device)
```

```python
learning_rate = 0.0001 
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr = learning_rate)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=7, factor=0.1, verbose=True)
```

```python
def validation(model, validloader, criterion):
  # 전방향 예측후 나온 점수(logits)의 최대값을 최종 예측으로 준비
  # 이 최종 예측과 정답을 비교
  # 전체 중 맞은 것의 개수 비율을 정확도(accuracy)로 계산
  valid_accuracy = 0
  valid_loss = 0

  # 전방향 예측을 구할 때는 gradient가 필요가 없음
  with torch.no_grad():
    for images, labels in validloader: # 10000개의 데이터에 대해 16개씩(미니배치 사이즈) 10000/16번을 iterations
      # 0. Data를 GPU로 보내기
      images, labels = images.to(device), labels.to(device)

      # 1. 입력데이터 준비
      # not Flatten!!
      # images.resize_(images.size()[0], 784) # 16, 1, 28, 28
      
      # 2. 전방향(Forward) 예측 
      logits = model.forward(images) # 점수 반환
      _, preds = torch.max(logits, 1) # 16개에 대한 최종 예측
      # preds= probs.max(dim=1)[1] 
      correct = (preds == labels).sum()

      accuracy = correct / images.shape[0]
      loss = criterion(logits, labels) # 16개에 대한 loss
      
      valid_accuracy += accuracy
      valid_loss += loss.item() # tensor 값을 꺼내옴
    

  return valid_loss, valid_accuracy # validloader 전체 대한 총 loss, 총 accuracy
```

```python
def train(model, epochs, criterion, optimizer):
  steps = 0
  min_loss = 10000
  max_accuracy = 0

  trigger = 0
  patience = 13 # for Early stopping

  # 1 에폭(epoch)당 반복수
  #steps_per_epoch = len(trainset)/batch_size # 2500 iterations
  steps_per_epoch = len(trainloader) # 2500 iterations

  for epoch in range(epochs):
    model.train()
    train_loss = 0
    for images, labels in iter(trainloader): # 이터레이터로부터 미니배치 16개씩을 가져와 images, labels에 준비
      steps += 1
      # 0. Data를 GPU로 보내기
      images, labels = images.to(device), labels.to(device)

      # 1. 입력 데이터 준비
      # not Flatten!!
      # images.resize_(images.size()[0], 784) # 16, 1, 28, 28

      # 2. 전방향(Forward) 예측 
      outputs = model.forward(images) # 예측
      loss = criterion(outputs, labels) # 예측과 결과를 통해 Cross Entropy Loss 반환

      # 3. 역방향(Backward) 오차(Gradient) 전파
      optimizer.zero_grad() # 파이토치에서 gradient가 누적되지 않게 하기 위해
      loss.backward()

      # 4. 경사하강법으로 모델 파라미터 업데이트
      optimizer.step() # W <- W -lr*Gradient

      train_loss += loss.item()
      if (steps % steps_per_epoch) == 0: # step : 2500, .... (epoch 마다)
        model.eval() # 배치 정규화, 드롭아웃이 적용될 때는 model.forward 연산이 training때와 다르므로 반드시 설정
        valid_loss, valid_accuracy = validation(model, validloader, criterion)

        # tensorboad 시각화를 위한 로그 이벤트 등록
        writer.add_scalar("Loss/train", train_loss/len(trainloader), epoch)
        writer.add_scalar("Loss/valid", valid_loss/len(validloader), epoch)
        writer.add_scalars("Loss/train and valid",
                          {'train' : train_loss/len(trainloader),
                          'valid' : valid_loss/len(validloader)}, epoch)
        
        writer.add_scalar("Valid Accuracy", valid_accuracy/len(validloader), epoch)

        print('Epoch : {}/{}.....'.format(epoch+1, epochs),
              'Train Loss : {:.3f}'.format(train_loss/len(trainloader)),
              'Valid Loss : {:.3f}'.format(valid_loss/len(validloader)),
              'Valid Accuracy : {:.3f}'.format(valid_accuracy/len(validloader)))
        
        # Best model 저장
        # option 1
        # if valid_loss < min_loss:
        #   min_loss = valid_loss
        #   torch.save(model.state_dict(), 'best_checkpoint.pth')

        # option 2
        if valid_accuracy > max_accuracy: 
          max_accuracy = valid_accuracy
          torch.save(model.state_dict(), 'best_checkpoint.pth')

        # Early Stopping (조기 종료)
        if valid_loss > min_loss:
          trigger += 1 # valid loss가 min_loss 를 갱신하지 못할때마다 증가
          print('trigger : ', trigger )
          if trigger > patience:
            print('Early Stopping!!!')
            print('Traning step is finished!!')
            writer.flush()  
            return   
        else:
          trigger = 0
          min_loss = valid_loss

        train_loss = 0
        model.train()

        # Learning Rate Scheduler
        scheduler.step(valid_loss)

  writer.flush()

epochs=50
train(model, epochs, criterion, optimizer)
```

![image](https://user-images.githubusercontent.com/106129152/210048580-18ce8b18-36fe-45ec-8739-c8e561e69eee.png)

![image](https://user-images.githubusercontent.com/106129152/210048601-6e7ebf00-aae5-4d88-a161-4e8d30784067.png)

![image](https://user-images.githubusercontent.com/106129152/210048612-84e9b6f8-32c8-429b-875e-c929be06d236.png)

정말 높은 성능을 보여준다!

# 파이토치 데이터 불러오기

```python
import torch # 파이토치 기본 라이브러리 
import torchvision # 이미지 관련 된 파이토치 라이브러리
from torchvision import datasets # 토치비전에서 제공하는 데이터셋
from torchvision import transforms # 이미지 전처리 기능들을 제공하는 라이브러리
from torch.utils.data import DataLoader # 데이터를 모델에 사용할 수 있도록 적재해 주는 라이브러리
from torch.utils.data import random_split
import numpy as np 
import matplotlib.pyplot as plt
```

## 1. 파이토치 제공 데이터 사용하기

```python
transform = transforms.Compose([transforms.RandomHorizontalFlip(p=0.5),
                                transforms.ToTensor()])

trainset = datasets.CIFAR10('CIFAR10_data/', download=True, train=True, transform=transform)
testset = datasets.CIFAR10('CIFAR10_data/', download=True, train=False, transform=transform)
```

```python
batch_size = 16
trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)
testloader = DataLoader(testset, batch_size=batch_size, shuffle=False)
```

## 2. 같은 클래스별로 폴더를 정리한 경우

```python
!pip install kaggle
from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/

# permmision warning 방지
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d tongpython/cat-and-dog
!unzip cat-and-dog.zip -d catanddog/
data_dir = './catanddog/'
```

```python
transform = transforms.Compose([transforms.Resize([256, 256]), transforms.ToTensor()])

trainset = datasets.ImageFolder(root= data_dir + 'training_set/training_set/', transform=transform)
testnset = datasets.ImageFolder(root= data_dir + 'test_set/test_set/', transform=transform)

batch_size = 16
trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)
testloader = DataLoader(testset, batch_size=batch_size, shuffle=False)

train_iter = iter(trainloader)
images, labels = next(train_iter)
images.shape, labels.shape
# (torch.Size([16, 3, 256, 256]), torch.Size([16]))
```

## 3. 나만의 데이터셋 만들기

파이토치에서 제공하는 Dataset 클래스(추상 클래스)를 활용해 데이터셋 객체를 만들 수 있다.
Dataset을 상속받은 다음 특수 메서드인 `__len__()`과 `__getitem__()`을 재정의(오버라이딩)

```
# __len__() : 데이터셋의 크기를 반환
# __getitem__() : 인덱스를 전달받아 인덱스에 해당하는 데이터를 반환
# __len__(), __getitem__()의 호출 방식 (일반적인 메서드와는 다름)
# __len__()는 len(MyDataset) 형태로 호출
# __getitem__()는 MyDataset[index]
```

```python
from torch.utils.data import Dataset
from PIL import Image # PIL에서 이미지를 읽는 함수
import cv2
import glob

class CatandDogDataset(Dataset):
  def __init__(self, root, transform):
    self.filepaths = glob.glob(root + '*/*.jpg')
    self.transform = transform

  def __len__(self):
    return len(self.filepaths)

  def __getitem__(self, index):
    image_path = self.filepaths[index]
    # pytorch dataset에서는 image 데이터를 PIL 형태로 읽음
    image = Image.open(image_path)
    image_transformed = self.transform(image)

    dir_label = image_path.split('/')[-2]
    if dir_label == 'cats':
      label = 0
    else: # 'dog'
      label =1

    return image_transformed, label
```

```python
transform = transforms.Compose([transforms.Resize([256, 256]), transforms.ToTensor()])

trainset = CatandDogDataset(root = data_dir + 'training_set/training_set/', transform=transform)
testset = CatandDogDataset(root = data_dir + 'test_set/test_set/', transform=transform)

len(trainset), len(testset) # trainset.__len__(), testset.__len__()
# (8005, 2023)

print(type(trainset[0][0]), type(trainset[0][1]))
# <class 'torch.Tensor'> <class 'int'>

trainset[0][0] # trainset.__getitem__(0)[0] : 이미지
trainset[0][1] # trainset.__getitem__(0)[1] : 레이블
trainset[0][0].shape # torch.Size([3, 256, 256])

plt.imshow(trainset[0][0].permute(1, 2, 0))
```

![image](https://user-images.githubusercontent.com/106129152/210048632-4eb36a01-ffe3-4c8e-bc2b-da8f01728a91.png)

```python
batch_size = 32
trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)
testloader = DataLoader(testset, batch_size=batch_size, shuffle=False)
```

```python
train_iter = iter(trainloader)
images, labels = next(train_iter)

grid_image = torchvision.utils.make_grid(images)
# grid_image.shape -> torch.Size([3, 518, 2066])
plt.figure(figsize=(20, 200))
plt.imshow(grid_image.permute(1, 2, 0))
```

![image](https://user-images.githubusercontent.com/106129152/210048644-5afb92e5-6d3b-4110-83ea-70f689ebdd68.png)

### **참고 : torchvision.transforms**

```python
from google.colab import files
uploaded_img = files.upload()
```

```python
trans = transforms.Resize((224, 224)) # 사이즈를 변경해주는 변환기
image = Image.open('cactus.png')
resized_image = trans(image) # torchvision의 전처리기(변환기)는 PIL 포맷의 이미지 기대 
resized_image # type : PIL
```

![image](https://user-images.githubusercontent.com/106129152/210048667-83b77dea-b6a2-4b4f-a7ad-fec563af148f.png)

```python
trans = transforms.Grayscale() # Grayscale로 변환해주는 전처리기(변환기)
image = Image.open('cactus.png')
gray_image = trans(image)
gray_image
```

![image](https://user-images.githubusercontent.com/106129152/210048679-e26cb609-0e62-424b-8534-39189ed98dcf.png)

```python
trans = transforms.RandomRotation(degrees=(0, 180)) # Random하게 Roatation해주는 전처리기(변환기)
image = Image.open('cactus.png')
rotated_image = trans(image)
rotated_image
```

![image](https://user-images.githubusercontent.com/106129152/210048694-d9a60e25-de85-4774-bf5e-758eb0592c83.png)

```python
trans = transforms.RandomCrop(size=(128, 128)) # random하게 Crop 시켜주는 전처리기(변환기)
image = Image.open('cactus.png')
cropped_image = trans(image)
cropped_image
```

![image](https://user-images.githubusercontent.com/106129152/210048715-a6385de5-4bb2-4119-a61e-385bc84edd4f.png)

## 4. **외부 전처리기 이용하기 (Albumentation 제공)**

torchvison 변환기와 비교했을 때 처리 속도가 빠르고, 더 다양한 이미지 변환을 제공하며
Object Detection, Segmentation 에서도 사용이 가능한 장점이 있다.

```python
import albumentations as A
from albumentations.pytorch import ToTensorV2
from torch.utils.data import Dataset
from PIL import Image # PIL에서 이미지를 읽는 함수
import cv2
import glob

class CatandDogDataset(Dataset):
  def __init__(self, root, transform):
    self.filepaths = glob.glob(root + '*/*.jpg')
    self.transform = transform

  def __len__(self):
    return len(self.filepaths)

  def __getitem__(self, index):
    image_path = self.filepaths[index]

    # albumentation 전처리기에서는 numpy ndarray를 기대
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    transformed_data = self.transform(image=image)

    dir_label = image_path.split('/')[-2]
    if dir_label == 'cats':
      label = 0
    else: # 'dog'
      label =1

    return transformed_data['image'], label
```

정규화는 되어있지 않은 상태이기 때문에 바로 사용하기에는 무리가 있다.

```python
transform = A.Compose([A.Resize(224, 224), ToTensorV2()]) # 이미지 사이즈 조정 → 텐서로 변환 (정규화는 X)

trainset = CatandDogDataset(root=data_dir + 'training_set/training_set/', transform=transform)
testset = CatandDogDataset(root=data_dir + 'test_set/testset/', transform=transform)

len(trainset) # 8005
trainset[300][0].shape # torch.Size([3, 224, 224])
```

### **참고 : Albumentation 제공 전처리기 예시**

```python
from google.colab import files
uploaded_img = files.upload()

image = cv2.imread('dog.png')
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

def show_image(image):
  plt.figure(figsize=(8, 8))
  plt.imshow(image)
  plt.axis('off')

show_image(image)
print(image.shape) # height * width * channel
# (558, 557, 3)
```

![image](https://user-images.githubusercontent.com/106129152/210048734-8e185737-f927-4e49-8a7b-03ecd5cbde14.png)

```python
# 1. 이미지 크기 변환

augmentor = A.Resize(450, 650)
aug_img = augmentor(image=image)['image'] # Albumentation에서는 image뿐만 아니라 label도 전처리 가능 : augmentor(image=image, label=label)
show_image(aug_img)
print(aug_img.shape)
# (450, 650, 3)
```

![image](https://user-images.githubusercontent.com/106129152/210048747-3fda91ae-6241-4300-8361-bd7b36c77f0f.png)

```python
# 2. RandomBrightnessContrast : 이미지 밝기와 대비 조절 변환기

augmentor = A.RandomBrightnessContrast(brightness_limit=0.2, # -0.2 ~ 0.2 범위에서 임의로 밝기 조절 / -1: 검은색, 1: 흰색
                           contrast_limit=0.2, # 명암비 조절 
                           p=0.3) # 적용 확률 설정 (30% 정도로)
          
aug_img = augmentor(image=image)['image']
show_image(aug_img)
print(aug_img.shape)
# (558, 557, 3)
```

![image](https://user-images.githubusercontent.com/106129152/210048759-17091409-3968-47b7-999e-df571c53cbd7.png)

```python
def show_images(images, labels, ncols=4, title=None):
  figure, axes = plt.subplots(figsize=(22, 4), nrows=1, ncols=ncols)
  for i in range(ncols):
    axes[i].imshow(images[i])
    axes[i].set_title(labels[i])

def repeat_aug(count=4, org_image=None, label=None, augmentor=None):
  image_list = [org_image]
  label_list = ['Original']

  for i in range(count):
    aug_image = augmentor(image=org_image)['image']
    image_list.append(aug_image)
    label_list.append(label)
  show_images(image_list, label_list, ncols=count+1)
```

```python
augmentor = A.RandomBrightnessContrast(brightness_limit=0.2, 
                           contrast_limit=0.2, 
                           p=0.8) # 육안상 확인을 위해 임시로 확률을 높임

repeat_aug(count=4, org_image=image, label='RandomBrightnessContrast', augmentor=augmentor)
```

![image](https://user-images.githubusercontent.com/106129152/210048773-cfbf775e-2915-43df-b098-478d74b6d8e0.png)
