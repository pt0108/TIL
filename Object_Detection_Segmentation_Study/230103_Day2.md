> 부트캠프 12주차 2023년 1월 3일

# Detection

## 1. Installation

### MMDetection github

**google mmdetection 검색 :**  [https://github.com/open-mmlab/mmdetection](https://github.com/open-mmlab/mmdetection)

### Install

**installation :** [https://github.com/open-mmlab/mmdetection/blob/master/docs/en/get_started.md/#Installation](https://github.com/open-mmlab/mmdetection/blob/master/docs/en/get_started.md/#Installation)

### 1. Install MMCV using MIM

```python
!pip3 install openmim
!mim install mmcv-full
```

### 2. Install MMdetection from ther source

```python
!git clone https://github.com/open-mmlab/mmdetection.git
%cd mmdetection
!pip install -e .
```

### 3. Verification

```python
import mmdet
print(mmdet.__version__)
# 2.26.0
```

### 4. Inference

github home에서 Overview of Benchmark and Model Zoo 섹션 [확인](https://github.com/open-mmlab/mmdetection/tree/master/configs/yolo)

```python
!mim download mmdet --config yolov3_mobilenetv2_320_300e_coco --dest .

from mmdet.apis import init_detector, inference_detector

config_file = 'yolov3_mobilenetv2_320_300e_coco.py'
checkpoint_file = 'yolov3_mobilenetv2_320_300e_coco_20210719_215349-d18dff72.pth'
model = init_detector(config_file, checkpoint_file, device='cpu')  # or device='cuda:0'
result = inference_detector(model, 'demo/demo.jpg')
```

```python
from mmdet.apis import show_result_pyplot

show_result_pyplot(model, 'demo/demo.jpg', result, score_thr=0.4)
```

![image](https://user-images.githubusercontent.com/106129152/210311685-7d8857a3-3d0e-4d36-821e-78c3eaeb21fe.png)

## 2. Infrence with Faster-RCNN

MS-COCO 데이터 기반으로 학습된 Faster RCNN Pretrained 모델을 활용하여 Inference 수행.

github의 demo 디렉토리 아래 [MMDet_Tutorial.ipynb](https://github.com/open-mmlab/mmdetection/blob/master/demo/MMDet_Tutorial.ipynb) 파일 참조!

### Download pretrained model

github의 configs/faster_rcnn 디렉토리의 Results and Models 섹션 확인

→ 1x : epoch 12회, 2x : epoch 24회

```python
# We download the pre-trained checkpoints for inference and finetuning.
!mkdir checkpoints
!wget -c https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth \
      -O checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth
```

### Initialize Detector and Inference

github의 demo 디렉토리 아래 [inference_demo.ipynb](https://github.com/open-mmlab/mmdetection/blob/master/demo/inference_demo.ipynb) 파일 참조

results는 list형으로 coco class의  0부터 79까지 class_id별로 80개의 array를 가짐. 
개별 array들은 각 클래스별로 5개의 값(좌표값과 class별로 confidence)을 가짐. 
개별 class별로 여러개의 좌표를 가지면 여러개의 array가 생성됨. 
좌표는 좌상단(xmin, ymin), 우하단(xmax, ymax) 기준. 
개별 array의 shape는 (Detection된 object들의 수, 5(좌표와 confidence))

```python
from mmdet.apis import init_detector, inference_detector

# mmdetection 디렉토리 기준으로 상대 경로 지정
config_file = 'configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py'
checkpoint_file = 'checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'

# config 파일과 pretrained 모델을 기반으로 Detector 모델을 생성. 
model = init_detector(config_file, checkpoint_file, device='cuda:0')  # or device='cpu'
# test sample에 대해 inference 수행
result = inference_detector(model, 'demo/demo.jpg')

type(result), len(result) # (list, 80)

result[0].shape, result[1].shape, result[2].shape, result[3].shape
# ((3, 5), (0, 5), (46, 5), (0, 5))
```

```python
from mmdet.apis import show_result_pyplot

show_result_pyplot(model, 'demo/demo.jpg', result, score_thr=0.4)
```

![image](https://user-images.githubusercontent.com/106129152/210312322-13512c97-092a-4318-8e83-cb9cbf9a8ef2.png)

### Video Inference

github의 demo 디렉토리 아래 [video_demo.py](https://github.com/open-mmlab/mmdetection/blob/master/demo/video_demo.py) 파일 참조

```python
!mkdir data

# !wget -c https://github.com/mue76/data/raw/master/fast_furious_Trim.mp4 \
#       -O data/fast_furious_Trim.mp4

import cv2
import mmcv

video_reader = mmcv.VideoReader('./data/fast_furious_Trim.mp4')
video_writer = None
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
video_writer = cv2.VideoWriter('./data/fast_furious_Trim_out.mp4', fourcc, video_reader.fps,(video_reader.width, video_reader.height))

for frame in mmcv.track_iter_progress(video_reader):
  result = inference_detector(model, frame)
  frame = model.show_result(frame, result, score_thr=0.5)

  video_writer.write(frame)

!cp ./data/fast_furious_Trim_out.mp4 /content/drive/MyDrive/Classroom/data/out
```

# Object Detection의 주요 dataset

## **[Pascal VOC Dataset](http://host.robots.ox.ac.uk/pascal/VOC/)**

 [Visual Object Classes Challenge 2012 (VOC2012)](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/#data)에서 다운로드 가능

```python
!wget "http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar"

!mkdir data
!tar -xvf VOCtrainval_11-May-2012.tar -C ./data
!cat ./data/VOCdevkit/VOC2012/Annotations/2007_000032.xml

import xml.etree.ElementTree as ET

xml_file = "./data/VOCdevkit/VOC2012/Annotations/2007_000032.xml"
```

→ 이미지 1개당 1개의 Annotation 파일을 가짐

![image](https://user-images.githubusercontent.com/106129152/210312378-e3c71110-61cb-485f-9def-8c999b76b14a.png)

```python
import os
import cv2
import matplotlib.pyplot as plt

img_dir = "./data/VOCdevkit/VOC2012/JPEGImages/"

tree = ET.parse(xml_file)
root = tree.getroot()

img_name = root.find('filename').text
img_size = root.find('size')
img_width = int(img_size.find('width').text)
img_height = int(img_size.find('height').text)

img_full_path = os.path.join(img_dir, img_name)
img = cv2.imread(img_full_path)
dst = img.copy()

objects = root.findall('object')
object_list = []

for obj in objects:
  box = obj.find("bndbox")
  x1 = int(box.find('xmin').text)
  y1 = int(box.find('ymin').text)
  x2 = int(box.find('xmax').text)
  y2 = int(box.find('ymax').text)

  bndbox_coor = (x1, y1, x2, y2)
  class_name = obj.find('name').text

  cv2.rectangle(dst, (x1, y1), (x2, y2), color=(0, 0, 255), thickness=1)
  cv2.putText(dst, class_name, (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), thickness=1 )

  object_dict = {'class_name' : class_name, 'bndbox_coor' : bndbox_coor}
  object_list.append(object_dict)

plt.figure(figsize=(10, 10))
plt.imshow(cv2.cvtColor(dst, cv2.COLOR_BGR2RGB))
```

![image](https://user-images.githubusercontent.com/106129152/210312406-0be99baa-7609-44b0-9f85-b302bd60842e.png)

## **[MS-COCO DataSet](https://cocodataset.org/#home)**

[홈페이지의 Download 메뉴](https://cocodataset.org/#download)에서 다운로드 가능

```python
!wget "http://images.cocodataset.org/zips/train2017.zip"
!wget "http://images.cocodataset.org/annotations/annotations_trainval2017.zip"

!mkdir ./data/mscoco
!unzip "train2017.zip" -d ./data/mscoco/
!unzip -q "annotations_trainval2017.zip" -d ./data/mscoco/
!wget "http://images.cocodataset.org/zips/val2017.zip"
!unzip "val2017.zip" -d ./data/mscoco/
```

→ 모든 이미지들에 대해서 1개의 Annotation 파일을 가짐

![image](https://user-images.githubusercontent.com/106129152/210312437-f5f60d0d-e8be-4889-8b41-ec2d78de6c0d.png)

```python
# http://json.parser.online.fr/ (사이트에서 변환)
!sudo apt-get install jq

!jq . ./data/mscoco/annotations/instances_val2017.json > output.json
!head -200 output.json
!tail -800 output.json
!grep -n 'annotations' output.json
!head -50400 output.json
```

### **pycocotools를 이용한 COCO 데이터 액세스**

[pycocotools 사용 방법](https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocoDemo.ipynb) 

```python
 # /content/data/mscoco/annotations/instances_val2017.json

dataDir='./data/mscoco'
dataType='val2017'
annFile='{}/annotations/instances_{}.json'.format(dataDir,dataType)
print(annFile)
# ./data/mscoco/annotations/instances_val2017.json
```

### **COCO API를 활용하기 위해 annotation 파일을 COCO 객체로 로드하기**

```python
# annotation 파일을 COCO객체로 로드하면 다양한 COCO객체의 API들을 이용하여 COCO DATASET 활용 가능
from pycocotools.coco import COCO

coco=COCO(annFile)
# loading annotations into memory...
# Done (t=1.20s)
# creating index...
# index created!
```

### **Category 정보 가져 오기**

```python
# getCatIds()는 COCO Dataset의 category id를 리스트로 반환
print(coco.getCatIds())
# 90개의 카테고리가 출력됨

# loadCats()는 category id 리스트를 입력받아 category들에 대한 세부 정보를 여러개의 딕셔너리를 개별 원소를 가지는 리스트로 반환
cats = coco.loadCats(coco.getCatIds())
```

```python
# COCO Category와 Super Category 출력
nms=[cat['name'] for cat in cats]
print('COCO categories: \n{}\n'.format(' '.join(nms)))

nms = set([cat['supercategory'] for cat in cats])
print('COCO supercategories: \n{}'.format(' '.join(nms)))

# COCO categories: 
# person bicycle car motorcycle airplane bus train truck boat traffic light fire hydrant stop sign parking meter bench bird cat dog horse sheep cow elephant bear zebra giraffe backpack umbrella handbag tie suitcase frisbee skis snowboard sports ball kite baseball bat baseball glove skateboard surfboard tennis racket bottle wine glass cup fork knife spoon bowl banana apple sandwich orange broccoli carrot hot dog pizza donut cake chair couch potted plant bed dining table toilet tv laptop mouse remote keyboard cell phone microwave oven toaster sink refrigerator book clock vase scissors teddy bear hair drier toothbrush

# COCO supercategories: 
# food indoor electronic appliance sports vehicle kitchen furniture accessory outdoor animal person
```

### **지정된 이미지를 데이터 세트에서 로드하기**

(출력 내용 생략 → 자세한 내용은 파일 확인)

```python
catIds = coco.getCatIds(catNms=['person','dog','skateboard']);
print(catIds)
# coco.getImgIds(catIds=catIds)는 해당 catogory id별로 한개의 image id을 임의로 출력
imgIds = coco.getImgIds(catIds=catIds )
print(imgIds)
# [1, 18, 41]
# [549220, 324158, 279278]

#loadImgs()는 인자로 들어온 image id에 대한 메타 정보를 딕셔너리를 개별 원소로 가지는 리스트로 반환
img = coco.loadImgs(324158)
print(img)

# 전체 리스트는 필요 없고 내부 딕셔너리만 필요하므로 [0]으로 내부 딕셔너리 추출 
print("\n내부 딕셔너리 파일 메타정보 추출")
img = coco.loadImgs(324158)[0]
print(img)
```

### **COCO 이미지를 다운로드 후 시각화**

```python
coco_url = img['coco_url']
print(coco_url)
# http://images.cocodataset.org/val2017/000000324158.jpg
```

```python
import urllib.request

def download_image(url, target_path):
  urllib.request.urlretrieve(url, target_path) 

download_image(img['coco_url'], './data/mscoco/' + img['file_name'])
```

```python
import cv2
import matplotlib.pyplot as plt
import pylab
%matplotlib inline

file_path = './data/mscoco/' + img['file_name']

image_array = cv2.imread(file_path)
image_array = cv2.cvtColor(image_array, cv2.COLOR_BGR2RGB)

plt.figure(figsize=(12, 14))
plt.axis('off')
plt.imshow(image_array)
plt.show()
```

![image](https://user-images.githubusercontent.com/106129152/210312474-ac681e8d-32e9-4717-8806-eab36d637684.png)

## **Instance Segmentation 시각화 (COCO API 활용)**

`getAnnIds()`로 특정 image에 해당하는 annotation id를 가져온 후에 이 id를 `loadAnns()`로 입력하여 해당 이미지의 모든 annotation 정보를 가져옴.

segmentation 정보는 polygon 형태로 되어 있음.

annotation 정보를 `coco.showAnns(anns)`에 입력하여 instance segmentation 시각화 수행.

```python
# 해당 image의 annotation을 가져오기 위해서 getAnnIds() 를 이용함. 인자로 image의 id(파일명이 아님)와 category id를 입력
# 하나의 image는 segmentation별로 여러개의 annotation을 가질 수 있음
annIds = coco.getAnnIds(imgIds=img['id'], catIds=catIds, iscrowd=None)
annIds # [10673, 638724, 2162813]

# loadAnns()에 annotation id를 리스트로 입력하면 annotation 정보들을 반환함. 
anns = coco.loadAnns(annIds)
```

```python
# showAnns( )는 annotation 정보들을 입력 받아서 Visualization 시켜줌. 단 먼저 matplotlib 객체로 원본 이미지가 먼저 로드되어 있어야 함. 
plt.figure(figsize=(12, 14))
plt.imshow(image_array)
plt.axis('off')

coco.showAnns(anns)
```

![image](https://user-images.githubusercontent.com/106129152/210312500-bbbd548e-8872-41c3-af81-1f5e45f79160.png)

```python
# coco api의 annToMask()를 이용하여 polygon을 mask 형태로 변환
mask = coco.annToMask(anns[2])

plt.figure(figsize=(12, 14))
plt.imshow(mask, cmap='gray')
plt.axis('off')
```

![image](https://user-images.githubusercontent.com/106129152/210312518-1f5b7ab1-8bdb-4703-a7e1-a4b806f7acdd.png)

# Faster RCNN Train(Kitti Tiny)

## 1. Installation

### MMDetection github

**google mmdetection 검색**

[https://github.com/open-mmlab/mmdetection](https://github.com/open-mmlab/mmdetection)

### Install

**installation 링크**

[https://github.com/open-mmlab/mmdetection/blob/master/docs/en/get_started.md/#Installation](https://github.com/open-mmlab/mmdetection/blob/master/docs/en/get_started.md/#Installation)

**1. Install MMCV using MIM**

```python
!pip3 install openmim
!mim install mmcv-full
```

**2. Install MMDetection from the source**

```python
!git clone https://github.com/open-mmlab/mmdetection.git
%cd mmdetection
!pip install -e .
```

**3. Verification**

```python
import mmdet
print(mmdet.__version__) # 2.27.0
```

**4. Inference**

github home에서 Overview of Benchmark and Model Zoo 섹션 확인

[https://github.com/open-mmlab/mmdetection/tree/master/configs/yolo](https://github.com/open-mmlab/mmdetection/tree/master/configs/yolo)

```python
!mim download mmdet --config yolov3_mobilenetv2_320_300e_coco --dest .

from mmdet.apis import init_detector, inference_detector

config_file = 'yolov3_mobilenetv2_320_300e_coco.py'
checkpoint_file = 'yolov3_mobilenetv2_320_300e_coco_20210719_215349-d18dff72.pth'
model = init_detector(config_file, checkpoint_file, device='cpu')  # or device='cuda:0'
inference_detector(model, 'demo/demo.jpg')
```

## 2. **Train A Detector on A Customized Dataset**

github의 demo 디렉토리 아래 [MMDet_Tutorial.ipynb](https://github.com/open-mmlab/mmdetection/blob/master/demo/MMDet_Tutorial.ipynb) 파일 참조

### **KITTI Tiny Dataset 다운로드**

[Original KITTY Dataset](https://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d) 에서 75장만 가져와 PNG에서 JPEG 포맷으로 압축 변환된 작은 용량의 데이터

```python
# download, decompress the data
!wget https://download.openmmlab.com/mmdetection/data/kitti_tiny.zip
!unzip kitti_tiny.zip > /dev/null
```

### **KITTI Tiny Dataset의 디렉토리 구조 보기**

```python
# Check the directory structure of the tiny data

# Install tree first
!apt-get -q install tree
!tree kitti_tiny
```

### **이미지와 레이블 보기**

```python
# Let's take a look at the dataset image
import mmcv
import matplotlib.pyplot as plt

img = mmcv.imread('kitti_tiny/training/image_2/000073.jpeg')
plt.figure(figsize=(15, 10))
plt.imshow(mmcv.bgr2rgb(img))
plt.show()
```

![image](https://user-images.githubusercontent.com/106129152/210312542-8ba37d4d-6688-4913-aa2c-a6f8ee861f40.png)

```python
# Check the label of a single image
!cat kitti_tiny/training/label_2/000073.txt
```

### **KITTY Tiny Dataset을 Middle Format으로 변환**

```
According to the KITTI's documentation, the first column indicates the class of the object,
and the 5th to 8th columns indicate the bboxes.
We need to read annotations of each image and convert them into middle format that
MMDetection can accept, as follows:

[
    {
        'filename': 'a.jpg',
        'width': 1280,
        'height': 720,
        'ann': {
            'bboxes': <np.ndarray> (n, 4) in (x1, y1, x2, y2) order,
            'labels': <np.ndarray> (n, ),
            'bboxes_ignore': <np.ndarray> (k, 4), (optional field)
            'labels_ignore': <np.ndarray> (k, 4) (optional field)
        }
    },
    ...
]
```

github의 mmdet/datasets 디렉토리 아래 [custom.py](https://github.com/open-mmlab/mmdetection/blob/master/mmdet/datasets/custom.py)에 CustomDataset 클래스 정의

새로 정의한 데이터셋이 CustomDataset을 상속받으므로 부모 클래스의 **`init**()`함수 적용

이 클래스 객체를 생성하는 것은 MMDetection Framework 내부에서 하며, 이 때 Config에서 설정한 Dataset 관련 내용(이미지, 라벨 등이 위치한 디렉토리)들이 **`init**()`을 통해 설정

```python
lines = mmcv.list_from_file('./kitti_tiny/training/label_2/000001.txt')
content = [line.strip().split(' ') for line in lines]

bbox_names = [x[0] for x in content]
bbox_names
# ['Truck', 'Car', 'Cyclist', 'DontCare', 'DontCare', 'DontCare', 'DontCare']

bboxes = [[float(info) for info in x[4:8]] for x in content]
```

```python
CLASSES = ('Car', 'Pedestrian', 'Cyclist')
cat2label = {k: i for i, k in enumerate(CLASSES)}
gt_bboxes = []
gt_labels = [] 
gt_bboxes_ignore = []
gt_labels_ignore = []
for bbox_name, bbox in zip(bbox_names, bboxes):
    if bbox_name in cat2label:
        gt_labels.append(cat2label[bbox_name])
        gt_bboxes.append(bbox)
    else:
        gt_labels_ignore.append(-1)
        gt_bboxes_ignore.append(bbox)
gt_labels, gt_bboxes
# ([0, 2], [[387.63, 181.54, 423.81, 203.12], [676.6, 163.95, 688.98, 193.93]])
```

```python
import copy
import os.path as osp

import mmcv
import numpy as np

from mmdet.datasets.builder import DATASETS
from mmdet.datasets.custom import CustomDataset

@DATASETS.register_module() # 데코레이터를 통해 CustomDataset(KittiTinyDataset)을 MMDetection FWK에 등록, 재등록 실패시 (force=True)
class KittiTinyDataset(CustomDataset):

    CLASSES = ('Car', 'Pedestrian', 'Cyclist')

    def load_annotations(self, ann_file):
        # self.ann_file : ./kitti_tiny/train.txt
        # self.img_prefix : ./kitti_tiny/training/image_2

        cat2label = {k: i for i, k in enumerate(self.CLASSES)}
        # load image list from file
        image_list = mmcv.list_from_file(self.ann_file)
    
        data_infos = []
        # convert annotations to middle format
        for image_id in image_list:
            filename = f'{self.img_prefix}/{image_id}.jpeg'
            image = mmcv.imread(filename)
            height, width = image.shape[:2]
    
            data_info = dict(filename=f'{image_id}.jpeg', width=width, height=height)
    
            # load annotations
            label_prefix = self.img_prefix.replace('image_2', 'label_2')
            lines = mmcv.list_from_file(osp.join(label_prefix, f'{image_id}.txt'))
    
            content = [line.strip().split(' ') for line in lines]
            bbox_names = [x[0] for x in content]
            bboxes = [[float(info) for info in x[4:8]] for x in content]
    
            gt_bboxes = []
            gt_labels = []
            gt_bboxes_ignore = []
            gt_labels_ignore = []
    
            # filter 'DontCare'
            for bbox_name, bbox in zip(bbox_names, bboxes):
                if bbox_name in cat2label:
                    gt_labels.append(cat2label[bbox_name])
                    gt_bboxes.append(bbox)
                else:
                    gt_labels_ignore.append(-1)
                    gt_bboxes_ignore.append(bbox)

            data_anno = dict(
                bboxes=np.array(gt_bboxes, dtype=np.float32).reshape(-1, 4),
                labels=np.array(gt_labels, dtype=np.long),
                bboxes_ignore=np.array(gt_bboxes_ignore,
                                       dtype=np.float32).reshape(-1, 4),
                labels_ignore=np.array(gt_labels_ignore, dtype=np.long))

            data_info.update(ann=data_anno)
            data_infos.append(data_info)

        return data_infos
```

### **Config 파일 수정하기**

아래 주어진 config 파일은 COCO dataset 기준으로 Faster R-CNN을 훈련 시키는 설정이므로 KITTI Dataset 에 맞게 수정 필요

```python
from mmcv import Config
cfg = Config.fromfile('./configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py')
print(f'Config:\n{cfg.pretty_text}')
```

### **Pretrained 모델 다운로드하기**

```python
# We download the pre-trained checkpoints for inference and finetuning.
!mkdir checkpoints
!wget -c https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth \
      -O checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth
```

```python
from mmdet.apis import set_random_seed

# Modify dataset type and path
cfg.dataset_type = 'KittiTinyDataset'
cfg.data_root = 'kitty_tiny/'

cfg.data.test.type = 'KittiTinyDataset'
cfg.data.test.data_root = 'kitty_tiny/'
cfg.data.test.ann_file = 'val.txt'
cfg.data.test.img_prefix = 'training/image_2'

cfg.data.train.type = 'KittiTinyDataset'
cfg.data.train.data_root = 'kitty_tiny/'
cfg.data.train.ann_file = 'train.txt'
cfg.data.train.img_prefix = 'training/image_2'

cfg.data.val.type = 'KittiTinyDataset'
cfg.data.val.data_root = 'kitty_tiny/'
cfg.data.val.ann_file = 'val.txt'
cfg.data.val.img_prefix = 'training/image_2'

# modify num classes of the model in box head
cfg.model.roi_head.bbox_head.num_classes = 3
# If we need to finetune a model based on a pre-trained detector, we need to
# use load_from to set the path of checkpoints.
cfg.load_from = 'checkpoints/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'

# Set up working dir to save files and logs.
cfg.work_dir = './tutorial_exps'

# The original learning rate (LR) is set for 8-GPU training.
# We divide it by 8 since we only use one GPU.
cfg.optimizer.lr = 0.02 / 8
cfg.lr_config.warmup = None
cfg.log_config.interval = 10

# Change the evaluation metric since we use customized dataset.
cfg.evaluation.metric = 'mAP'
# We can set the evaluation interval to reduce the evaluation times
cfg.evaluation.interval = 12
# We can set the checkpoint saving interval to reduce the storage cost
cfg.checkpoint_config.interval = 12

# Set seed thus the results are more reproducible
cfg.seed = 0
set_random_seed(0, deterministic=False)
cfg.device = 'cuda'
cfg.gpu_ids = range(1)

# We can also use tensorboard to log the training process
cfg.log_config.hooks = [
    dict(type='TextLoggerHook'),
    dict(type='TensorboardLoggerHook')]

# We can initialize the logger for training and have a look
# at the final config used for training
print(f'Config:\n{cfg.pretty_text}')
```

### **Config에서 설정한 KITTI Dataset 적용**

`build_dataset()` 가 호출될 때 MMDetection Framework에서 KittiTinyDataset 클래스 객체 생성

`load_annotations()` 함수가 호출될 때 `self.ann_file`, `self.data_root`, `self.img_prefix` 값이 Config에서 설정한 파일들로 적용되어 있음

```python
from mmdet.datasets import build_dataset

# Build dataset
datasets = [build_dataset(cfg.data.train)]
datasets
```

![image](https://user-images.githubusercontent.com/106129152/210312573-bce807ea-f855-4f3a-acfd-2693738f1b12.png)

### **Config에서 설정한 모델 적용**

```python
from mmdet.models import build_detector

# Build the detector
model = build_detector(cfg.model)
# Add an attribute for visualization convenience
model.CLASSES = datasets[0].CLASSES
```

### **학습 수행**

```python
from mmdet.apis import train_detector

# Create work_dir
mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))
train_detector(model, datasets, cfg, distributed=False, validate=True) # validate=True : validation 데이터로 evaluation
```

### **로그 확인**

```python
# load tensorboard in colab
%load_ext tensorboard

# see curves in tensorboard
%tensorboard --logdir ./tutorial_exps
```

### **학습된 모델로 예측**

```python
from mmdet.apis import show_result_pyplot

img = mmcv.imread('kitti_tiny/training/image_2/000068.jpeg')

model.cfg = cfg
result = inference_detector(model, img)
show_result_pyplot(model, img, result)
```

![image](https://user-images.githubusercontent.com/106129152/210312602-92b1a71d-cd7a-43e4-b607-06e83c9e7385.png)

```python
!mkdir data

!wget -c https://github.com/mue76/data/raw/master/songdo_Trim.mp4 \
      -O data/songdo_Trim.mp4

import cv2
import mmcv

video_reader = mmcv.VideoReader('./data/songdo_Trim.mp4')
video_writer = None
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
video_writer = cv2.VideoWriter('./data/songdo_Trim_out.mp4', fourcc, video_reader.fps,(video_reader.width, video_reader.height))

for frame in mmcv.track_iter_progress(video_reader):
  result = inference_detector(model, frame)
  frame = model.show_result(frame, result, score_thr=0.5)

  video_writer.write(frame)

if video_writer:
        video_writer.release()

!cp ./data/songdo_Trim_out.mp4 /content/drive/MyDrive/Classroom/data/out
```
