> 부트캠프 12주차 2023년 1월 6일

# RCNN Train(Pascal VOC)

## 1. Install

```python
!pip3 install openmim
!mim install mmcv-full

!git clone https://github.com/open-mmlab/mmdetection.git
%cd mmdetection
!pip install -e .

import mmdet
print(mmdet.__version__) # 2.27.0

!mim download mmdet --config yolov3_mobilenetv2_320_300e_coco --dest .

from mmdet.apis import init_detector, inference_detector

config_file = 'yolov3_mobilenetv2_320_300e_coco.py'
checkpoint_file = 'yolov3_mobilenetv2_320_300e_coco_20210719_215349-d18dff72.pth'
model = init_detector(config_file, checkpoint_file, device='cpu')  # or device='cuda:0'
inference_detector(model, 'demo/demo.jpg')
```

## 2. **Train A Detector on A Customized Dataset**

github의 demo 디렉토리 아래 [MMDet_Tutorial.ipynb](https://github.com/open-mmlab/mmdetection/blob/master/demo/MMDet_Tutorial.ipynb) 파일 참조

### **[Pascal VOC 2007 Dataset](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/index.html)**

Download the training/validation data (450MB tar file)

```python
!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar
!tar -xvf VOCtrainval_06-Nov-2007.tar > /dev/null 2>&1
```

### **Pascal VOC 2007 Dataset의 디렉토리 구조**

```python
!apt-get -q install tree
!tree ./VOCdevkit/VOC2007
```

### **MS-COCO 포맷으로 변환**

MMDetection은 Mask RCNN을 학습하기 위해서는 COCO 포맷을 가장 선호

CocoDataset으로 지정해야만, evaluation시 mask evaluation 정보 제공.(2023년 1월 기준)

[https://mmdetection.readthedocs.io/en/latest/tutorials/customize_dataset.html](https://mmdetection.readthedocs.io/en/latest/tutorials/customize_dataset.html)

Pascal voc 포맷을 Coco 포맷으로 변환할 수 있는 유틸리티를 활용 

→ 데이터 변환 [https://github.com/ISSResearch/Dataset-Converters](https://github.com/ISSResearch/Dataset-Converters)

***Dataset converter 패키지가 opencv를 3.4로 downgrade함에 유의***

```python
import cv2
cv2.__version__ # 4.6.0

!git clone https://github.com/ISSReasearch/Dataset-Converters.git
```

```
**%cd** 디렉토리 (현재 변경된 디렉토리 안에 있음)
**!cd** 디렉토리 (현재 라인에서만 디렉토리가 변경되고, 실행 후 이전 디렉토리로 복귀)
```

```python
!cd Dataset-Converters; pip install -r requirements.txt

# 런타임 다시 시작후 버전 확인
import cv2
cv2.__version__ # 3.4.18

%cd mmdetection
!mkdir ./coco_output
!cd Dataset-Converters; python convert.py --input-folder ../VOCdevkit/VOC2007 --output-folder ../coco_output \
                  --input-format VOCSEGM --output-format COCO --copy

# opencv 원래 버전 복구
!pip install opencv-python==4.6.0.66

# 런타임 다시 시작
import cv2
cv2.__version__ # 4.6.0

%cd mmdetection
```

### MS-COCO 포맷으로 변환된 json 파일 확인

```python
# http://json.parser.online.fr/ (사이트에서 변환)
!sudo apt-get install jq

!jq . ./coco_output/annotations/val.json > output.json
!head -100 output.json
!tail -200 output.json

!grep -n 'annotations' output.json
# 1283:  "annotations": [

# 1100 ~ 1500
!head -1500 output.json | tail -400
```

### **이미지와 레이블 보기**

```python
# Let's take a look at the dataset image
import mmcv
import matplotlib.pyplot as plt

img = mmcv.imread('./coco_output/train/000032.jpg')
plt.figure(figsize=(15, 10))
plt.imshow(mmcv.bgr2rgb(img))
plt.show()
```

![image](https://user-images.githubusercontent.com/106129152/210948060-446a1697-eac4-47b5-b566-dd3230f7b5f2.png)

**pycocotools를 이용한 COCO 데이터 액세스**

```python
dataDir = './coco_output'
dataType = 'train'
annFile = '{}/annotations/{}.json'.format(dataDir, dataType)
print(annFile)
# ./coco_output/annotations/train.json
```

**COCO API를 활용하기 위해 annotation 파일을 COCO 객체로 로드**

```python
from pycocotools.coco import COCO

coco=COCO(annFile)
```

**Category 정보 가져 오기**

```python
# getCatIds()는 COCO Dataset의 category id를 리스트로 반환
print(coco.getCatIds())
# [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]

# loadCats()는 category id 리스트를 입력받아 category들에 대한 세부 정보를 여러개의 딕셔너리를 개별 원소를 가지는 리스트로 반환
cats = coco.loadCats(coco.getCatIds())
cats

```

![image](https://user-images.githubusercontent.com/106129152/210948086-3565ed2b-8f76-45c3-bd29-dc0254c2efc0.png)

**지정된 이미지를 데이터 세트에서 로드**

```python
catIds = coco.getCatIds(catNms=['aeroplane','person']);
print(catIds)
# coco.getImgIds(catIds=catIds)는 해당 catogory id별로 한개의 image id을 임의로 출력
imgIds = coco.getImgIds(catIds=catIds )
print(imgIds)

# [1, 15]
# [116, 1, 12, 118]
```

```python
#loadImgs()는 인자로 들어온 image id에 대한 메타 정보를 딕셔너리를 개별 원소로 가지는 리스트로 반환
img = coco.loadImgs(1)
print(img)
# [{'file_name': '000032.jpg', 'height': 281, 'width': 500, 'id': 1}]

# 전체 리스트는 필요 없고 내부 딕셔너리만 필요하므로 [0]으로 내부 딕셔너리 추출 
print("\n내부 딕셔너리 파일 메타정보 추출")
img = coco.loadImgs(1)[0]
print(img)
# 내부 딕셔너리 파일 메타정보 추출
# {'file_name': '000032.jpg', 'height': 281, 'width': 500, 'id': 1}
```

**COCO 이미지를 경로 만들어서 시각화**

```python
import cv2
import matplotlib.pyplot as plt
import pylab
%matplotlib inline

file_path = './coco_output/train/' + img['file_name']

image_array = cv2.imread(file_path)
image_array = cv2.cvtColor(image_array, cv2.COLOR_BGR2RGB)

plt.figure(figsize=(12, 14))
plt.axis('off')
plt.imshow(image_array)
plt.show()
```

![image](https://user-images.githubusercontent.com/106129152/210948121-4bf2dc7d-ddea-42e8-b984-4f555450ace8.png)

### **Instance Segmentation 시각화 (COCO API 활용)**

`getAnnIds()`로 특정 image에 해당하는 annotation id를 가져온 후에 이 id를 `loadAnns()`로 입력하여 해당 이미지의 모든 annotation 정보를 가져옴.

segmentation 정보는 polygon 형태.

annotation 정보를 `coco.showAnns(anns)`에 입력하여 instance segmentation 시각화 수행.

```python
img
# {'file_name': '000032.jpg', 'height': 281, 'width': 500, 'id': 1}

# 해당 image의 annotation을 가져오기 위해서 getAnnIds() 를 이용함. 인자로 image의 id(파일명이 아님)와 category id를 입력
# 하나의 image는 segmentation별로 여러개의 annotation을 가질 수 있음
annIds = coco.getAnnIds(imgIds=img['id'], catIds=catIds, iscrowd=None)
annIds
# [1, 2, 3, 4]

# loadAnns()에 annotation id를 리스트로 입력하면 annotation 정보들을 반환함. 
anns = coco.loadAnns(annIds)
```

```python
# showAnns( )는 annotation 정보들을 입력 받아서 Visualization 시켜줌. 단 먼저 matplotlib 객체로 원본 이미지가 먼저 로드되어 있어야 함. 
plt.figure(figsize=(12, 14))
plt.imshow(image_array)
plt.axis('off')

coco.showAnns(anns)
```

![image](https://user-images.githubusercontent.com/106129152/210949107-28ff807a-01e5-4a5a-a7e2-e8380ac207be.png)

### **Pascal VOC Dataset을 COCO Format으로 변환**

github의 mmdet/datasets 디렉토리 아래 [custom.py](https://github.com/open-mmlab/mmdetection/blob/master/mmdet/datasets/coco.py)에 CoCoDataset 클래스 정의

새로 정의한 데이터셋이 CoCoDataset을 상속받으므로 부모 클래스의 `init()`함수 적용

이 클래스 객체를 생성하는 것은 MMDetection Framework 내부에서 하며 이 때 Config에서 설정한 Dataset 관련 내용(이미지, 라벨 등이 위치한 디렉토리)들이 `init()`을 통해 설정되어 있음

**CocoDataset을 상속받는 경우에는 별다른 수정사항없이 현 데이터셋의 클래스만 변경해주면됨**

```python
from mmdet.datasets.builder import DATASETS
from mmdet.datasets.coco import CocoDataset

@DATASETS.register_module(force=True) # 데코레이터를 통해 VOCDDataset을 MMDetection FWK에 등록, 재등록 실패시 (force=True)
class VOCDDataset(CocoDataset):   
    CLASSES = ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car',
                  'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse',
                  'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train',
                  'tvmonitor')
```

### **Config 파일 수정하기**

아래 주어진 config 파일은 COCO dataset 기준으로 Mask R-CNN 을 훈련 시키는 설정이므로 Pascal VOC Dataset 에 맞게 수정

```python
%pwd # /content/mmdetection

from mmcv import Config
cfg = Config.fromfile('./configs/mask_rcnn/mask_rcnn_r101_fpn_1x_coco.py')
print(f'Config:\n{cfg.pretty_text}')
```

### **Pretrained 모델 다운로드**

```python
# We download the pre-trained checkpoints for inference and finetuning.
!mkdir checkpoints
!wget -c https://download.openmmlab.com/mmdetection/v2.0/mask_rcnn/mask_rcnn_r101_fpn_1x_coco/mask_rcnn_r101_fpn_1x_coco_20200204-1efe0ed5.pth \
      -O checkpoints/mask_rcnn_r101_fpn_1x_coco_20200204-1efe0ed5.pth
```

```python
from mmdet.apis import set_random_seed

# Modify dataset type and path
cfg.dataset_type = 'VOCDDataset'
cfg.data_root = 'coco_output/'

cfg.data.test.type = 'VOCDDataset'
cfg.data.test.data_root = 'coco_output/'
cfg.data.test.ann_file = 'annotations/val.json'
cfg.data.test.img_prefix = 'val'

cfg.data.train.type = 'VOCDDataset'
cfg.data.train.data_root = 'coco_output/'
cfg.data.train.ann_file = 'annotations/train.json'
cfg.data.train.img_prefix = 'train'

cfg.data.val.type = 'VOCDDataset'
cfg.data.val.data_root = 'coco_output/'
cfg.data.val.ann_file = 'annotations/val.json'
cfg.data.val.img_prefix = 'val'

# modify num classes of the model in box head
cfg.model.roi_head.bbox_head.num_classes = 20
cfg.model.roi_head.mask_head.num_classes = 20

# If we need to finetune a model based on a pre-trained detector, we need to
# use load_from to set the path of checkpoints.
cfg.load_from = 'checkpoints/mask_rcnn_r101_fpn_1x_coco_20200204-1efe0ed5.pth'

# Set up working dir to save files and logs.
cfg.work_dir = '/content/drive/MyDrive/voc_log'

# The original learning rate (LR) is set for 8-GPU training.
# We divide it by 8 since we only use one GPU.
cfg.optimizer.lr = 0.02 / 8
# cfg.lr_config.warmup = None
cfg.log_config.interval = 10

# max epochs 12 
# cfg.runner.max_epochs = 12 # 기본값 변경시에만 설정

# 학습 시 Batch size 설정(단일 GPU 별 Batch size로 설정됨)
# samples_per_gpu 2 
# cfg.data.samples_per_gpu = 2 # 기본값 변경시에만설정

# Change the evaluation metric since we use customized dataset.
# note.
# detection evaluation에 map를 설정하면 iou가 0.5 일때만
# cocodata로 detection 할 때는 bbox를 설정해야 다양한 iou 기준 출력
# cocodata로 segmentation 할때는 bbox, segm을 지정
# CocoDataset의 경우 metric을 bbox로 설정해야 함.(mAP아님. bbox로 설정하면 mAP를 iou threshold를 0.5 ~ 0.95까지 변경하면서 측정)
cfg.evaluation.metric = ['bbox', 'segm']

# We can set the evaluation interval to reduce the evaluation times
cfg.evaluation.interval = 12
# We can set the checkpoint saving interval to reduce the storage cost
cfg.checkpoint_config.interval = 12
# cfg.workflow = [('train', 1), ('val', 1)] # val loss 나오도록

# Set seed thus the results are more reproducible
cfg.seed = 0
set_random_seed(0, deterministic=False)
cfg.device = 'cuda'
cfg.gpu_ids = range(1)

# We can also use tensorboard to log the training process
cfg.log_config.hooks = [
    dict(type='TextLoggerHook'),
    dict(type='TensorboardLoggerHook')]

# We can initialize the logger for training and have a look
# at the final config used for training
print(f'Config:\n{cfg.pretty_text}')
```

### **Config에서 설정한 Pascal VOC Dataset 적용**

`build_dataset()` 가 호출될 때 MMDetection Framework에서 VOCDataset 클래스 객체를 생성

`load_annotations()` 함수가 호출될 때 `self.ann_file`, `self.data_root`, `self.img_prefix` 값이 Config에서 설정한 파일들로 적용되어 있음

```python
from mmdet.datasets import build_dataset

# Build dataset
train_datasets = [build_dataset(cfg.data.train)]
test_dataset =build_dataset(cfg.data.test)

train_datasets # 209개의train imgage
```

![image](https://user-images.githubusercontent.com/106129152/210949135-c7b39115-687a-4aff-9281-31ef17dab3e6.png)

```python
train_datasets[0].__dict__.keys()
train_datasets[0].data_infos
train_datasets[0].pipeline
```

### **Config에서 설정한 모델 적용**

```python
from mmdet.models import build_detector

# Build the detector
model = build_detector(cfg.model)
# Add an attribute for visualization convenience
model.CLASSES = train_datasets[0].CLASSES
```

### **학습 수행**

```python
from google.colab import drive
drive.mount('/content/drive')
# Mounted at /content/drive

import os
from mmdet.apis import train_detector

# Create work_dir
mmcv.mkdir_or_exist(os.path.abspath(cfg.work_dir))
train_detector(model, train_datasets, cfg, distributed=False, validate=True) # validate=True : validation 데이터로 evaluation
```

![image](https://user-images.githubusercontent.com/106129152/210949168-af56f928-186e-4e0c-ae0b-c488f4fd258e.png)

### **로그 확인**

```python
# load tensorboard in colab
%load_ext tensorboard

# see curves in tensorboard
%tensorboard --logdir /content/drive/MyDrive/voc_log
```

![image](https://user-images.githubusercontent.com/106129152/210949191-c0646525-7873-4253-b5bc-955d96178be3.png)

### **학습된 모델로 예측**

**이미지 한 장 예측**

```python
from mmdet.apis import init_detector, inference_detector
from mmdet.apis import show_result_pyplot

img = mmcv.imread('./coco_output/train/000032.jpg')

model.cfg = cfg
result = inference_detector(model, img)
show_result_pyplot(model, img, result, score_thr=0.4)
```

![image](https://user-images.githubusercontent.com/106129152/210949218-b6160653-d1b5-45ae-9440-8c0335f5c056.png)

```python
from mmdet.apis import show_result_pyplot

checkpoint_file = '/content/drive/MyDrive/voc_log/epoch_12.pth'

# checkpoint 저장된 model 파일을 이용하여 모델을 생성, 이때 Config는 위에서 update된 config 사용. 
model_ckpt = init_detector(cfg, checkpoint_file, device='cuda:0')
img = mmcv.imread('./coco_output/train/000032.jpg') # BGR Image 사용 
#model_ckpt.cfg = cfg

result = inference_detector(model_ckpt, img)
show_result_pyplot(model_ckpt, img, result, score_thr=0.4)
```

![image](https://user-images.githubusercontent.com/106129152/210949247-8846cdcf-00e9-46ff-91fb-85025ea66922.png)

**이미지 여러 장 예측**

```python
import pandas as pd

test_df = pd.read_csv('./VOCdevkit/VOC2007/ImageSets/Main/val.txt', header=None, names=['image_name'])
zf = lambda x: str(x).zfill(6)
test_img_paths = './VOCdevkit/VOC2007/JPEGImages/' + test_df['image_name'].apply(zf) + '.jpg'

test_imgs = [mmcv.imread(x) for x in test_img_paths.values]
len(test_imgs), test_imgs[0].shape
# (2510, (375, 500, 3))
```

```python
for i in range(10):
  result = inference_detector(model_ckpt, test_imgs[i])
  show_result_pyplot(model_ckpt, test_imgs[i], result, score_thr=0.4)
```

![image](https://user-images.githubusercontent.com/106129152/210949273-8a558841-70a7-4dd9-abd6-247a7e54c97f.png)

![image](https://user-images.githubusercontent.com/106129152/210949289-2e44d058-c10c-45ae-8ed2-3fc506d18804.png) 

![image](https://user-images.githubusercontent.com/106129152/210949311-dea424c2-0e72-43e2-98ed-59fe315cc73c.png)

![image](https://user-images.githubusercontent.com/106129152/210949352-e01590b3-2e8f-4f9a-a82c-26e4250863ad.png)

![image](https://user-images.githubusercontent.com/106129152/210949381-ab0395cb-73fb-4ea2-a269-16d0db4187ec.png)

![image](https://user-images.githubusercontent.com/106129152/210949399-a3b3f97d-0ee4-46b1-9c15-7fb5da4b333a.png)

![image](https://user-images.githubusercontent.com/106129152/210949413-82a88a3a-60be-42d9-a196-452c97abfb68.png)

![image](https://user-images.githubusercontent.com/106129152/210949435-0bca8a89-cc54-4188-ad49-ff1fc5dc8bc2.png)

![image](https://user-images.githubusercontent.com/106129152/210949453-99bf5e22-29be-4cf7-9d48-b97a73fba0b9.png)

![image](https://user-images.githubusercontent.com/106129152/210949468-48212472-4173-4b4f-bb96-d854d6769f6c.png)

**TEST Evaluation**

```python
from mmdet.datasets import build_dataloader, build_dataset

# test용 Dataset과 DataLoader 생성. 
# build_dataset()호출 시 list로 감싸지 않는 것이 train용 dataset 생성시와 차이. 
#test_dataset = build_dataset(cfg.data.test)
data_loader = build_dataloader(
        test_dataset,
        # 반드시 아래 samples_per_gpu 인자값은 1로 설정
        samples_per_gpu=1, #cfg.data.samples_per_gpu,
        workers_per_gpu=cfg.data.workers_per_gpu,
        dist=False,
        shuffle=False)

# 반드시 아래 코드에서 'img' 키값이 tensor로 출력되어야 함. 
next(iter(data_loader))
```
